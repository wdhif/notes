[{"id":0,"href":"/notes/docs/linux-kernel-development/1-introduction-to-the-linux-kernel/","title":"1. Introduction to the Linux Kernel","section":"Linux Kernel Development","content":" Introduction to Unix # Unix is a family of operating systems featuring a similar API and design, including Unix, BSD, Solaris and Linux.\nSome Unix characteristics that are at the core of its strength:\nUnix is simple: only hundreds of system calls and have a straightforward, even basic, design. In Unix, everything is a file: simplifies the manipulation of data and devices into a set of core system calls: open(),read(), write(), lseek(), and close(). Unix has robust IPC (Interprocess communication) systems. Overview of Operating Systems and Kernels # The operating system is the parts of the system responsible for basic use and administration. The kernel is the innermost portion of the operating system. It is the core internals: the software that provides basic services for all other parts of the system, manages hardware, and distributes system resource.\nAn OS contains:\nKernel: Interrupt handlers to service interrupt requests. A scheduler to share processor time among multiple processes. A memory management system to manage process address spaces. System services such as networking and interprocess communication. Device drivers Boot loader Command shell User interfaces Basic utilities Kernel-space and User-space # Kernel-space: on modern systems with protected memory management units, the kernel typically resides in an elevated system state, which includes a protected memory space and full access to the hardware. This system state and memory space is collectively referred to as kernel-space.\nUser-space: applications execute in user-space, where they can access a subset of the machine’s available resources and can perform certain system functions, directly access hardware, access memory outside of that allotted them by the kernel, or otherwise misbehave.\nWhen executing kernel code, the system is in kernel-space executing in kernel mode. When running a regular process, the system is in user-space executing in user mode.\nApplications running on the system comminicate with the kernel using system calls (Figure 1.1)\nBasically, an application will use a C library to use the system call interface. The system calls will allow the Kernel to do operation on behalf of the application. Furthermore, the application is said to be executing a system call in kernel-space, and the kernel is running in process context.\nSome function of this C library provide many feature not found in system calls, E.G. the printf() function will not only wall the write() system call but will also do formating, etc\u0026hellip;\nOn the other hand, the open() function simply use the open() system call. Other functions of this C library does not use system calls at all, E.G. strcpy().\nThe Kernel also manage system\u0026rsquo;s hardware. When the hardware want to communicate, it will send an hardware interrupt to the CPU, which will in turn interrupt the Kernel, which will in turn operate a context switch to execute the interrupt handler.\ngraph LR; A[Hardware] --\u003e|Interrupt lines| B[CPU] B[CPU] --\u003e|Interrupt| C[Kernel] C[Kernel] --\u003e|Interrupt Handler| D[Run the interrupt handler code] A number identifies each interrupt, the kernel reads this number and then execute the handler code.\nFor example:\nsequenceDiagram participant Y as You participant KC as Keyboard Controller participant C as CPU participant KR as Kernel participant IH as Interrupt Handler Process Y-\u003e\u003eKC: Type on keyboard KC-\u003e\u003eC: Issue an hardware interrupt C-\u003e\u003eKR: Forward interrupt with interrupt number KR-\u003e\u003eIH: Run interrupt handler according to interrupt number IH-\u003e\u003eKC: Process Keyboard data IH-\u003e\u003eKC: Let it know it's ready for more data Running interrupt handlers put the Kernel in a special context, the interrupt context, not associated with any process.\nEach CPU in the Linux Kernel is doing exaclty one of the following:\nIn user-space, running user code in a process. In kernel-space, in process context, running kernel code on behalf of a specific process. In kernel-space, in interrupt context, not associated with any process, handling an interrupt. This list is inclusive, every cases fit any of the previous element. E.G. Even when the Kernel is idle, the CPU is actually running an idle process\nMonolithic Kernels vs Microkernels # Most Unix kernels are monolithics and although the Linux kernel is monolithic, it can dynamically load and unload kernel modules on demand.\nMonolithic Kernels are:\nSimpler than Microkernels. Implemented as a single process. Exist as a single binary. Runs in kernel mode. Runs everything in the same address space. Because of this, communication is trivial, the kernel simply invoke functions. Microkernels are:\nBroken down into separate process, called servers Only some servers runs in priviledged mode, if they need it, the rest runs in user mode. Because of this, direct function invocation is impossible. Microkernel uses message passing, an Inter Process communication (IPC) method. Servers invoke services from other servers, using message passing. Microkernels are modular, if one server crash, it doesn\u0026rsquo;t crash the all kernel. IPC (message passing) and context switches (user mode vs privileged mode) involve some serious overhead To avoid this, modern Microkernels (Windows NT and mac OS) runs every servers in privileged mode, defeating the primary purpose of microkernels. "},{"id":1,"href":"/notes/docs/linux-performance-tools/1-methodologies/","title":"1. Methodologies","section":"Linux Performance Tools","content":" Methodologies # There are dozens of performance tools for Linux\nPackages: sysstat, procps, coreutils, \u0026hellip; Commercial products Methodologies can provide guidance for choosing and using tools effectively, by giving you a starting point, a process, and an ending point in your diagnotics. Problem Statement Method # What makes you think there is a performance problem? Has this system ever performed well? What has changed recently? (Software? Hardware? Load?) Can the performance degradation be expressed in terms of latency or run time? Does the problem affect other people or applications (or is it just you)? What is the environment? Software, hardware, instance types? Versions? Configuration? Workload Characterization Method # Who is causing the load? PID, UID, IP addr, \u0026hellip; Why is the load called? code path, stack trace What is the load? IOPS, throughput, type, r/w How is the load changing over time? The USE Method # For every resource, check:\nUtilization: Busy Saturation Errors "},{"id":2,"href":"/notes/docs/linux-kernel-development/","title":"Linux Kernel Development","section":"Docs","content":" 1. Introduction to the Linux Kernel 2. Getting Started with the Kernel 3. Process Management 4. Process Scheduling 5. System Calls 6. Kernel Data Structures 7. Interrupts and Interrupt Handlers 8. Bottom Halves and Deferring Work 9. An Introduction to Kernel Synchronization 10. Kernel Synchronization Methods 11. Timers and Time Management 12. Memory Management "},{"id":3,"href":"/notes/docs/linux-performance-tools/","title":"Linux Performance Tools","section":"Docs","content":" 1. Methodologies 2. Tools "},{"id":4,"href":"/notes/docs/linux-kernel-development/2-getting-started-with-the-kernel/","title":"2. Getting Started with the Kernel","section":"Linux Kernel Development","content":" TODO # "},{"id":5,"href":"/notes/docs/linux-performance-tools/2-tools/","title":"2. Tools","section":"Linux Performance Tools","content":" Tools # There are dozens of performance tools for Linux\nPackages: sysstat, procps, coreutils, \u0026hellip; Commercial products Methodologies can provide guidance for choosing and using tools effectively, by giving you a starting point, a process, and an ending point in your diagnotics. Command Line Tools # Useful to study even if you never use them: GUIs and commercial products often use the same interfaces.\nTool Types\nType Characteristic Observability Watch activity. Usually safe, depending on resource overhead. Benchmarking Load test. Caution: production tests can cause issues due to contention. Tuning Change. Danger: changes could hurt performance, now or later with load. Static Check configuration. Should be safe Observability Tools # Basic Observability Tools:\nuptime top (or htop) ps vmstat iostat mpstat free uptime # One way to print load averages: $ uptime 07:42:06 up 8:16, 1 user, load average: 2.27, 2.84, 2.91 A measure of resource demand: CPUs + disks Other OSes only show CPUs: easier to interpret Exponentially-damped moving averages Time constants of 1, 5, and 15 minutes Allow to have an historic trend without a line graph If the load is larger than the number of CPUs, it may indicate CPU saturation Don’t spend more than 5 seconds studying these top (or htop) # System and per-process interval summary: $ top - 18:50:26 up 7:43, 1 user, load average: 4.11, 4.91, 5.22 Tasks: 209 total, 1 running, 206 sleeping, 0 stopped, 2 zombie Cpu(s): 47.1%us, 4.0%sy, 0.0%ni, 48.4%id, 0.0%wa, 0.0%hi, 0.3%si, 0.2%st Mem: 70197156k total, 44831072k used, 25366084k free, 36360k buffers Swap: 0k total, 0k used, 0k free, 11873356k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 5738 apiprod 20 0 62.6g 29g 352m S 417 44.2 2144:15 java 1386 apiprod 20 0 17452 1388 964 R 0 0.0 0:00.02 top 1 root 20 0 24340 2272 1340 S 0 0.0 0:01.51 init 2 root 20 0 0 0 0 S 0 0.0 0:00.00 kthreadd […] %CPU is summed across all CPUs Can miss short-lived processes (atop won’t) Can consume noticeable CPU to read /proc ps # Process status listing (eg, “ASCII art forest”):\nCustom fields:\n"},{"id":6,"href":"/notes/docs/linux-kernel-development/3-process-management/","title":"3. Process Management","section":"Linux Kernel Development","content":" The Process # A process, or task in the Kernel, is a program (object code) that is currently running, but a program is not a process, two or more processes can actually run the same program at the same time.\nThey are more than juste code (text section), they also include a set of resources such as:\nOpen files. Pending signals. Internal Kernel data. Processor state. A memory address space with one or more mappings. One or more threads of execution. A data section containing global variables. Threads of execution, or threads, are the objects of activity within the process. Each thread includes:\nA unique program counter. A Process stack. A set of processor registers. The kernel schedules individual threads, not processes.\nProcess that runs more than one thread are call multithreaded.\nOn modern operating systems, processes provide two virtualizations:\nA virtualized processor: The virtual processor gives the process the illusion that it alone monopolizes the system, despite possibly sharing the processor among hundreds of other processes. A virtual memory: Virtual memory lets the process allocate and manage memory as if it alone owned all the memory in the system. Note that threads share the virtual memory abstraction, but they each receives their own virtualized processor.\nProcess lifecycle # The process lifecycle goes as follow:\nfork(): A parent process will run the fork() system call, effectively cloning himself (fork() is implemented using clone()) into a child process. The parent process will then resume execution and the child process will start execution at the same place, where the fork() system call returns. fork() will in fact return twice, once in the parent process, and once in the child process. At some point however, the parent process will have to run a wait() type system call to reap its exited child process. exec(): After its creation, the child process will run one of the exec() system calls that will create a new address space and load a new program into it. exit(): Finally, when a process exits, it run the exit() system call. This will terminate the process and free all its ressources. The process will then be placed in a zombie state and until its parent process run the wait4() system call. Process Descriptor and the Task Structure # The kernel stores the list of processes in a circular doubly linked list called the task list. Each element of this list is called a process descriptor that will contain all informations about a given process.\nThe system identifies processes by a unique process identification value or PID, stored in the process descriptor. The PID is typically an integer going up to 32,768. This maximum value is important because it is essentially the maximum number of processes that may exist concurrently on the system.\nThe Kernel will affect PID to each process in an ascending order, this creates the notion that the higher the PID, the later the process started. But if the maximum number of PID is reached, then it will wrap back and try to affect freed PID to new process, detroying this useful notion.\nYou can also increase this value up to four millons, by modifying /proc/sys/kernel/pid_max. Take note that this might break compatibility with older programs.\nProcess State # The state field of the process descriptor describes the current condition of the process. Each process on the system is in exactly one of the five following states:\nTASK_RUNNING: The process is runnable, it is either currently running or on a runqueue waiting to run. This is the only possible state for a process executing in user-space. It can also apply to a process in kernel-space that is actively running. TASK_INTERRUPTIBLE: The process is sleeping, or blocked, waiting for some condition to exist. When this condition exists, the kernel sets the process’s state to TASK_RUNNING. The process also awakes prematurely and becomes runnable if it receives a signal. TASK_UNINTERRUPTIBLE: This state is identical to TASK_INTERRUPTIBLE except that it does not wake up and become runnable if it receives a signal. This is used in situations where the process must wait without interruption or when the event is expected to occur quite quickly. Because the task does not respond to signals in this state, TASK_UNINTERRUPTIBLE is less often used than TASK_INTERRUPTIBLE. __TASK_TRACED: The process is being traced by another process, such as a debugger (like strace), using the ptrace() system call. __TASK_STOPPED: Process execution has stopped. The task is not running nor is it eligible to run. This occurs if the task receives the SIGSTOP, SIGTSTP, SIGTTIN, or SIGTTOU signal or if it receives any signal while it is being debugged. Take note that a process in the TASK_UNINTERRUPTIBLE state will not even respond to SIGKILL. This state is represented by the famous D state in the ps command.\nProcess Context # The goal of a process is to execute program code. This code is read from an executable file and executed from within the program\u0026rsquo;s address space. Normal program execution occurs in user-space, when a program executes a system call or triggers an exception, it enters kernel-space. At this point, the kernel is said to be “executing on behalf of the process” and the kernel is in process context.\nUpon exiting the kernel, the process resumes execution in user-space, unless a higher-priority process has become runnable in the interim, in which case the scheduler is invoked to select the higher priority process.\nSystem calls and exception handlers are well-defined interfaces into the kernel. A process can begin executing in kernel-space only through one of these interfaces. All access to the kernel is through these interfaces.\nThe Process Family Tree # There is a hierarchy between processes. All processes are descendant of the init process, whose PID is 1. The Kernel will start the init process as the last step of the boot procedure. The init process will then read the initscripts and start all other programs necessary to complete the boot procedure.\nEvery process on the system has exactly one parent. Likewise, every process has zero or more children. Processes that are all direct children of the same parent are called siblings.\nProcess Creation # Process creation in Unix is unique. Unix takes the unusual approach of separating these steps into two distinct system calls: fork() and exec().\nfork() will create a child process that is a copy of the current task. It only differs by its PID, PPID (Parent PID) and some other resources, such as pending signals which are not inherited. exec() will load a new program in the task address space and start executing it. Copy-on-Write # Traditionnaly, upon fork(), all resources owned by the parent process are copied to the child process memory space. Even worse, if the first call of the child process is exec(), then all the copied data is discarded to allow the new program to run. To optimize this, Linux\u0026rsquo;s fork() is using what is called copy-on-write pages.\nCopy-on-write, or COW is working like this:\nWhen the fork() system call is invoked, rather than duplicating the address space, each process, parent and child, will share a single read-only copy. The data is marked so that if it is written to, a duplicate is made and each process will get its own copy, therefor, copy-on-write. Only the pages that are written to are duplicated, the rest of the memory space will still be shared. If the pages are never written, for example if the child calls exec() directly after the fork(), then the pages are never copied. To make sure that the child process is given the possibility to run exec() before the parent write to the data, the Kernel will schedule first the child process.\nThreads # Thread provide multiple threads of execution within the same program in a shared memory address space. They can also share open files and other resources. Threads enable concurrent programming and, on multiple processor systems, true parallelism.\nTo the Linux kernel, there is no concept of a thread. Linux implements all threads as standard processes. The Linux kernel does not provide any special scheduling semantics or data structures to represent threads. Instead, a thread is merely a process that shares certain resources with other processes. Each thread has appears to the kernel as a normal process and just happen to share resources, such as an address space, with other processes.\nKernel Threads # It is often useful for the kernel to perform some operations in the background. The kernel accomplishes this via kernel threads, standard processes that exist solely in kernel-space. The significant difference between kernel threads and normal processes is that kernel threads do not have an address space. They operate only in kernel-space and do not context switch into user-space. Kernel threads, however, are schedulable and preemptable, the same as normal processes.\nKernel threads are created on system boot by other kernel threads. Indeed, a kernel thread can be created only by another kernel thread.\nProcess Termination # When a process terminates, the kernel:\nReleases the resources owned by the process. Notifies the process’s parent of the termination of its child. Generally, process destruction is self-induced. It occurs when the process calls the exit() system call. A process can also terminate involuntarily, this occurs when the process receives a signal or exception it cannot handle or ignore.\nUpon deletion, the task enters the EXIT_ZOMBIE exit state. The only memory consumed is the one in the task list of the kernel. After the parent reap the child using the wait() system call, the kernel fully removes the task from the task list and the process is deleted.\nParentless Tasks, AKA Zombie Processes # If a parent exits before its children, some mechanism must exist to reparent any child tasks to a new process, or else parentless terminated processes would forever remain zombies, wasting system memory. The solution is to reparent a task’s children on exit to either another process in the current thread group or, if that fails, the init process.\nWith the process successfully reparented, there is no risk of stray zombie processes. The init process routinely calls wait() on its children, cleaning up any zombies assigned to it.\n"},{"id":7,"href":"/notes/docs/linux-kernel-development/4-process-scheduling/","title":"4. Process Scheduling","section":"Linux Kernel Development","content":" The Scheduler # The process scheduler decides which process runs, when, and for how long.\nThe process scheduler (or simply the scheduler, to which it is often shortened) divides the finite resource of processor time between the runnable processes on a system. By deciding which process runs next, the scheduler is responsible for best utilizing the system and giving users the impression that multiple processes are executing simultaneously, AKA Multitasking.\nThe ideas behind the scheduler are simple:\nTo best utilize processor time, assuming there are runnable processes, a process should always be running. If there are more runnable processes than processors in a system, some processes will not be running at a given moment. These processes are waiting to run. Deciding which process runs next, given a set of runnable processes, is the fundamental decision that the scheduler must make. Multitasking # A multitasking operating system is one that can simultaneously interleave execution of more than one process.\nOn single processor machines, this gives the illusion of multiple processes running concurrently. On multiprocessor machines, such functionality enables processes to actually run concurrently, in parallel, on different processors. On either type of machine, it also enables many processes to block or sleep, not actually executing until work is available. These processes, although in memory, are not runnable. Instead, such processes utilize the kernel to wait until some event (keyboard input, network data, passage of time, and so on) occurs.\nMultitasking operating systems come in two flavors:\ncooperative multitasking: A process does not stop running until it voluntary decides to do so, it is called yielding. Ideally, processes yield often, giving each runnable process a decent chunk of the processor. preemptive multitasking: The scheduler decides when a process is to cease running and a new process is to begin running, it is called preemption. The time a process runs before it is preempted is usually predetermined, and it is called the timeslice of the process. The process is given a slice of processor\u0026rsquo;s time to run. The shortcomings of the cooperative multitasking approach are manifest, the scheduler cannot make global decisions regarding how long processes run. Processes can monopolize the processor for longer than the user desires, and a hung process that never yields can potentially bring down the entire system.\nOn preemptive multitasking, the scheduler manage each process timeslices, making global decisions for the system (e.g. priorization) and preventing any one process from monopolizing the processor. The timeslice is dynamically calculated as a function of process behavior and configurable system policy.\nLinux, like all Unix variants and most modern operating systems, implements preemptive multitasking.\nLinux\u0026rsquo;s Process Scheduler # The current Linux scheduler is the Completely Fair Scheduler, or CFS. The CFS was introduced as a replacement for the O(1) scheduler, which was performant, but was lacking when dealing with interactive process, like on a desktop for example.\nPolicy # The policy of a scheduler determines what runs when. Therefore it is very important.\nI/O-Bound Versus Processor-Bound Processes # Processes can be classified as two types:\nI/O-bound: A process that spend much of its time waiting on I/O requests (I/O here means any blockable resources, like keyboard inputs or network I/O, not only disk I/O). Such processes are runnable for only short durations before having to wait on requests, like GUI waiting on user inputs. processor-bound: Processor-bound processes spend much of their time executing code. They tend to run until they are preempted because they do not block on I/O requests very often. Linux, aiming to provide good interactive response and desktop performance, optimizes for response time (low latency), thus favoring I/O-bound processes over processor-bound processors. As we will see, this is done in a creative manner that does not neglect processor-bound processes.\nProcess Priority # A common type of scheduling algorithm is priority-based scheduling. The goal is to rank processes based on their worth and need for processor time. Processes with a higher priority run before those with a lower priority, whereas processes with the same priority are scheduled in round-robin (one after the next, repeating).\nThe runnable process with timeslice remaining and the highest priority always runs. Both the user and the system can set a process’s priority to influence the scheduling behavior of the system.\nThe Linux kernel implement two priority ranges:\nNiceness: A number from -20 to +19 with a default at 0. Larger nice values correspond to a lower priority, you are being “nice” to the other processes on the system. Processes with a lower nice value (higher priority) receive a larger proportion of the system’s processor compared to processes with a higher nice value (lower priority). Real-time priority: The values are configurable, but by default range from 0 to 99, inclusive. Higher real-time priority values correspond to a greater priority and all real-time processes are at a higher priority than normal processes. By default, processes are not real-time, their value is \u0026ldquo;-\u0026rdquo;, or null. Timeslice # The timeslice is the numeric value that represents how long a task can run until it is preempted. The scheduler policy must dictate a default timeslice, which is not a trivial exercise.\nToo long a timeslice causes the system to have poor interactive performance, the system will no longer feel as if applications are concurrently executed. Too short a timeslice causes significant amounts of processor time to be wasted on the overhead of switching processes. Furthermore, the conflicting goals of I/O-bound versus processor-bound processes again arise:\nI/O-bound processes do not need longer timeslices (although they do like to run often) Processor-bound processes crave long timeslices (to keep their caches hot). To avoid thoses issues, Linux’s CFS scheduler does not directly assign timeslices to processes. Instead, it will assign to processes a proportion of the processor, therefore, the amount of processor time that a process receives is a function of the load of the system. This assigned proportion is further affected by each process’s nice value. The nice value acts as a weight, changing the proportion of the processor time each process receives.\nWhen a process enters the runnable state, it becomes eligible to run. Whether the process runs immediately, preempting the currently running process, is a function of how much of a proportion of the processor the newly runnable processor has consumed.\nIf it has consumed a smaller proportion of the processor than the currently executing process, it runs immediately, preempting the current process. If not, it is scheduled to run at a later time. The Scheduling Policy in Action # Consider two runnable processes:\nA text editor: I/O-bound. It will spend nearly all its time waiting for user key presses. A video encoder: Processor-bound. Aside from waiting on disk to read the video at first, it will then spend all its time encoding the video, consuming 100% of the processor. The text editor, despite spending most of its time waiting on user input, when the text editor does receive a key press, the user expects the editor to respond immediately. Latency is a primary concern. The video encoder doesn\u0026rsquo;t have strong time constraint, we don\u0026rsquo;t really care if it runs now or at the next CPU cycle. The sooner it finishes the better, but latency is not a primary concern.\nIf both processes are runnable and have the same nicelevel, they would be awarded both with the same proportion of the processor\u0026rsquo;s time. But since the text editor spend a lot of time waiting, it would almost always have more processor time left than the video encoder. And because of that, CFS would let the text editor run in priority, therefore limiting latency.\nThis will allow the text editor to run everytime it\u0026rsquo;s ready, and the video encoder to run the rest of the time, assuming no other process needs to run.\nThe Linux Scheduling Algorithm # Scheduler Classes # The Linux scheduler is modular, enabling different algorithms to schedule different types of processes. This modularity is called scheduler classes. Scheduler classes enable different, pluggable algorithms to coexist, scheduling their own types of processes. Each scheduler class has a priority.\nThe base scheduler code, iterates over each scheduler class in order of priority. The highest priority scheduler class that has a runnable process wins, selecting who runs next. The Completely Fair Scheduler (CFS) is the registered scheduler class for normal processes, called SCHED_NORMAL in Linux.\nFair Scheduling # CFS is based on a simple concept: Model process scheduling as if the system had an ideal, perfectly multitasking processor. In such a system, each process would receive 1/n of the processor’s time, where n is the number of runnable processes. We’d schedule them for infinitely small durations, so that in any measurable period we’d have run all n processes for the same amount of time.\nThe Linux Scheduling Implementation # Time Accounting # All process schedulers must account for the time that a process runs. Most Unix systems do so, as discussed earlier, by assigning each process a timeslice. On each tick of the system clock, the timeslice is decremented by the tick period. When the timeslice reaches zero, the process is preempted in favor of another runnable process with a nonzero timeslice.\nCFS does not have the notion of a timeslice, but it must still keep account for the time that each process runs, because it needs to ensure that each process runs only for its fair share of the processor. This information is embedded in the process descriptor. We discussed the process descriptor in Chapter 3, “Process Management.”\nThe Virtual Runtime # The Virtual Runtime (vruntime) of a process the actual runtime (the amount of time spent running) normalized (or weighted) by the number of runnable processes, in nanoseconds. The Virtual Runtime update is triggered periodically by the system timer and also whenever a process becomes runnable or blocks, becoming unrunnable. In this manner, vruntime is an accurate measure of the runtime of a given process and an indicator of what process should run next.\nProcess Selection # On an ideal processor, the Virtual Runtime of all processes of the same priority would be identical, all tasks would have received an equal, fair share of the processor. In reality, we cannot perfectly multitask, so CFS attempts to balance a process’s Virtual Runtime with a simple rule: When CFS is deciding what process to run next, it picks the process with the smallest vruntime. The CFS maintains a red-black tree for the Virtual Runtimes.\nPreemption and Context Switching # Context switching, the switching from one runnable task to another, is handled by the scheduler. It is called when a new process has been selected to run. It does two basic jobs:\nSwitch the virtual memory mapping from the previous process’s to that of the new process. Switch the processor state from the previous process’s to the current’s. This involves saving and restoring stack information and the processor registers and any other architecture-specific state that must be managed and restored on a per-process basis. The kernel, however, must know when to call the scheduler. If it called it only when code explicitly did so, user-space programs could run indefinitely.\nInstead, the kernel provides the need_resched flag to signify whether a reschedule should be performed.\nThis flag is set by scheduler_tick() (which is ran by a timer) when a process should be preempted. This flag is set by try_to_wake_up() when a process that has a higher priority than the currently running process is awakened. Upon returning to user-space or returning from an interrupt, the kernel checks the need_resched flag. If it is set, the kernel invokes the scheduler (using schedule()) before continuing. The flag is a message to the kernel that the scheduler should be invoked as soon as possible because another process deserves to run.\nUser preemption # User preemption occurs when the kernel is about to return to user-space, need_resched is set, and therefore, the scheduler is invoked. If the kernel is returning to user-space, it knows it is safe to continue executing the current task or to pick a new task to execute.\nConsequently, whenever the kernel is preparing to return to user-space either on return from an interrupt or after a system call, the value of need_resched is checked. If it is set, the scheduler is invoked to select a new (more fit) process to execute.\nUser preemption can occur\nWhen returning to user-space from a system call. When returning to user-space from an interrupt handler. Kernel Preemption # The Linux kernel, unlike most other Unix variants and many other operating systems, is a fully preemptive kernel. In nonpreemptive kernels, kernel code runs until completion, the scheduler cannot reschedule a task while it is in the kernel. Kernel code runs until it finishes (returns to user-space) or explicitly blocks. However, the Linux kernel is able to preempt a task at any point, so long as the kernel is in a state in which it is safe to reschedule.\nSo when is it safe to reschedule? The kernel can preempt a task running in the kernel so long as it does not hold a lock. That is, locks are used as markers of regions of nonpreemptibility. If a lock is not held, the current code is reentrant and capable of being preempted.\nKernel preemption can occur\nWhen an interrupt handler exits, before returning to kernel-space. When kernel code becomes preemptible again, when all locks are released. If a task in the kernel explicitly calls schedule(). If a task in the kernel blocks (which results in a call to schedule()). Real-Time Scheduling Policies # Linux provides two real-time scheduling policies\nSCHED_FIFO SCHED_RR The normal, not real-time scheduling policy is SCHED_NORMAL (using the CFS). Via the scheduling classes framework, these real-time policies are managed not by the Completely Fair Scheduler, but by a special real-time scheduler.\nSCHED_FIFO implements a simple first-in, first-out scheduling algorithm without timeslices.\nA runnable SCHED_FIFO task is always scheduled over any SCHED_NORMAL tasks. When a SCHED_FIFO task becomes runnable, it continues to run until it blocks or explicitly yields the processor; it has no timeslice and can run indefinitely. Only a higher priority SCHED_FIFO or SCHED_RR task can preempt a SCHED_FIFO task. Two or more SCHED_FIFO tasks at the same priority run round-robin, but only yielding the processor when they explicitly choose to do so. If a SCHED_FIFO task is runnable, all other tasks at a lower priority cannot run until the SCHED_FIFO task becomes unrunnable. SCHED_RR is identical to SCHED_FIFO except that each process can run only until it exhausts a predetermined timeslice. In other words, SCHED_RR is SCHED_FIFO with timeslices. It is a real-time, round-robin scheduling algorithm.\nWhen a SCHED_RR task exhausts its timeslice, any other real-time processes at its priority are scheduled round-robin. The timeslice is used to allow only rescheduling of same-priority processes. As with SCHED_FIFO, a higher-priority process always immediately preempts a lower-priority one, and a lower-priority process can never preempt a SCHED_RR task, even if its timeslice is exhausted. Both real-time scheduling policies implement static priorities. The kernel does not calculate dynamic priority values for real-time tasks. This ensures that a real-time process at a given priority always preempts a process at a lower priority.\nTwo types of real-time behavior exists:\nSoft real-time, meaning that the kernel tries to schedule applications within timing deadlines, but the kernel does not promise to always achieve these goals. Hard real-time systems are guaranteed to meet any scheduling requirements within certain limits. The real-time scheduling policies in Linux provide soft real-time behavior. Linux makes no guarantees on the capability to schedule real-time tasks. Despite not having a design that guarantees hard real-time behavior, the real-time scheduling performance in Linux is quite good.\nReal-time priorities range inclusively from 0 to MAX_RT_PRIO - 1. By default, this range is 0 to 99, since MAX_RT_PRIO is 100. This priority space is shared with the nice values of SCHED_NORMAL tasks. They use the space from MAX_RT_PRIO to (MAX_RT_PRIO + 40). By default, this means the –20 to +19 nice range maps directly onto the priority space from 100 to 139.\nDefault priority ranges:\n0 to 99: real-time priorities. 100 to 139: normal priorities. Scheduler-Related System Calls # Linux provides a family of system calls for the management of scheduler parameters. These system calls allow manipulation of process priority, scheduling policy, and processor affinity, as well as provide an explicit mechanism to yield the processor to other tasks.\n"},{"id":8,"href":"/notes/docs/linux-kernel-development/5-system-calls/","title":"5. System Calls","section":"Linux Kernel Development","content":" System Calls # In any modern operating system, the kernel provides a set of interfaces by which processes running in user-space can interact with the system. These interfaces give applications controlled access to hardware, a mechanism with which to create new processes and communicate with existing ones, and the capability to request other operating system resources. The existence of these interfaces, and the fact that applications are not free to directly do whatever they want, is key to providing a stable system.\nCommunicating with the Kernel # System calls provide a layer between the hardware and user-space processes, which serves three primary purposes:\nProviding an abstracted hardware interface for userspace. For example, when reading or writing from a file, applications are not concerned with the type of disk, media, or even the type of filesystem on which the file resides. Ensuring system security and stability. The kernel acts as a middleman between system resources and user-space, so it can arbitrate access based on permissions, users, and other criteria. For example, this arbitration prevents applications from incorrectly using hardware, stealing other processes’ resources, or otherwise doing harm to the system. A single common layer between user-space and the rest of the system allows for the virtualized system provided to processes. It would be impossible to implement multitasking and virtual memory if applications were free to access access system resources without the kernel’s knowledge. In Linux, system calls are the only means user-space has of interfacing with the kernel and the only legal entry point into the kernel other than exceptions and traps. Other interfaces, such as device files or /proc, are ultimately accessed via system calls. Interestingly, Linux implements far fewer system calls than most systems.\nAPIs, POSIX, and the C Library # Applications are typically programmed against an Application Programming Interface (API) implemented in user-space, not directly to system calls, because no direct correlation is needed between the interfaces used by applications and the actual interface provided by the kernel. An API defines a set of programming interfaces used by applications. Those interfaces can be implemented as a system call, implemented through multiple system calls, or implemented without the use of system calls at all. The same API can exist on multiple systems and provide the same interface to applications while the implementation of the API itself can differ greatly from system to system.\nThe most common APIs in the Unix world is based on POSIX. Technically, POSIX is composed of a series of standards from the IEEE that aim to provide a portable operating system standard roughly based on Unix. Linux strives to be POSIX- and SUSv3-compliant where applicable.\nOn most Unix systems, the POSIX-defined API calls have a strong correlation to the system calls. Some systems that are rather un-Unix, such as Microsoft Windows, offer POSIX-compatible libraries.\nThe system call interface in Linux, as with most Unix systems, is provided in part by the C library.\nThe C library implements the main API on Unix systems, including the standard C library and the system call interface. The C library is used by all C programs and, because of C’s nature, is easily wrapped by other programming languages for use in their programs. The C library additionally provides the majority of the POSIX API.\nFrom the application programmer’s point of view, system calls are irrelevant; all the programmer is concerned with is the API. Conversely, the kernel is concerned only with the system calls; what library calls and applications make use of the system calls is not of the kernel’s concern. Nonetheless, it is important for the kernel to keep track of the potential uses of a system call and keep the system call as general and flexible as possible.\nSyscalls # System calls (often called syscalls in Linux) are typically accessed via function calls defined in the C library. The functions can define zero, one, or more arguments (inputs) and might result in one or more side effects. Although nearly all system calls have a side effect (that is, they result in some change of the system’s state), a few syscalls, such as getpid(), merely return some data from the kernel. System calls also provide a return value of type long (for compatibility with 64-bit architectures) that signifies success or error. Usually, although not always, a negative return value denotes an error. A return value of zero is usually (not always) a sign of success.\nSystem Call Numbers # In Linux, each system call is assigned a syscall number.This is a unique number that is used to reference a specific system call. When a user-space process executes a system call, the syscall number identifies which syscall was executed; the process does not refer to the syscall by name.\nWhen assigned, the syscall number cannot change; otherwise, compiled applications will break. If a system call is removed, its system call number cannot be recycled, or previously compiled code would aim to invoke one system call but would in reality invoke another. Linux provides a \u0026ldquo;not implemented\u0026rdquo; system call, sys_ni_syscall(), which does nothing except return ENOSYS, the error corresponding to an invalid system call. This function is used to \u0026ldquo;plug the hole\u0026rdquo; in the rare event that a syscall is removed or otherwise made unavailable.\nSystem Call Performance # System calls in Linux are faster than in many other operating systems. This is partly because of Linux’s fast context switch times; entering and exiting the kernel is a streamlined and simple affair. The other factor is the simplicity of the system call handler and the individual system calls themselves.\nSystem Call Handler # It is not possible for user-space applications to execute kernel code directly. They cannot simply make a function call to a method existing in kernel-space because the kernel exists in a protected memory space. If applications could directly read and write to the kernel’s address space, system security and stability would be nonexistent.\nUser-space applications signal the kernel that they want to execute a system call and have the system switch to kernel mode, where the system call can be executed in kernel-space by the kernel on behalf of the application. This mechanism is a software interrupt: incur an exception, and the system will switch to kernel mode and execute the exception handler. The exception handler in this case is actually the system call handler. The defined software interrupt on x86 for the syscall handler is interrupt number 128.\nRecently, x86 processors added a feature known as sysenter, which provides a faster, more specialized way of trapping into a kernel to execute a system call than using the int interrupt instruction.\nDenoting the Correct System Call # Simply entering kernel-space alone is not sufficient because multiple system calls exist, all of which enter the kernel in the same manner. Thus, the system call number must be passed into the kernel. On x86, the syscall number is fed to the kernel via the eax register. Before causing the trap into the kernel, user-space sticks in eax the number corresponding to the desired system call. The system call handler then reads the value from eax.\nParameter Passing # In addition to the system call number, most syscalls require that one or more parameters be passed to them. Somehow, user-space must relay the parameters to the kernel during the trap. The easiest way to do this is via the same means that the syscall number is passed: the parameters are stored in registers.\nOn x86-32, the registers ebx, ecx, edx, esi, and edi contain, in order, the first five arguments. In the unlikely case of six or more arguments, a single register is used to hold a pointer to user-space where all the parameters are stored.\nThe return value is sent to user-space also via register. On x86, it is written into the eax register.\nSystem Call Implementation # The actual implementation of a system call in Linux does not need to be concerned with the behavior of the system call handler. Thus, adding a new system call to Linux is relatively easy. The hard work lies in designing and implementing the system call; registering it with the kernel is simple.\nImplementing System Calls # Each syscall should have exactly one purpose. Multiplexing syscalls (a single system call that does wildly different things depending on a flag argument) is discouraged in Linux. System call should have a clean and simple interface with the smallest number of arguments possible. The semantics and behavior of a system call are important; they must not change, because existing applications will come to rely on them. Design the system call to be as general as possible with an eye toward the future. The purpose of the system call will remain constant but its uses may change. Verifying the Parameters # System calls must carefully verify all their parameters to ensure that they are valid, legal and correct to guarantee the system’s security and stability.\nOne of the most important checks is the validity of any pointers that the user provides. Before following a pointer into user-space, the system must ensure that:\nThe pointer points to a region of memory in user-space. Processes must not be able to trick the kernel into reading data in kernel-space on their behalf. The pointer points to a region of memory in the process’s address space.The process must not be able to trick the kernel into reading someone else’s data. The process must not be able to bypass memory access restrictions. If reading, the memory is marked readable. If writing, the memory is marked writable. If executing, the memory is marked executable. System Call Context # The kernel is in process context during the execution of a system call. The current pointer points to the current task, which is the process that issued the syscall.\nIn process context, the kernel is capable of sleeping (for example, if the system call blocks on a call or explicitly calls schedule()) and is fully preemptible. The capability to sleep means that system calls can make use of the majority of the kernel’s functionality.\n"},{"id":9,"href":"/notes/docs/linux-kernel-development/6-kernel-data-structures/","title":"6. Kernel Data Structures","section":"Linux Kernel Development","content":" Kernel Data Structures # TODO\n"},{"id":10,"href":"/notes/docs/linux-kernel-development/7-interrupts-and-interrupt-handlers/","title":"7. Interrupts and Interrupt Handlers","section":"Linux Kernel Development","content":" Interrupts and Interrupt Handlers # A core responsibility of any operating system kernel is managing the hardware connected to the machine, hard drives and Blu-ray discs, keyboards and mice, 3D processors and wireless radios. To meet this responsibility, the kernel needs to communicate with the machine’s individual devices.\nProcessors can be orders of magnitudes faster than the hardware they talk to; it is not ideal for the kernel to issue a request and wait for a response from slower hardware. Instead, the kernel must be free to go and handle other work, dealing with the hardware only after that hardware has actually completed its work.\nHow can the processor work with hardware without impacting the machine’s overall performance? One answer to this question is polling. Periodically, the kernel can check the status of the hardware in the system and respond accordingly. Polling incurs overhead, however, because it must occur repeatedly regardless of whether the hardware is active or ready. A better solution is to provide a mechanism for the hardware to signal to the kernel when attention is needed. This mechanism is called an interrupt. In this chapter, we discuss interrupts and how the kernel responds to them, with special functions called interrupt handlers.\nInterrupts # Interrupts enable hardware to signal to the processor. For example, as you type, the keyboard controller (the hardware device that manages the keyboard) issues an electrical signal to the processor to alert the operating system to newly available key presses. These electrical signals are interrupts. The processor receives the interrupt and signals the operating system to enable the operating system to respond to the new data.\nHardware devices generate interrupts asynchronously to the processor clock, they can occur at any time. The kernel can be interrupted at any time to process interrupts.\nAn interrupt is physically produced by electronic signals originating from hardware devices and directed into input pins on an interrupt controller, a simple chip that multiplexes multiple interrupt lines into a single line to the processor. Upon receiving an interrupt, the interrupt controller sends a signal to the processor. The processor detects this signal and interrupts its current execution to handle the interrupt. The processor can then notify the operating system that an interrupt has occurred, and the operating system can handle the interrupt appropriately. Different devices can be associated with different interrupts by means of a unique value associated with each interrupt. This way, interrupts from the keyboard are distinct from interrupts from the hard drive. This enables the operating system to differentiate between interrupts and to know which hardware device caused which interrupt. In turn, the operating system can service each interrupt with its corresponding handler. These interrupt values are often called interrupt request (IRQ) lines. Each IRQ line is assigned a numeric value, for example, on the classic PC, IRQ zero is the timer interrupt and IRQ one is the keyboard interrupt. Not all interrupt numbers, however, are so rigidly defined. Interrupts associated with devices on the PCI bus, for example, generally are dynamically assigned.\nA specific interrupt is associated with a specific device, and the kernel knows this. The hardware then issues interrupts to get the kernel’s attention.\nExceptions and Interrupts # Exceptions are often discussed at the same time as interrupts. Unlike interrupts, exceptions occur synchronously with respect to the processor clock; they are often called synchronous interrupts. Exceptions are produced by the processor while executing instructions either in response to a programming error (e.g. divide by zero) or abnormal conditions that must be handled by the kernel (e.g. a page fault). Because many processor architectures handle exceptions in a similar manner to interrupts, the kernel infrastructure for handling the two is similar.\nWe can define Interrupts and Exceptions as follows:\nInterrupts: asynchronous interrupts generated by hardware, also called hardware interrupts. Exceptions: synchronous interrupts generated by the processor, also called software interrupts. Interrupt Handlers # The function the kernel runs in response to a specific interrupt is called an interrupt handler or interrupt service routine (ISR). Each device that generates interrupts has an associated interrupt handler. For example, one function handles interrupts from the system timer, whereas another function handles interrupts generated by the keyboard. The interrupt handler for a device is part of the device’s driver, the kernel code that manages the device. Each device has one associated driver. If that device uses interrupts (and most do), that driver must register one interrupt handler.\nIn Linux, interrupt handlers are normal C functions, which match a specific prototype and thus enables the kernel to pass the handler information in a standard way. What differentiates interrupt handlers from other kernel functions is that the kernel invokes them in response to interrupts and that they run in a special context called interrupt context. This special context is occasionally called atomic context because code executing in this context is unable to block.\nBecause an interrupt can occur at any time, an interrupt handler can be executed at any time. It is imperative that the handler runs quickly, to resume execution of the code that was interrupted and the hardware as soon as possible. At the very least, an interrupt handler’s job is to acknowledge the interrupt’s receipt to the hardware to let it continue its normal operation.\nOften, however, interrupt handlers have a large amount of work to perform. For example, consider the interrupt handler for a network device. On top of responding to the hardware, the interrupt handler needs to copy networking packets from the hardware into memory, process them, and push the packets down to the appropriate protocol stack or application.\nTop Halves Versus Bottom Halves # These two goals, that an interrupt handler execute quickly and perform a large amount of work, conflict with one another.\nBecause of these competing goals, the processing of interrupts is split into two parts, or halves:\nTop half: The interrupt handler is the top half. The top half is run immediately upon receipt of the interrupt and performs only the work that is time-critical, such as acknowledging receipt of the interrupt or resetting the hardware. Bottom half: Work that can be performed later is deferred until the bottom half. The bottom half runs in the future, at a more convenient time, with all interrupts enabled. For example using the network card:\nWhen network cards receive packets from the network, the network cards immediately issue an interrupt. This optimizes network throughput and latency and avoids timeouts. The kernel responds by executing the network card\u0026rsquo;s registered interrupt. The interrupt runs, acknowledges the hardware, copies the new networking packets into main memory, and readies the network card for more packets. These jobs are the important, time-critical, and hardware-specific work. The kernel generally needs to quickly copy the networking packet into main memory because the network data buffer on the networking card is fixed and miniscule in size, particularly compared to main memory. Delays in copying the packets can result in a buffer overrun, with incoming packets overwhelming the networking card\u0026rsquo;s buffer and thus packets being dropped. After the networking data is safely in the main memory, the interrupt\u0026rsquo;s job is done, and it can return control of the system to whatever code was interrupted when the interrupt was generated. The rest of the processing and handling of the packets occurs later, in the bottom half. Reentrancy and Interrupt Handlers # Interrupt handlers in Linux need not be reentrant. When a given interrupt handler is executing, the corresponding interrupt line is masked out on all processors, preventing another interrupt on the same line from being received. Normally all other interrupts are enabled, so other interrupts are serviced, but the current line is always disabled. Consequently, the same interrupt handler is never invoked concurrently to service a nested interrupt. This greatly simplifies writing your interrupt handler.\nShared Handlers # Multiple devices could use the same IRQ line, its called Interrupt Sharing. When the kernel receives an interrupt, it invokes sequentially each registered handler on the line. Therefore, it is important that the handler be capable of distinguishing whether it generated a given interrupt. The handler must quickly exit if its associated device did not generate the interrupt. This requires the hardware device to have a status register (or similar mechanism) that the handler can check. Most hardware has such a feature.\nInterrupt Context # When executing an interrupt handler, the kernel is in interrupt context.\nThe Kernel can only be in two different context:\nIn process context In user-space, running user code in a process. In kernel-space, running kernel code on behalf of a specific process. in interrupt context, in kernel-space, not associated with any process, handling an interrupt. Interrupt context cannot sleep and cannot reschedule. Therefore, you cannot call certain functions from interrupt context. If a function sleeps, you cannot use it from your interrupt handler: this limits the functions that can be called from an interrupt handler. Interrupt context is time-critical, because the interrupt handler interrupts other code.\nImplementing Interrupt Handlers # The implementation of the interrupt handling system in Linux is architecture-dependent. The implementation depends on the processor, the type of interrupt controller used, and the design of the architecture and machine.\nInterrupt path # From a device to the processor\nA device issues an interrupt by sending an electric signal over its bus to the interrupt controller. If the interrupt line is enabled (they can be masked out), the interrupt controller sends the interrupt to the processor. In most architectures, this is accomplished by an electrical signal sent over a special pin to the processor. If interrupts are not disabled in the processor, the processor immediately stops what it is doing, disables the interrupt system, and jumps to a predefined location in memory and executes the code located there. This predefined point is set up by the kernel and is the entry point for interrupt handlers. Inside the Kernel\nSimilarly to system calls that enter the kernel through a predefined exception handler, the interrupt in the kernel begins at this predefined entry point.\nFor each interrupt line, the processor jumps to a unique location in memory and executes the code located there. In this manner, the kernel knows the IRQ number (the interrupt value) of the incoming interrupt. The initial entry point (assembly entry routine) saves the interrupt value and stores the current register values of the interrupted task on the stack. Then, the Kernel ensures that a valid handler is registered on the line and that it is enabled and not currently executing. If so, it calls the corresponding interrupt handler (using do_IRQ()). From this point, most of the interrupt handling code is written in C, but is still architecture-dependent.\nSince the processor disabled interrupts, they are turned back on if IRQF_DISABLED was not specified during the handler\u0026rsquo;s registration. Each potential handler is executed in a loop. If this line is not shared, the loop terminates after the first iteration. Otherwise, all handlers are executed. At this point, all potential handlers have been executed.\nIf a reschedule is pending If the kernel is returning to user-space (that is, the interrupt interrupted a user process), the scheduler is called. If the kernel is returning to kernel-space (that is, the interrupt interrupted the kernel itself), the scheduler is called only if the Kernel does not hold locks. Otherwise it is not safe to preempt the kernel. After schedule() returns, or if there is no work pending, the initial registers are restored and the kernel resumes whatever was interrupted. Interrupts and ProcFS # ProcFS is a virtual filesystem that exists only in kernel memory and is typically mounted at /proc. Reading or writing files in procfs invokes kernel functions that simulate reading or writing from a real file.\nThe /proc/interrupts file is populated with statistics related to interrupts on the system.\nInterrupt Control # The Linux kernel implements a family of interfaces for manipulating the state of interrupts. These interfaces enable you to disable the interrupt system for the current processor or mask out an interrupt line for the entire machine. These routines are all architecture-dependent.\nControling the interrupt system is needed to provide synchronization.\nDisabling interrupts guarantees that an interrupt handler will not preempt the current code. Disabling interrupts also disables kernel preemption. However, neither disabling interrupt delivery nor disabling kernel preemption provides any protection from concurrent access from another processor. Because Linux supports multiple processors, kernel code generally needs to obtain some sort of lock to prevent another processor from accessing shared data simultaneously. These locks are often obtained in conjunction with disabling local interrupts.\n"},{"id":11,"href":"/notes/docs/linux-kernel-development/8-bottom-halves-and-deferring-work/","title":"8. Bottom Halves and Deferring Work","section":"Linux Kernel Development","content":" Bottom Halves and Deferring Work # Interrupt handlers, can form only the first half of any interrupt processing solution, with the following limitations:\nInterrupt handlers run asynchronously and interrupt other potentially important code, including other interrupt handlers. Therefore, to avoid stalling the interrupted code for too long, interrupt handlers need to run as quickly as possible. Interrupt handlers are often timing-critical because they deal with hardware. Interrupt handlers do not run in process context. Therefore, they cannot block. This limits what they can do. Operating systems need a quick, asynchronous, and simple mechanism for immediately responding to hardware and performing any time-critical actions. Interrupt handlers serve this function well. Less critical work can and should be deferred to a later point when interrupts are enabled.\nConsequently, managing interrupts is divided into two parts, or halves. Interrupt handlers are the top halves.\nBottom Halves # The job of bottom halves is to perform any interrupt-related work not performed by the interrupt handler. You want the interrupt handler to perform as little work as possible and in turn be as fast as possible. By offloading as much work as possible to the bottom half, the interrupt handler can return control of the system to whatever it interrupted as quickly as possible.\nThe interrupt handler must perform some of the work. For example, the interrupt handler almost assuredly needs to acknowledge to the hardware the receipt of the interrupt. It may need to copy data to or from the hardware. This work is timing sensitive, so it makes sense to perform it in the interrupt handler. Almost anything else can be performed in the bottom half. For example, if you copy data from the hardware into memory in the top half, it makes sense to process it in the bottom half.\nNo hard and fast rules exist about what work to perform where. The decision is left entirely up to the device-driver author.\nThe point of a bottom half is not to do work at some specific point in the future, but simply to defer work until any point in the future when the system is less busy and interrupts are again enabled. Often, bottom halves run immediately after the interrupt returns. The key is that they run with all interrupts enabled.\nA World of Bottom Halves # While the top half is implemented entirely via the interrupt handler, multiple mechanisms are available for implementing a bottom half. These mechanisms are different interfaces and subsystems that enable you to implement bottom halves.\nThe Original \u0026ldquo;Bottom Half\u0026rdquo; # In the beginning, Linux provided only the \u0026ldquo;bottom half\u0026rdquo; for implementing bottom halves. This name was logical because at the time that was the only means available for deferring work. The infrastructure was also known as BH to avoid confusion with the generic term bottom half.\nThe BH interface was simple. It provided a statically created list of 32 bottom halves for the entire system. Each BH was globally synchronized. No two could run at the same time, even on different processors. This was simple and easy to use, but was also inflexible and a bottleneck.\nTask Queues # The kernel developers later introduced task queues both as a method of deferring work and as a replacement for the BH mechanism.\nThe kernel defined a family of queues.\nEach queue contained a linked list of functions to call. The queued functions were run at certain times, depending on which queue they were in. Drivers could register their bottom halves in the appropriate queue. This worked fairly well, but it was still too inflexible to replace the BH interface entirely. It also was not lightweight enough for performance-critical subsystems, such as networking.\nSoftirqs and Tasklets # The softirqs and tasklets were later introduced to completely replace the BH interface.\nSoftirqs are a set of statically defined bottom halves that can run simultaneously on any processor; even two of the same type can run concurrently. Tasklets are flexible, dynamically created bottom halves built on top of softirqs. Two different tasklets can run concurrently on different processors, but two of the same type of tasklet cannot run simultaneously. Tasklets are a good trade-off between performance and ease of use. For most bottom-half processing, the tasklet is sufficient. Softirqs are useful when performance is critical, such as with networking. Using softirqs requires more care, however, because two of the same softirq can run at the same time. In addition, softirqs must be registered statically at compile time. Conversely, code can dynamically register tasklets.\nAll BH users were converted to the other bottom-half interfaces. Additionally, the task queue interface was replaced by the work queue interface. Work queues are a simple yet useful method of queuing work to later be performed in process context.\nConsequently, the 2.6 kernel has three bottom-half mechanisms in the kernel:\nSoftirqs Tasklets Work queues Tasklets are built on softirqs and work queues are their own subsystem.\nKernel Timers # Kernel timers is another mechanism for deferring work. Unlike other mechanisms seen previously, timers defer work for a specified amount of time. That is, although the tools discussed before are useful to defer work to any time but now, you use timers to defer work until at least a specific time has elapsed. Therefore, timers have also other uses that Bottom Halves.\nSoftirqs # Softirqs are rarely used directly; tasklets, which are built on softirqs are a much more common form of bottom half.\nImplementing Softirqs # Softirqs are statically allocated at compile time. Unlike tasklets, you cannot dynamically register and destroy softirqs. A 32-entry array is used to store softirqs, registered softirq consumes one entry in the array. Consequently, there are predefined number of registered softirqs that is statically determined at compile time and cannot be changed dynamically. The kernel enforces a limit of 32 registered softirqs. In the current kernel, only nine exist.\nA softirq never preempts another softirq. The only event that can preempt a softirq is an interrupt handler. Another softirq (even the same one) can run on another processor, however.\nExecuting Softirqs # A registered softirq must be marked before it will execute. Usually, an interrupt handler marks its softirq for execution before returning. This is called raising the softirq. Then, at a suitable time, the softirq runs.\nPending softirqs are checked for and executed in the following places:\nIn the return from hardware interrupt code path. In the ksoftirqd kernel thread. In any code that explicitly checks for and executes pending softirqs, such as the networking subsystem. Using Softirqs # Softirqs are reserved for the most timing-critical and important bottom-half processing on the system.\nCurrently, only two subsystems directly use softirqs:\nNetworking devices Block devices Additionally, kernel timers and tasklets are built on top of softirqs.\nTasklets # Tasklets are a bottom-half mechanism built on top of softirqs. As mentioned, they have nothing to do with tasks. Tasklets are similar in nature and behavior to softirqs, but have a simpler interface and relaxed locking rules.\nImplementing Tasklets # Because tasklets are implemented on top of softirqs, they are softirqs. Tasklets are represented by two of the nine softirqs:\nHI_SOFTIRQ. TASKLET_SOFTIRQ. Since softirqs are sorted by priority, the HI_SOFTIRQ-based tasklets run prior to the TASKLET_SOFTIRQ-based tasklets.\nThe implementation of tasklets is simple, but rather clever:\nAll tasklets are multiplexed on top of two softirqs, HI_SOFTIRQ and TASKLET_SOFTIRQ. When a tasklet is scheduled, the kernel raises one of these softirqs. These softirqs, in turn, are handled by special functions that then run any scheduled tasklets. The special functions ensure that only one tasklet of a given type runs at the same time. However, other tasklets can run simultaneously. Tasklets are dynamically created and, as with softirqs, cannot sleep. You cannot use semaphores or other blocking functions in a tasklet. Tasklets also run with all interrupts enabled, so you must take precautions (for example, disable interrupts and obtain a lock) if your tasklet shares data with an interrupt handler. Unlike softirqs, two of the same tasklets never run concurrently, though two different tasklets can run at the same time on two different processors. If a tasklet shares data with another tasklet or softirq, proper locking need to be used.\nAfter a tasklet is scheduled, it runs once at some time in the near future. If the same tasklet is scheduled again, before it has had a chance to run, it still runs only once. If it is already running, for example on another processor, the tasklet is rescheduled and runs again. As an optimization, a tasklet always runs on the processor that scheduled it, making better use of the processor\u0026rsquo;s cache.\nksoftirqd # Softirq processing is aided by a set of per-processor kernel threads. These kernel threads help in the processing of softirqs when the system is overwhelmed with softirqs. Because tasklets are implemented using softirqs, the following discussion applies equally to softirqs and tasklets.\nAs described, the kernel processes softirqs in a number of places, most commonly on return from handling an interrupt. There are two characteristics with softirqs:\nSoftirqs might be raised at high rates, such as during heavy network traffic. Softirq functions can reactivate themselves. That is, while running, a softirq can raise itself so that it runs again. For example, the networking subsystem\u0026rsquo;s softirq raises itself. The issue is that in an high load environments, in which many softirqs continually reactivate themselves:\nIf the Kernel keep handling softirqs, it might not accomplish much else. Resulting in user-space programs being starved of processor time. If the Kernel stops handling softirqs, it prevents starving user-space, but it does starve the softirqs and does not take good advantage of an idle system. The solution ultimately implemented in the kernel is to not immediately process reactivated softirqs. Instead, if the number of softirqs grows excessive, the kernel wakes up a family of kernel threads to handle the load. The kernel threads run with the lowest possible priority (nice value of 19), which ensures they do not run in lieu of anything important.\nThe advantage it brings are:\nThe concession prevents heavy softirq activity from completely starving user-space of processor time. It also ensures that excessive softirqs do run eventually. On an idle system the softirqs are handled rather quickly because the kernel threads will schedule immediately. There is one thread per processor. Having a thread on each processor ensures an idle processor, if available, can always service softirqs.\nWork Queues # Work queues are a different form of deferring work. Work queues defer work into a kernel thread; this bottom half always runs in process context. Code deferred to a work queue has all the usual benefits of process context. Most importantly, work queues are schedulable and can therefore sleep.\nNormally, it is easy to decide between using work queues and softirqs/tasklets:\nIf the deferred work needs to sleep, work queues are used. If the deferred work need not sleep, softirqs or tasklets are used. If you need a schedulable entity to perform your bottom-half processing, you need work queues. They are the only bottom-half mechanisms that run in process context, and thus the only ones that can sleep. This means they are useful for situations in which you need to allocate a lot of memory, obtain a semaphore, or perform block I/O. If you do not need a kernel thread to handle your deferred work, consider a tasklet instead.\nImplementing Work Queues # In its most basic form, the work queue subsystem is an interface for creating kernel threads to handle work queued from elsewhere. These kernel threads are called worker threads. Work queues enables your driver to create a special worker thread to handle deferred work. The work queue subsystem, however, implements and provides a default worker thread for handling work. Therefore, in its most common form, a work queue is a simple interface for deferring work to a generic kernel thread.\nThe default worker threads are called events/n where n is the processor number; there is one per processor.\nThe default worker thread handles deferred work from multiple locations. Many drivers in the kernel defer their bottom-half work to the default thread. Unless a driver or subsystem has a strong requirement for creating its own thread, the default thread is preferred.\nUsing a dedicated worker thread might be advantageous if the driver performs large amounts of processing in the worker thread. Processor-intense and performance-critical work might benefit from its own thread. This also lightens the load on the default threads, which prevents starving the rest of the queued work.\nTasks for work threads are strung into a linked list, one for each type of queue on each processor. For example, there is one list of deferred work for the generic thread, per processor.\nWhen a worker thread wakes up (the thread\u0026rsquo;s state is set to TASK_RUNNING), it runs any work in its list. A worker thread executes the work in process context. By default, interrupts are enabled and no locks are held. If needed, it can sleep. As it completes work, it removes the corresponding task entries from the linked list. When the list is empty, it goes back to sleep (the thread\u0026rsquo;s state is set to TASK_INTERRUPTIBLE). Despite running in process context, the work handlers cannot access user-space memory because there is no associated user-space memory map for kernel threads. The kernel can access user memory only when running on behalf of a user-space process, such as when executing a system call. Only then is user memory mapped in.\nWhich Bottom Half Should I Use? # Softirqs: least serialization, for highly threaded code # Softirqs, by design, provide the least serialization. This requires softirq handlers to go through extra steps to ensure that shared data is safe because two or more softirqs of the same type may run concurrently on different processors. If the code in question is already highly threaded, such as in a networking subsystem that is chest-deep in per-processor variables, softirqs make a good choice. They are certainly the fastest alternative for timing-critical and high-frequency uses.\nTasklets: simple interface, for less threaded code # Tasklets make more sense if the code is not finely threaded. They have a simpler interface and, because two tasklets of the same type might not run concurrently, they are easier to implement. Tasklets are effectively softirqs that do not run concurrently. A driver developer should always choose tasklets over softirqs, unless prepared to utilize per-processor variables or similar magic to ensure that the softirq can safely run concurrently on multiple processors.\nWork queues: process context # If the deferred work needs to run in process context, the only choice of the three is work queues. If process context is not a requirements (specifically, if you have no need to sleep), softirqs or tasklets are perhaps better suited. Work queues involve the highest overhead because they involve kernel threads and, therefore, context switching. This doesn\u0026rsquo;t mean they are inefficient, but in light of thousands of interrupts hitting per second (as the networking subsystem might experience), other methods make more sense. However, work queues are sufficient for most situations.\nSoftirqs vs. tasklets vs. work queues # In terms of ease of use, work queues wins. Using the default events queue is easy. Next come tasklets, which also have a simple interface. Coming in last are softirqs, which need to be statically created and require careful thinking with their implementation.\nIf the driver need a schedulable entity to perform the deferred work, and if fundamentally, it need to sleep for any reason, then work queues are your only option. Otherwise, tasklets are preferred. Only if scalability becomes a concern the driver could use softirqs.\nThe following table is a comparison between the three bottom-half interfaces.\nBottom Half Context Inherent Serialization Softirq Interrupt None Tasklets Interrupt Against the same tasklet Work Queues Process None (scheduled as process context) Locking Between the Bottom Halves # It is crucial to protect shared data from concurrent access while using bottom halves, even on a single processor machine. A bottom half can run at virtually any moment.\nOne benefit of tasklets is that they are serialized with respect to themselves. The same tasklet will not run concurrently, even on two different processors. This means you do not have to worry about intra-tasklet concurrency issues. Inter-tasklet concurrency (when two different tasklets share the same data) requires proper locking.\nOn the other hand, because softirqs provide no serialization, (even two instances of the same softirq might run simultaneously), all shared data needs an appropriate lock.\nIf process context code and a bottom half share data, you need to do both of the following before accessing the data:\nDisable bottom-half processing. Obtain a lock. If interrupt context code and a bottom half share data, you need to do both of the following before accessing the data:\nDisable interrupts. Obtain a lock. In both cases, this ensures local and SMP protection and prevents a deadlock.\nBecause work queues run in process context, there are no issues with asynchronous execution, and thus, there is no need to disable them. On the other hand, protecting shared data is the same as in any process context and requires locking. However, because softirqs and tasklets can occur asynchronously (for example, on return from handling an interrupt), kernel code may need to disable them.\nThe locking is no different from normal kernel code because work queues run in process context.\n"},{"id":12,"href":"/notes/docs/linux-kernel-development/9-an-introduction-to-kernel-synchronization/","title":"9. An Introduction to Kernel Synchronization","section":"Linux Kernel Development","content":" An Introduction to Kernel Synchronization # In a shared memory application, developers must ensure that shared resources are protected from concurrent access. The kernel is no exception. Symmetrical multiprocessing is supported in Linux. Multiprocessing support implies that kernel code can simultaneously run on two or more processors.\nShared resources require protection from concurrent access because if multiple threads of execution access and manipulate the data at the same time, the threads may overwrite each other\u0026rsquo;s changes or access data while it is in an inconsistent state. Consequently, without protection, code in the kernel, running on two different processors, can simultaneously access shared data at exactly the same time.\nThe term threads of execution implies any instance of executing code. For example, this includes any of the following:\nA task in the kernel An interrupt handler A bottom half A kernel thread Concurrent access of shared data often results in instability is hard to track down and debug.\nThe Linux kernel is preemptive. This implies that (in the absence of protection) the scheduler can preempt kernel code at virtually any point and reschedule another task. A number of scenarios enable for concurrency inside the kernel, and they all require protection.\nCritical Regions and Race Conditions # Code paths that access and manipulate shared data are called critical regions (also called critical sections). It is usually unsafe for multiple threads of execution to access the same resource simultaneously. To prevent concurrent access during critical regions, the programmer must ensure that code executes atomically, which means that operations complete without interruption as if the entire critical region were one indivisible instruction.\nIf two threads of execution simultaneously execute within the same critical region, it is called a race condition, so-named because the threads raced to get there first. Debugging race conditions is often difficult because they are not easily reproducible. Ensuring that unsafe concurrency is prevented and that race conditions do not occur is called synchronization.\nWhy Do We Need Protection? # To best understand the need for synchronization, look at the ubiquity of race conditions. The first example is a real-world case: an ATM (Automated Teller Machine, called a cash machine).\nThe ATM works as follows:\nCheck whether the deduction is possible. Compute the new total funds. Finally execute the physical deduction. Assuming a user, with a $100 in the bank, and user\u0026rsquo;s spouse are initiating withdrawal of $75.\nBoth the user and user\u0026rsquo;s spouse initiate withdrawal at the same time. Both transactions verify that sufficient funds exist, in both cases, the user has $100 in the bank for the withdrawal of $75. In both transactions, the new computed total fund would be $25. In both cases, the users would be getting $75, for a total of $150, while still having in the bank $25. This means that the user funds would increase from $100 to $175. Clearly, financial institutions must ensure that this can never happen. They must lock the account during certain operations, making each transaction atomic with respect to any other transaction. Such transactions must occur in their entirety, without interruption, or not occur at all.\nThe Single Variable # Consider a simple shared resource, a single global integer, and a simple critical region, the operation of merely incrementing it: i++.\nThis might translate into machine instructions to the computer\u0026rsquo;s processor that resemble the following:\nGet the current value of i and copy it into a register. Add one to the value stored in the register. Write back to memory the new value of i. Assume that there are two threads of execution, both enter this critical region, and the initial value of i is 7. The desired outcome is then similar to the following (with each row representing a unit of time):\nThread 1 Thread 2 get i (7) — increment i (7 -\u0026gt; 8) — write back i (8) — — get i (8) — increment i (8 -\u0026gt; 9) — write back i (9) As expected, 7 incremented twice is 9.\nHowever, another possible outcome is the following:\nThread 1 Thread 2 get i (7) get i (7) increment i (7 -\u0026gt; 8) — — increment i (7 -\u0026gt; 8) write back i (8) — — write back i (9) If both threads of execution read the initial value of i before it is incremented, both threads increment and save the same value. As a result, the variable i contains the value 8 when, in fact, it should now contain 9. This is one of the simplest examples of a critical region. The solution is simple. We merely need a way to perform these operations in one indivisible step. Most processors provide an instruction to atomically read, increment, and write back a single variable.\nUsing this atomic instruction, the only possible outcome is:\nThread 1 Thread 2 increment \u0026amp; store i (7 -\u0026gt; 8) — — increment \u0026amp; store i (8 -\u0026gt; 9) Thread 2 could also be incrementing i first, but the result would be the same.\nIt would never be possible for the two atomic operations to interleave. The processor would physically ensure that it was impossible. Using such an instruction would alleviate the problem. The kernel provides a set of interfaces that implement these atomic instructions, which are discussed in the next chapter.\nLocking # Assuming a queue of requests that needs to be serviced and the implementation is a linked list, in which each node represents a request. Two functions manipulate the queue:\nOne function adds a new request to the tail of the queue. One function removes a request from the head of the queue and service request. Requests are continually being added, removed, and serviced, since various parts of the kernel invoke these two functions. Manipulating the request queues certainly requires multiple instructions. If one thread attempts to read from the queue while another is in the middle of manipulating it, the reading thread will find the queue in an inconsistent state. It should be apparent the sort of damage that could occur if access to the queue could occur concurrently. Often, when the shared resource is a complex data structure, the result of a race condition is corruption of the data structure.\nAlthough it is feasible for a particular architecture to implement simple instructions, such as arithmetic and comparison, atomically it is ludicrous for architectures to provide instructions to support the indefinitely sized critical regions that would exist in the example.\nWhat is needed is a way of making sure that only one thread manipulates the data structure at a time, a mechanism for preventing access to a resource while another thread of execution is in the marked region. A lock provides such a mechanism. Threads hold locks; locks protect data.\nWhenever there was a new request to add to the queue, the thread would first obtain the lock. Then it could safely add the request to the queue and ultimately release the lock. When a thread wanted to remove a request from the queue, it would also obtain the lock. Then it could read the request and remove it from the queue. Finally, it would release the lock. Any other access to the queue would similarly need to obtain the lock. Because the lock can be held by only one thread at a time, only a single thread can manipulate the queue at a time. If a thread comes along while another thread is already updating it, the second thread has to wait for the first to release the lock before it can continue. The lock prevents concurrency and protects the queue from race conditions.\nThread 1 Thread 2 try to lock the queue try to lock the queue succeeded: acquired lock failed: waiting\u0026hellip; access queue\u0026hellip; waiting\u0026hellip; unlock the queue waiting\u0026hellip; — succeeded: acquired lock — access queue\u0026hellip; — unlock the queue Notice that locks are advisory and voluntary. Locks are entirely a programming construct that the programmer must take advantage of. Nothing prevents you from writing code that manipulates the fictional queue without the appropriate lock, but such a practice would eventually result in a race condition and corruption.\nLocks come in various shapes and sizes. Linux alone implements a handful of different locking mechanisms. The most significant difference between the various mechanisms is the behavior when the lock is unavailable because another thread already holds it:\nSome lock variants busy wait (spin in a tight loop, checking the status of the lock over and over, waiting for the lock to become available). Other locks put the current task to sleep until the lock becomes available. The itself lock does not solve the problem; it simply shrinks the critical region down to just the lock and unlock code: probably much smaller, but still a potential race. What if a lock is aquired by two threads at the exact same time?\nFortunately, locks are implemented using atomic operations that ensure no race exists. A single instruction can verify whether the key is taken and, if not, seize it. How this is done is architecture-specific, but almost all processors implement an atomic test and set instruction that tests the value of an integer and sets it to a new value only if it is zero. A value of zero means unlocked.\nCauses of Concurrency # In user-space, programs are scheduled preemptively at the will of the scheduler. Because a process can be preempted at any time and another process can be scheduled onto the processor, a process can be involuntarily preempted in the middle of accessing a critical region. If the newly scheduled process then enters the same critical region (for example, if the two processes manipulate the same shared memory or write to the same file descriptor), a race can occur. The same problem can occur with multiple single-threaded processes sharing files, or within a single program with signals, because signals can occur asynchronously. This type of concurrency in which two things do not actually happen at the same time but interleave with each other is called pseudo-concurrency.\nWith a symmetrical multiprocessing machine (multiple processors or cores), two processes can actually be executed in a critical region at the exact same time. That is called true concurrency. Although the causes and semantics of true versus pseudo concurrency are different, they both result in the same race conditions and require the same sort of protection.\nThe kernel has similar causes of concurrency:\nInterrupts: An interrupt can occur asynchronously at almost any time, interrupting the currently executing code. Softirqs and tasklets: The kernel can raise or schedule a softirq or tasklet at almost any time, interrupting the currently executing code. Kernel preemption: Because the kernel is preemptive, one task in the kernel can preempt another. Sleeping and synchronization with user-space: A task in the kernel can sleep and thus invoke the scheduler, resulting in the running of a new process. Symmetrical multiprocessing: Two or more processors can execute kernel code at exactly the same time. This is the reason locking and synchronization mechanisms are needed.\nDeadlocks # A deadlock is a condition involving one or more threads of execution and one or more resources, such that each thread waits for one of the resources, but all the resources are already held.\nThe threads all wait for each other, but they never make any progress toward releasing the resources that they already hold. Therefore, none of the threads can continue, which results in a deadlock.\nA good analogy is a four-way traffic stop. If each car at the stop decides to wait for the other cars before going, no car will ever proceed, and we have a traffic deadlock.\nThe most common example is with two threads and two locks, which is often called the deadly embrace or the ABBA deadlock:\nThread 1 Thread 2 acquire lock A acquire lock B try to acquire lock B try to acquire lock A wait for lock B wait for lock A Each thread is waiting for the other, and neither thread will ever release its original lock; therefore, neither lock will become available.\nPrevention of deadlock scenarios is important. Although it is difficult to prove that code is free of deadlocks, the following rules can help in writing a deadlock-free code:\nImplement lock ordering. Nested locks must always be obtained in the same order. This prevents the deadly embrace deadlock. Prevent starvation. Does this code always finish? If foo does not occur, will bar wait forever? Do not double acquire the same lock. Design for simplicity. Complexity locking schemes invites deadlocks. The first point is most important and worth stressing. If two or more locks are acquired at the same time, they must always be acquired in the same order. The order of unlock does not matter with respect to deadlock, although it is common practice to release the locks in an order inverse to that in which they were acquired.\nPreventing deadlocks is important. The Linux kernel has some basic debugging facilities for detecting deadlock scenarios in a running kernel, which are discussed in the next chapter.\nContention and Scalability # The term lock contention, or simply contention, describes a lock currently in use but that another thread is trying to acquire. A lock that is highly contended often has threads waiting to acquire it. High contention can occur because a lock is frequently obtained, held for a long time after it is obtained, or both. Because a lock\u0026rsquo;s job is to serialize access to a resource, they can slow down a system\u0026rsquo;s performance. A highly contended lock can become a bottleneck in the system, quickly limiting its performance. However, a solution to high contention must continue to provide the necessary concurrency protection, because locks are also required to prevent the system from tearing itself to shreds.\nScalability is a measurement of how well a system can be expanded. In operating systems, we talk of the scalability with a large number of processes, a large number of processors, or large amounts of memory. We can discuss scalability in relation to virtually any component of a computer to which we can attach a quantity. Ideally, doubling the number of processors should result in a doubling of the system\u0026rsquo;s processor performance, which, of course, is never the case.\nThe granularity of locking is a description of the size or amount of data that a lock protects:\nA very coarse lock protects a large amount of data, e.g. an entire subsystem’s set of data structures. On the other hand, a very fine-grained lock protects a small amount of data, e.g. only a single element in a larger structure. Scalability improvement is generally a good thing because it improves Linux\u0026rsquo;s performance on larger and more powerful systems. However, rampant scalability \u0026ldquo;improvements\u0026rdquo; can lead to a decrease in performance on smaller SMP and UP machines, because smaller machines may not need such fine-grained locking but will nonetheless need to put up with the increased complexity and overhead.\n"},{"id":13,"href":"/notes/docs/linux-kernel-development/10-kernel-synchronization-methods/","title":"10. Kernel Synchronization Methods","section":"Linux Kernel Development","content":" Kernel Synchronization Methods # The previous chapter discussed the sources of and solutions to race conditions. The Linux kernel provides a family of synchronization methods, which enable developers to write efficient and race-free code. This chapter discusses these methods and their interfaces, behavior, and use.\nAtomic Operations # As the foundation on which other synchronization methods are built, atomic operations provide instructions that execute atomically, without interruption. Atomic operators are indivisible instructions. For example, an atomic increment can read and increment a variable by one in a single indivisible and uninterruptible step.\nThe kernel provides two sets of interfaces for atomic operations: one that operates on integers and another that operates on individual bits. These interfaces are implemented on every architecture that Linux supports. Most architectures contain instructions that provide atomic versions of simple arithmetic operations. Other architectures, lacking direct atomic operations, provide an operation to lock the memory bus for a single operation, thus guaranteeing that another memory-affecting operation cannot occur simultaneously.\nAtomicity Versus Ordering # Atomicity and Ordering are not the same. As discussed, an atomic read (supported by most architectures) always occurs atomically. It never interleaves with a write to the same word. The read always returns the word in a consistent state: perhaps before the write completes, perhaps after, but never during. For example, if an integer is initially 42 and then set to 365, a read on the integer always returns 42 or 365 and never some commingling of the two values. We call this atomicity.\nHowever, your code might have more stringent requirements than this. Perhaps you require that the read always occurs before the pending write. This type of requirement is not atomicity, but ordering.\nAtomicity ensures that instructions occur without interruption and that they complete either in their entirety or not at all. Ordering, on the other hand, ensures that the desired, relative ordering of two or more instructions (even if they are to occur in separate threads of execution or even separate processors) is preserved. The atomic operations discussed in this section guarantee only atomicity. Ordering is enforced via barrier operations, which is discussed later in this chapter.\nIt is usually preferred to choose atomic operations over more complicated locking mechanisms. On most architectures, one or two atomic operations incur less overhead and less cache-line thrashing than a more complicated synchronization method.\nAtomic Integer Operations # The atomic integer methods operate on a special data type, atomic_t for 32-bit architectures and atomic64_t for 64-bit architectures. Usage is exactly the same, except that the usable range of the integer is 32 bits for one and 64 bits for the other. This special type is used, as opposed to having the functions work directly on the C int type, for several reasons:\nHaving the atomic functions accept only the atomic_t type ensures that the atomic operations are used only with these special types. Likewise, it also ensures that the data types are not passed to any non-atomic functions. The use of atomic_t ensures the compiler does not (erroneously but cleverly) optimize access to the value—it is important the atomic operations receive the correct memory address and not an alias. Use of atomic_t can hide any architecture-specific differences in its implementation. The atomic_t data type has several mechanisms to help developers:\natomic_read() to convert from atomic_t to int. atomic_inc() and atomic_dec() to implement simple counters without having to use locking scheme. Atomic Bitwise Operations # In addition to atomic integer operations, the kernel also provides a family of functions that operate at the bit level, which are architecture-specific.\nThe bitwise functions operates on generic memory addresses. The arguments are a pointer and a bit number. Bit zero is the least significant bit of the given address. On 32-bit machines, bit 31 is the most significant bit, and bit 32 is the least significant bit of the following word. There are no limitations on the bit number supplied; although, most uses of the functions provide a word and, consequently, a bit number between 0 and 31 on 32-bit machines and 0 and 63 on 64-bit machines.\nBecause the functions operate on a generic pointer, there is no equivalent of the atomic integer\u0026rsquo;s atomic_t type. Instead, the functions work with a pointer to whatever data in memory.\nConveniently, nonatomic versions of all the bitwise functions are also provided. They behave identically to their atomic siblings, except they do not guarantee atomicity. These variants of the bitwise functions might be faster.\nIf issuing two atomic bit operations, setting and then unsetting a bit, atomicity requires that either instructions succeed in their entirety, uninterrupted, or instructions fail to execute at all. Moreover, however, at some point in time prior to the final operation, the bit needs to hold the value as specified by the first operation. Real atomicity requires that all intermediate states be correctly realized.\nWithout atomic operations, the bit might end up cleared, but it might never have been set. The set operation could occur simultaneously with the clear operation and fail. The clear operation would succeed, and the bit would emerge cleared as intended. With atomic operations, however, the set would actually occur: there would be a moment in time when a read would show the bit as set, and then the clear would execute and the bit would be zero.\nThis behavior can be important, especially when ordering comes into play or when dealing with hardware registers.\nSpin Locks # Critical regions can span multiple functions. For example, it is often the case that data must be removed from one structure, formatted and parsed, and added to another structure. This entire operation must occur atomically; it must not be possible for other code to read from or write to either structure before the update is completed.\nBecause simple atomic operations are clearly incapable of providing the needed protection in such a complex scenario, a more general method of synchronization is needed: locks.\nThe most common lock in the Linux kernel is the spin lock. A spin lock is a lock that can be held by at most one thread of execution. If a thread of execution attempts to acquire a spin lock while it is already held, which is called contended, the thread busy loops (spins) waiting for the lock to become available. If the lock is not contended, the thread can immediately acquire the lock and continue. The spinning prevents more than one thread of execution from entering the critical region at any one time. The same lock can be used in multiple locations, so all access to a given data structure, for example, can be protected and synchronized.\nThread 1 Thread 2 try to acquire spin lock — succeeded: acquired lock — working\u0026hellip; try to acquire spin lock working\u0026hellip; waiting for lock\u0026hellip; working\u0026hellip; waiting for lock\u0026hellip; release spin lock succeeded: acquired lock — working\u0026hellip; — release spin lock Since a spin lock causes threads to spin (essentially wasting processor time) while waiting for the lock to become available, it is not wise to hold a spin lock for a long time. This is the nature of the spin lock: a lightweight single-holder lock that should be held for short durations.\nAn alternative behavior when the lock is contended is to put the current thread to sleep and wake it up when it becomes available. This incurs a bit of overhead, most notably the two context switches required to switch out of and back into the blocking thread.\nSemaphores do provide a lock that makes the waiting thread sleep, rather than spin, when contended.\nIf a thread attempt to acquire a spin lock it already hold, it will spin, waiting for itself to release the spin lock, effectively causing a deadlock. Spin locks can be used in interrupt handlers, whereas semaphores cannot be used because they sleep. If a lock is used in an interrupt handler, local interrupts (interrupt requests on the current processor) must also be disabled before obtaining the lock. Otherwise, it is possible for an interrupt handler to interrupt kernel code while the lock is held and attempt to reacquire the lock. The interrupt handler spins, waiting for the lock to become available. The lock holder, however, does not run until the interrupt handler completes.\nSpin Locks and Bottom Halves # Certain locking precautions must be taken when working with bottom halves.\nBecause a bottom half might preempt process context code, if data is shared between a bottom-half process context, data in process context must be protected with both a lock and the disabling of bottom halves. Because an interrupt handler might preempt a bottom half, if data is shared between an interrupt handler and a bottom half, it must be protected with both a lock and the disabling of interrupts. Reader-Writer Spin Locks # Sometimes, lock usage can be clearly divided into reader and writer paths.\nFor example, considering a list that is both updated and searched.\nWhen the list is updated (written to), it is important that no other threads of execution concurrently write to or read from the list. Writing demands mutual exclusion. When the list is searched (read from), it is only important that nothing else writes to the list. Multiple concurrent readers are safe so long as there are no writers. The task list’s access patterns (discussed in Chapter 3,“Process Management”) fit this description. Not surprisingly, a reader-writer spin lock protects the task list.\nWhen a data structure is neatly split into reader/writer or consumer/producer usage patterns, it makes sense to use a locking mechanism that provides similar semantics. To satisfy this use, the Linux kernel provides reader-writer spin locks. Reader-writer spin locks provide separate reader and writer variants of the lock.\nOne or more readers can concurrently hold the reader lock. The writer lock, conversely, can be held by at most one writer with no concurrent readers. Reader/writer locks are sometimes called shared/exclusive or concurrent/exclusive locks because the lock is available in a shared (for readers) and an exclusive (for writers) form. Usage is similar to spin locks.\nAn important consideration in using the Linux reader-writer spin locks is that they favor readers over writers. If the read lock is held and a writer is waiting for exclusive access, readers that attempt to acquire the lock continue to succeed. The spinning writer does not acquire the lock until all readers release the lock. Therefore, a sufficient number of readers can starve pending writers. Sometimes this behavior is beneficial, sometimes it is catastrophic.\nSpin locks provide a quick and simple lock. The spinning behavior is optimal for short hold times and code that cannot sleep (interrupt handlers, for example). In cases where the sleep time might be long or you potentially need to sleep while holding the lock, the semaphore is a solution.\nSemaphores # Semaphores in Linux are sleeping locks. When a task attempts to acquire a semaphore that is unavailable, the semaphore places the task onto a wait queue and puts the task to sleep. The processor is then free to execute other code. When the semaphore becomes available, one of the tasks on the wait queue is awakened so that it can then acquire the semaphore.\nBecause the contending tasks sleep while waiting for the lock to become available, semaphores are well suited to locks that are held for a long time. Conversely, semaphores are not optimal for locks that are held for short periods because the overhead of sleeping, maintaining the wait queue, and waking back up can easily outweigh the total lock hold time. Because a thread of execution sleeps on lock contention, semaphores must be obtained only in process context because interrupt context is not schedulable. A thread can sleep while holding a semaphore because it will not deadlock when another process acquires the same semaphore. It will just go to sleep and eventually let the first thread continue. A thread cannot hold a spin lock while it acquire a semaphore, because it might have to sleep while waiting for the semaphore, and it cannot sleep while holding a spin lock. These facts highlight the uses of semaphores versus spin locks. In most uses of semaphores, there is little choice as to what lock to use.\nIf the code needs to sleep, which is often the case when synchronizing with user-space, semaphores are the sole solution. It is often easier, if not necessary, to use semaphores because they allow the flexibility of sleeping.\nWhen the code does not need to sleep, the decision between semaphore and spin lock should be based on lock hold time. Ideally, all your locks should be held as briefly as possible. With semaphores, however, longer lock hold times are more acceptable. Additionally, unlike spin locks, semaphores do not disable kernel preemption and, consequently, code holding a semaphore can be preempted. This means semaphores do not adversely affect scheduling latency.\nCounting and Binary Semaphores # A final useful feature of semaphores is that they can allow for an arbitrary number of simultaneous lock holders. Whereas spin locks permit at most one task to hold the lock at a time, the number of permissible simultaneous holders of semaphores can be set at declaration time. This value is called the usage count or simply the count.\nThe most common value is to allow, like spin locks, only one lock holder at a time.\nIn this case, the count is equal to one, and the semaphore is called either a binary semaphore (because it is either held by one task or not held at all) or a mutex (because it enforces mutual exclusion). Alternatively, the count can be initialized to a nonzero value greater than one.\nIn this case, the semaphore is called a counting semaphore, and it enables at most count holders of the lock at a time. Counting semaphores are not used to enforce mutual exclusion because they enable multiple threads of execution in the critical region at once. Instead, they are used to enforce limits in certain code. They are not used much in the kernel. When using a semaphore, it is almost assuredly wanted to use a mutex (a semaphore with a count of one).\nSemaphores were formalized by Edsger Wybe Dijkstra in 1968 as a generalized locking mechanism. A semaphore supports two atomic operations, P() and V(), named after the Dutch word Proberen, to test (literally, to probe), and the Dutch word Verhogen, to increment.\nLater systems called these methods down() and up(), respectively, and so does Linux.\nThe down() method is used to acquire a semaphore by decrementing the count by one. You down a semaphore to acquire it. If the new count is zero or greater, the lock is acquired and the task can enter the critical region. If the count is negative, the task is placed on a wait queue, and the processor moves on to something else. The up() method is used to release a semaphore upon completion of a critical region, the method increments the count value. You up a semaphore to release it. If the semaphore’s wait queue is not empty, one of the waiting tasks is awakened and allowed to acquire the semaphore. Using Semaphores # The function down_interruptible(), which if the semaphore is unavailable, places the calling process to sleep in the TASK_INTERRUPTIBLE state. If the task receives a signal while waiting for the semaphore, it is awakened and down_interruptible() returns -EINTR. The function down(), which if the semaphore is unavailable, places the calling process to sleep in the TASK_UNINTERRUPTIBLE state. The process will not respond to signals, which can be an issue. When in TASK_INTERRUPTIBLE, if the task receives a signal while waiting for the semaphore, it is awakened and down_interruptible() returns -EINTR.\nIt is also possible to use down_trylock() to try to acquire the given semaphore without blocking. If the semaphore is already held, the function immediately returns nonzero. Otherwise, it returns zero and the process successfully hold the lock.\nFinally, to release a given semaphore, the process must call up().\nReader-Writer Semaphores # Semaphores, like spin locks, also come in a reader-writer flavor. The situations where reader-writer semaphores are preferred over standard semaphores are the same as with reader-writer spin locks versus standard spin locks.\nAll reader-writer semaphores are mutexes, their usage count is one, although they enforce mutual exclusion only for writers, not readers. Any number of readers can concurrently hold the read lock, so long as there are no writers. Conversely, only a sole writer (with no readers) can acquire the write variant of the lock. All reader-writer locks use uninterruptible sleep.\nAs with normal semaphores, it is also possible to try to acquire the given semaphore without blocking, either as a reader or as a writer. In both cases, the semaphore down() function return nonzero if the lock is successfully acquired and zero if it is currently contended. For admittedly no good reason, this is the opposite of normal semaphore behavior!\nMutexes # Until recently, the only sleeping lock in the kernel was the semaphore. Most users of semaphores instantiated a semaphore with a count of one and treated them as a mutual exclusion lock, a sleeping version of the spin lock. Unfortunately, semaphores are rather generic and do not impose many usage constraints.\nThis makes them useful for managing exclusive access in complicated situations. But it also means that simpler locking is harder to do, and the lack of enforced rules makes any sort of automated debugging or constraint enforcement impossible. Seeking a simpler sleeping lock, the kernel developers introduced the mutex.\nThe term “mutex” is a generic name to refer to any sleeping lock that enforces mutual exclusion, such as a semaphore with a usage count of one. In recent Linux kernels, the proper noun “mutex” is now also a specific type of sleeping lock that implements mutual exclusion.That is, a mutex is a mutex. The mutex behaves similar to a semaphore with a count of one, but it has a simpler interface, more efficient performance, and additional constraints on its use.\nThe simplicity and efficiency of the mutex comes from the additional constraints it imposes on its users over and above what the semaphore requires.\nUnlike a semaphore, which implements the most basic of behavior in accordance with Dijkstra’s original design, the mutex has a stricter, narrower use case:\nOnly one task can hold the mutex at a time. That is, the usage count on a mutex is always one. Whoever locked a mutex must unlock it. That is, you cannot lock a mutex in one context and then unlock it in another. This means that the mutex isn’t suitable for more complicated synchronizations between kernel and user-space. Most use cases, however, cleanly lock and unlock from the same context. Recursive locks and unlocks are not allowed. That is, you cannot recursively acquire the same mutex, and you cannot unlock an unlocked mutex. A process cannot exit while holding a mutex. A mutex cannot be acquired by an interrupt handler or bottom half. A mutex can be managed only via the official API: It must be initialized via the methods described in this section and cannot be copied, hand initialized, or reinitialized. Perhaps the most useful aspect of the mutex is that, via a special debugging mode, the kernel can programmatically check for and warn about violations of these constraints.\nSemaphores Versus Mutexes # Mutexes and semaphores are similar. Having both in the kernel is confusing. Thankfully, the formula dictating which to use is quite simple: Unless one of mutex’s additional constraints prevent you from using them, prefer the new mutex type to semaphores. When writing new code, only specific, often low-level, uses need a semaphore. Start with a mutex and move to a semaphore only if you run into one of their constraints and have no other alternative.\nSpin Locks Versus Mutexes # Knowing when to use a spin lock versus a mutex (or semaphore) is important to writing optimal code. In many cases, however, there is little choice. Only a spin lock can be used in interrupt context, whereas only a mutex can be held while a task sleeps.\nWhat to Use: Spin Locks Versus Semaphores:\nRequirement Recommended Lock Low overhead locking Spin lock is preferred. Short lock hold time Spin lock is preferred. Long lock hold time Mutex is preferred. Need to lock from interrupt context Spin lock is preferred. Need to sleep while holding lock Mutex is preferred. Completion Variables # Using completion variables is an easy way to synchronize between two tasks in the kernel when one task needs to signal to the other that an event has occurred.\nOne task waits on the completion variable while another task performs some work. When the other task has completed the work, it uses the completion variable to wake up any waiting tasks. Completion variables merely provide a simple solution to a problem whose answer is otherwise semaphores.\nBKL: The Big Kernel Lock # The Big Kernel Lock (BKL) is a global spin lock that was created to ease the transition from Linux’s original SMP implementation to fine-grained locking. It is not used anymore and has been removed from the Kernel since version 2.6.\nSequential Locks # The sequential lock, generally shortened to seq lock, is a newer type of lock. It provides a simple mechanism for reading and writing shared data. It works by maintaining a sequence counter. Whenever the data in question is written to, a lock is obtained and a sequence number is incremented.\nPrior to and after reading the data, the sequence number is read. If the values are the same, a write did not begin in the middle of the read. Further, if the values are even, a write is not underway. Grabbing the write lock makes the value odd, whereas releasing it makes it even because the lock starts at zero.\nSeq locks are useful to provide a lightweight and scalable lock for use with many readers and a few writers. Seq locks, however, favor writers over readers. An acquisition of the write lock always succeeds as long as there are no other writers. Readers do not affect the write lock, as is the case with reader-writer spin locks and semaphores. Furthermore, pending writers continually cause the read loop to repeat, until there are no longer any writers holding the lock.\nSeq locks are ideal when your locking needs meet most or all these requirements:\nYour data has a lot of readers. Your data has few writers. Although few in number, you want to favor writers over readers and never allow readers to starve writers. Your data is simple, such as a simple structure or even a single integer that, for whatever reason, cannot be made atomic. A prominent user of the seq lock is jiffies, the variable that stores a Linux machine’s uptime. Jiffies holds a 64-bit count of the number of clock ticks since the machine booted. On machines that cannot atomically read the full 64-bit jiffies_64 variable, jiffies is implemented using seq locks.\nPreemption Disabling # Because the kernel is preemptive, a process in the kernel can stop running at any instant to enable a process of higher priority to run. This means a task can begin running in the same critical region as a task that was preempted. To prevent this, the kernel preemption code uses spin locks as markers of nonpreemptive regions. If a spin lock is held, the kernel is not preemptive. Because the concurrency issues with kernel preemption and SMP are the same, and the kernel is already SMP-safe; this simple change makes the kernel preempt-safe, too.\nIn reality, some situations do not require a spin lock, but do need kernel preemption disabled.The most frequent of these situations is per-processor data. If the data is unique to each processor, there might be no need to protect it with a lock because only that one processor can access the data. If no spin locks are held, the kernel is preemptive, and it would be possible for a newly scheduled task to access this same variable, as shown here:\nTask A Task B task A manipulates per-processor variable foo, which is not protected by a lock — task A is preempted — — task B is scheduled — task B manipulates variable foo — task B completes task A is rescheduled — task A continues manipulating variable foo — Consequently, even if this were a uniprocessor computer, the variable could be accessed pseudo-concurrently by multiple processes. Normally, this variable would require a spin lock (to prevent true concurrency on multiprocessing machines). If this were a perprocessor variable, however, it might not require a lock.\nTo solve this, kernel preemption can be disabled via preempt_disable(). The call is nestable; it can be called any number of times. For each call, a corresponding call to preempt_enable() is required. The final corresponding call to preempt_enable() reenables preemption.\nThe preemption count stores the number of held locks and preempt_disable() calls.\nIf the number is zero, the kernel is preemptive. If the value is one or greater, the kernel is not preemptive. Ordering and Barriers # When dealing with synchronization between multiple processors or with hardware devices, it is sometimes a requirement that memory-reads (loads) and memory-writes (stores) issue in the order specified in your program code.\nWhen talking with hardware, you often need to ensure that a given read occurs before another read or write. Additionally, on symmetrical multiprocessing systems, it might be important for writes to appear in the order that the code issues them (usually to ensure subsequent reads see the data in the same order). Complicating these issues is the fact that both the compiler and the processor can reorder reads and writes for performance reasons. Thankfully, all processors that do reorder reads or writes provide machine instructions to enforce ordering requirements. It is also possible to instruct the compiler not to reorder instructions around a given point.\nThese instructions are called barriers.\nEssentially, on some processors the following code may allow the processor to store the new value in b before it stores the new value in a:\na = 1; b = 2; Both the compiler and processor see no relation between a and b. The compiler would perform this reordering at compile time; the reordering would be static, and the resulting object code would simply set b before a. The processor, however, could perform the reordering dynamically during execution by fetching and dispatching seemingly unrelated instructions in whatever order it feels is best. The vast majority of the time, such reordering is optimal because there is no apparent relation between a and b. Sometimes the programmer knows best, though.\nAlthough the previous example might be reordered, the processor would never reorder writes such as the following because there is clearly a data dependency between a and b:\na = 1; b = a; Neither the compiler nor the processor, however, knows about code in other contexts. Occasionally, it is important that writes are seen by other code and the outside world in the specific order that was intend. This is often the case with hardware devices but is also common on multiprocessing machines.\nThe rmb() method provides a read memory barrier. It ensures that no loads are reordered across the rmb() call. That is, no loads prior to the call will be reordered to after the call, and no loads after the call will be reordered to before the call. The wmb() method provides a write barrier. It functions in the same manner as rmb(), but with respect to stores instead of loads, it ensures no stores are reordered across the barrier. The mb() call provides both a read barrier and a write barrier. No loads or stores will be reordered across a call to mb(). It is provided because a single instruction (often the same instruction used by rmb()) can provide both the load and store barrier. A variant of rmb(), read_barrier_depends(), provides a read barrier but only for loads on which subsequent loads depend. All reads prior to the barrier are guaranteed to complete before any reads after the barrier that depend on the reads prior to the barrier. Basically, it enforces a read barrier, similar to rmb(), but only for certain reads—those that depend on each other. On some architectures, read_barrier_depends() is much quicker than rmb() because it is not needed and is, thus, a noop.\nThread 1 Thread 2 a = 3; — mb(); — b = 4; c = b; — rmb(); — d = a; Without using the memory barriers, on some processors it is possible for c to receive the new value of b, whereas d receives the old value of a. For example, c could equal four (what you’d expect), yet d could equal one (not what you’d expect). Using the mb() ensured that a and b were written in the intended order, whereas the rmb() insured c and d were read in the intended order.\nThis sort of reordering occurs because modern processors dispatch and commit instructions out of order, to optimize use of their pipelines. What can end up happening in the previous example is that the instructions associated with the loads of b and a occur out of order.\nThe rmb() and wmb() functions correspond to instructions that tell the processor to commit any pending load or store instructions, respectively, before continuing.\nThe barrier() method prevents the compiler from optimizing loads or stores across the call. The compiler knows not to rearrange stores and loads in ways that would change the effect of the C code and existing data dependencies. It does not have knowledge, however, of events that can occur outside the current context. For example, the compiler cannot know about interrupts that might read the same data you are writing, you might want to ensure a store is issued before a load. The previous memory barriers also function as compiler barriers, but a compiler barrier is much lighter in weight than a memory barrier. Indeed, a compiler barrier is practically free, because it simply prevents the compiler from possibly rearranging things.\nNote that the actual effects of the barriers vary for each architecture. For example, if a machine does not perform out-of-order stores (for example, Intel x86 processors do not), wmb() does nothing. You can use the appropriate memory barrier for the worst case (that is, the weakest ordering processor) and your code will compile optimally for your architecture.\n"},{"id":14,"href":"/notes/docs/linux-kernel-development/11-timers-and-time-management/","title":"11. Timers and Time Management","section":"Linux Kernel Development","content":" Timers and Time Management # "},{"id":15,"href":"/notes/docs/linux-kernel-development/12-memory-management/","title":"12. Memory Management","section":"Linux Kernel Development","content":" Memory Management # Unlike user-space, the kernel is not always afforded the capability to easily allocate memory. For example, the kernel cannot easily deal with memory allocation errors, and the kernel often cannot sleep. Because of these limitations, and the need for a lightweight memory allocation scheme, getting hold of memory in the kernel is more complicated than in user-space.\nPages # The kernel treats physical pages as the basic unit of memory management. Although the processor’s smallest addressable unit is a byte or a word, the memory management unit (MMU, the hardware that manages memory and performs virtual to physical address translations) typically deals in pages. Therefore, the MMU maintains the system’s page tables with page-sized granularity (hence their name). In terms of virtual memory, pages are the smallest unit that matters.\nEach architecture defines its own page size. Many architectures even support multiple page sizes.\nMost 32-bit architectures have 4KB pages. Most 64-bit architectures have 8KB pages. The kernel represents every physical page on the system with a struct page structure. The kernel uses this structure to keep track of all the pages in the system, because the kernel needs to know whether a page is free (whether the page is not allocated). If a page is not free, the kernel needs to know who owns the page. Possible owners include (but not limited to):\nUser-space processes. Dynamically allocated kernel data. Static kernel code. Page cache. The page structure is associated with physical pages, not virtual pages; what the structure describes is transient at best. Even if the data contained in the page continues to exist, it might not always be associated with the same page structure because of swapping and so on. The kernel uses this data structure to describe the associated physical page. The data structure’s goal is to describe physical memory, not the data contained therein.\nSince an instance of this structure is allocated for each physical page in the system, how much space is used to maintain the pages structures? With each struct page consuming 40 bytes of memory and assuming that the system has 8KB physical pages and has 4GB of physical memory. In that case, there are about 524,288 pages and page structures on the system. The page structures consume 20MB: perhaps a surprisingly large number in absolute terms, but only a small fraction of a percent relative to the system’s 4GB. This is not too high a cost for managing all the system’s physical pages.\nZones # The kernel cannot treat all pages as identical due to hardware limitations. Some pages, because of their physical address in memory, cannot be used for certain tasks. Thus, the kernel divides pages into different zones. The kernel uses the zones to group pages of similar properties.\nLinux has to deal with two shortcomings of hardware with respect to memory addressing:\nSome hardware devices can perform DMA (direct memory access) to only certain memory addresses. Some architectures can physically addressing larger amounts of memory than they can virtually address. Consequently, some memory is not permanently mapped into the kernel address space. Due to these contraints, Linux has four primary memory zones:\nZONE_DMA: This zone contains pages that can undergo DMA. ZONE_DMA32: Like ZONE_DMA, this zone contains pages that can undergo DMA. Unlike ZONE_DMA, these pages are accessible only by 32-bit devices. On some architectures, this zone is a larger subset of memory. ZONE_NORMAL: This zone contains normal, regularly mapped, pages. ZONE_HIGHMEM: This zone contains \u0026ldquo;high memory\u0026rdquo;, which are pages not permanently mapped into the kernel’s address space. The layout of the memory zones is architecture-dependent.\nSome architectures can perform DMA into any memory address. In those architectures, ZONE_DMA is empty and ZONE_NORMAL is used for allocations regardless of their use. On the x86 architecture, ISA devices cannot perform DMA into the full 32-bit address space because ISA devices can access only the first 16MB of physical memory. Consequently, ZONE_DMA on x86 consists of all memory in the range 0MB–16MB. ZONE_HIGHMEM works similarly.\nOn 32-bit x86 systems, ZONE_HIGHMEM is all memory above the physical 896MB mark. On other architectures, ZONE_HIGHMEM is empty because all memory is directly mapped. The memory contained in ZONE_HIGHMEM is called high memory. The rest of the system’s memory is called low memory. ZONE_NORMAL is the remainder after the previous two zones claim their requisite shares. On x86, ZONE_NORMAL is all physical memory from 16MB to 896MB. On other architectures, ZONE_NORMAL is all available memory. The following table is a listing of each zone and its consumed pages on x86-32.\nZone Description Physical Memory ZONE_DMA DMA-able pages \u0026lt; 16MB ZONE_NORMAL Normally addressable pages 16–896MB ZONE_HIGHMEM Dynamically mapped pages \u0026gt; 896MB Linux partitions pages into zones to have a pooling in place to satisfy allocations as needed. For example, with a ZONE_DMA pool, the kernel has the capability to satisfy memory allocations needed for DMA. If such memory is needed, the kernel can simply pull the required number of pages from ZONE_DMA. The zones do not have any physical relevance but are simply logical groupings used by the kernel to keep track of pages.\nAlthough some allocations may require pages from a particular zone, other allocations may pull from multiple zones. For example:\nAn allocation for DMA-able memory must originate from ZONE_DMA A normal allocation can come from ZONE_DMA or ZONE_NORMAL but not both; allocations cannot cross zone boundaries. The kernel prefers to satisfy normal allocations from the normal zone to save the pages in ZONE_DMA for allocations that need it. Not all architectures define all zones. For example, a 64-bit architecture such as Intel’s x86-64 can fully map and handle 64-bits of memory. Thus, x86-64 has no ZONE_HIGHMEM and all physical memory is contained within ZONE_DMA and ZONE_NORMAL.\nEach zone is represented by struct zone structure.\nGetting Pages # The Kernel implement different interfaces to enable memory allocation and release.\nThe kernel provides one low-level mechanism for managing memory, along with several interfaces to access it. All these interfaces manage memory with page-sized granularity. It is possible for the kernel to provide zeroed pages. This is useful for pages given to userspace because the random garbage in an allocated page is not so random; it might contain sensitive data. All data must be zeroed or otherwise cleaned before it is returned to userspace to ensure system security is not compromised.\nThese low-level page functions are useful when page-sized chunks of physically contiguous pages are needed, especially if the need is exactly a single page or two. For more general byte-sized allocations, the kernel provides kmalloc().\nkmalloc() # The kmalloc() function is similar to user-space’s malloc(), with the exception of the additional flags parameter. The kmalloc() function is a simple interface for obtaining kernel memory in byte-sized chunks. If whole pages are needed, the previously discussed interfaces might be a better choice. For most kernel allocations, kmalloc() is the preferred interface.\nThe kmalloc() function returns a pointer to a region of memory that is at least size bytes in length. It may allocate more than asked, although it is not possible to know how much more. Because the kernel allocator is page-based, some allocations may be rounded up to fit within the available memory. The kernel never returns less memory than requested. If the kernel is unable to find at least the requested amount, the allocation fails and the function returns NULL. The region of memory allocated is physically contiguous. Kernel allocations always succeed, unless an insufficient amount of memory is available. Thus, a check for NULL after all calls to kmalloc() and handling the error appropriately should be done.\nFlags can be passed as parameters to kmalloc() to modify its behaviour. To specify from which zones to allocate memory for example, by default, it will allocate from either ZONE_DMA or ZONE_NORMAL, with a strong preference to satisfy the allocation from ZONE_NORMAL.\nkmalloc() cannot allocate from ZONE_HIGHMEM. Only low-level interfaces can allocate high memory.\nkfree() # The counterpart to kmalloc() is kfree(). It frees a block of memory previously allocated with kmalloc().\nCalling this function on memory not previously allocated with kmalloc(), or on memory that has already been freed is a bug, resulting in bad behavior such as freeing memory belonging to another part of the kernel. As in user-space, each allocations should be at some point deallocated to prevent memory leaks and other bugs.\nvmalloc() # The vmalloc() function works in a similar fashion to kmalloc(), except vmalloc() allocates memory that is only virtually contiguous and not necessarily physically contiguous. This is similar to user-space malloc() and its usage is identical. The returned pages by which are contiguous within the virtual address space, but necessarily contiguous in physical RAM.\nThe vmalloc() function ensures that the pages are physically contiguous by by allocating potentially noncontiguous chunks of physical memory and \u0026ldquo;fixing up\u0026rdquo; the page tables to map the memory into a contiguous chunk of the logical address space.\nUsually, only hardware devices require physically contiguous memory allocations, because they live on the other side of the memory management unit and do not understand virtual addresses. Blocks of memory used only by software (e.g.process-related buffers) are fine using memory that is only virtually contiguous. All memory appears to the kernel as logically contiguous. Though physically contiguous memory is required in only certain cases, most kernel code uses kmalloc() and not vmalloc() to obtain memory primarily for performance.\nThe vmalloc() function, to make nonphysically contiguous pages contiguous in the virtual address space, must specifically set up the page table entries. Worse, pages obtained via vmalloc() must be mapped by their individual pages (because they are not physically contiguous), which results in much greater TLB thrashing compared to using directly mapped memory. Because of these concerns, vmalloc() is used only when absolutely necessary (typically, to obtain large regions of memory). For example, when modules are dynamically inserted into the kernel, they are loaded into memory created via vmalloc().\nvfree() is used to free an allocation obtained via vmalloc().\nSlab Layer # Allocating and freeing data structures is one of the most common operations inside any kernel. To facilitate frequent allocations and deallocations of data, programmers often introduce free lists. A free list contains a block of available, already allocated, data structures.\nWhen code requires a new instance of a data structure, it can grab one of the structures off the free list rather than allocate the sufficient amount of memory and set it up for the data structure. When the data structure is no longer needed, it is returned to the free list instead of deallocated. In this sense, the free list acts as an object cache, caching a frequently used type of object. One of the main problems with free lists in the kernel is that there exists no global control. When available memory is low, there is no way for the kernel to communicate to every free list that it should shrink the sizes of its cache to free up memory. The kernel has no understanding of the random free lists at all. To remedy this, and to consolidate code, the Linux kernel provides the slab layer (also called the slab allocator). The slab layer acts as a generic data structure-caching layer.\nThe concept of slab layer comes from Sun Microsystem\u0026rsquo;s SunOS, Linux data structure caching layer shares the same name and basic design.\nThe slab layer attempts to leverage several basic tenets:\nFrequently used data structures tend to be allocated and freed often, so cache them. Frequent allocation and deallocation can result in memory fragmentation (the inability to find large contiguous chunks of available memory). To prevent this, the cached free lists are arranged contiguously. Because freed data structures return to the free list, there is no resulting fragmentation. The free list provides improved performance during frequent allocation and deallocation because a freed object can be immediately returned to the next allocation. If the allocator is aware of concepts such as object size, page size, and total cache size, it can make more intelligent decisions. If part of the cache is made per-processor (separate and unique to each processor on the system), allocations and frees can be performed without an SMP lock. If the allocator is NUMA-aware, it can fulfill allocations from the same memory node as the requestor. Stored objects can be colored to prevent multiple objects from mapping to the same cache lines. The slab layer in Linux was designed and implemented with these premises in mind.\nDesign of the Slab Layer # The slab layer divides different objects into groups called caches, each of which stores a different type of object. There is one cache per object type. For example, one cache is for process descriptors (a free list of task_struct structures), whereas another cache is for inode objects (struct inode). Interestingly, the kmalloc() interface is built on top of the slab layer, using a family of general purpose caches.\nThe caches are then divided into slabs (hence the name of this subsystem). The slabs are composed of one or more physically contiguous pages. Typically, slabs are composed of only a single page. Each cache may consist of multiple slabs. Each slab contains some number of objects, which are the data structures being cached.\nEach slab is in one of three states:\nA full slab has no free objects. (All objects in the slab are allocated.) A partial slab has some allocated objects and some free objects. An empty slab has no allocated objects. (All objects in the slab are free.) When some part of the kernel requests a new object, the request is satisfied from a partial slab, if one exists. Otherwise, the request is satisfied from an empty slab. If there exists no empty slab, one is created. Obviously, a full slab can never satisfy a request because it does not have any free objects. This strategy reduces fragmentation.\nStatically Allocating on the Stack # User-space can afforded large, dynamically growing stack, whereas the the kernel’s stack is small and fixed.\nThe size of the per-process kernel stacks depends on both the architecture and a compile-time option. Historically, the kernel stack has been two pages per process by default. It is now only one page per process.\nThis is usually:\n4KB for 32-bit architectures (with one 4KB page). 8KB for 64-bit architectures (wtih one 8KB page). The move to single-page kernel stacks was done for two reasons:\nIt results in a page with less memory consumption per process. As uptime increases, it becomes increasingly hard to find two physically contiguous unallocated pages. Physical memory becomes fragmented, and the resulting VM pressure from allocating a single new process is expensive. There is one more complication. Each process’s entire call chain has to fit in its kernel stack. Historically, however, interrupt handlers also used the kernel stack of the process they interrupted, thus they too had to fit. This was efficient and simple, but it placed even tighter constraints on the already meager kernel stack. When the stack moved to only a single page, interrupt handlers no longer fit.\nTo rectify this problem, the kernel developers implemented a new feature: interrupt stacks. Interrupt stacks provide a single per-processor stack used for interrupt handlers. With this option, interrupt handlers no longer share the kernel stack of the interrupted process. Instead, they use their own stacks. This consumes only a single page per processor.\nTo summarize, kernel stacks are either one or two pages, depending on compile-time configuration options. The stack can therefore range from 4KB to 16KB. Historically, interrupt handlers shared the stack of the interrupted process. When single page stacks are enabled, interrupt handlers are given their own stacks.\nPlaying Fair on the Stack # In any given function, stack usage should be kept to a minimum. The sum of all local (automatic) variables in a function should be kept to a maximum of a couple hundred bytes. Performing a large static allocation on the stack (e.g. a large array or structure) is dangerous. Otherwise, stack allocations are performed in the kernel just as in user-space.\nStack overflows occur silently and will undoubtedly result in problems. Because the kernel does not make any effort to manage the stack, when the stack overflows, the excess data simply spills into whatever exists at the tail end of the stack, the first thing of which is the thread_info structure, which is allocated at the end of each process’s kernel stack.\nBeyond the stack, any kernel data might lurk. At best, the machine will crash when the stack overflows. At worst, the overflow will silently corrupt data. Therefore, it is wise to use a dynamic allocation scheme for any large memory allocations.\nHigh Memory Mappings # By definition, pages in high memory might not be permanently mapped into the kernel’s (virtual) address space. Thus, pages obtained via alloc_pages() with the __GFP_HIGHMEM flag might not have a logical address.\nOn the x86 architecture, all physical memory beyond the 896MB mark is high memory and is not permanently or automatically mapped into the kernel’s address space, despite x86 processors being capable of physically addressing up to 4GB (64GB with PAE) of physical RAM.\nAfter they are allocated, these pages must be mapped into the kernel’s logical address space. On x86, pages in high memory are mapped somewhere between the 3GB and 4GB mark.\nPermanent Mappings # To map and unmap a given page structure into the kernel’s address space, the kmap() and kunmap() functions can be used.\nThese functions works on either high or low memory and they can sleep.\nWhen mapping:\nIf the page structure belongs to a page in low memory, the page’s virtual address is simply returned. If the page resides in high memory, a permanent mapping is created and the address is returned. Because the number of permanent mappings are limited, high memory should be unmapped when no longer needed.\nTemporary Mappings # When a mapping must be created but the current context cannot sleep, the kernel provides temporary mappings (also called atomic mappings). The kernel can atomically map a high memory page into one of the reserved mappings (which can hold temporary mappings). Consequently, a temporary mapping can be used in places that cannot sleep, such as interrupt handlers, because obtaining the mapping never blocks.\nSetting up and tearing down a temporary mapping is done via kmap_atomic() and kunmap_atomic().\nThis function does not block and thus can be used in interrupt context and other places that cannot reschedule. It also disables kernel preemption, which is needed because the mappings are unique to each processor and a reschedule might change which task is running on which processor.\nIn many architectures, unmapping does not do anything at all except enable kernel preemption, because a temporary mapping is valid only until the next temporary mapping. Thus, the kernel can just \u0026ldquo;forget about\u0026rdquo; the temporary mapping. The next atomic mapping then simply overwrites the previous one.\nPer-CPU Allocations # "}]