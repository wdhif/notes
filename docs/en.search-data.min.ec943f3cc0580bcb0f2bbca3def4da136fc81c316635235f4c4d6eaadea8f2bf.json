[{"id":0,"href":"/grimoire/docs/linux-kernel-development/1-introduction-to-the-linux-kernel/","title":"1. Introduction to the Linux Kernel","section":"Linux Kernel Development","content":" Introduction to Unix # Unix is a family of operating systems featuring a similar API and design, including Unix, BSD, Solaris and Linux.\nSome Unix characteristics that are at the core of its strength:\nUnix is simple: only hundreds of system calls and have a straightforward, even basic, design. In Unix, everything is a file: simplifies the manipulation of data and devices into a set of core system calls: open(),read(), write(), lseek(), and close(). Unix has robust IPC (Interprocess communication) systems. Overview of Operating Systems and Kernels # The operating system is the parts of the system responsible for basic use and administration. The kernel is the innermost portion of the operating system. It is the core internals: the software that provides basic services for all other parts of the system, manages hardware, and distributes system resource.\nAn OS contains:\nKernel: Interrupt handlers to service interrupt requests. A scheduler to share processor time among multiple processes. A memory management system to manage process address spaces. System services such as networking and interprocess communication. Device drivers Boot loader Command shell User interfaces Basic utilities Kernel-space and User-space # Kernel-space: on modern systems with protected memory management units, the kernel typically resides in an elevated system state, which includes a protected memory space and full access to the hardware. This system state and memory space is collectively referred to as kernel-space.\nUser-space: applications execute in user-space, where they can access a subset of the machine’s available resources and can perform certain system functions, directly access hardware, access memory outside of that allotted them by the kernel, or otherwise misbehave.\nWhen executing kernel code, the system is in kernel-space executing in kernel mode. When running a regular process, the system is in user-space executing in user mode.\nApplications running on the system comminicate with the kernel using system calls (Figure 1.1)\nBasically, an application will use a C library to use the system call interface. The system calls will allow the Kernel to do operation on behalf of the application. Furthermore, the application is said to be executing a system call in kernel-space, and the kernel is running in process context.\nSome function of this C library provide many feature not found in system calls, E.G. the printf() function will not only wall the write() system call but will also do formating, etc\u0026hellip;\nOn the other hand, the open() function simply use the open() system call. Other functions of this C library does not use system calls at all, E.G. strcpy().\nThe Kernel also manage system\u0026rsquo;s hardware. When the hardware want to communicate, it will send an hardware interrupt to the CPU, which will in turn interrupt the Kernel, which will in turn operate a context switch to execute the interrupt handler.\ngraph LR; A[Hardware] --\u003e|Interrupt lines| B[CPU] B[CPU] --\u003e|Interrupt| C[Kernel] C[Kernel] --\u003e|Interrupt Handler| D[Run the interrupt handler code] A number identifies each interrupt, the kernel reads this number and then execute the handler code.\nFor example:\nsequenceDiagram participant Y as You participant KC as Keyboard Controller participant C as CPU participant KR as Kernel participant IH as Interrupt Handler Process Y-\u003e\u003eKC: Type on keyboard KC-\u003e\u003eC: Issue an hardware interrupt C-\u003e\u003eKR: Forward interrupt with interrupt number KR-\u003e\u003eIH: Run interrupt handler according to interrupt number IH-\u003e\u003eKC: Process Keyboard data IH-\u003e\u003eKC: Let it know it's ready for more data Running interrupt handlers put the Kernel in a special context, the interrupt context, not associated with any process.\nEach CPU in the Linux Kernel is doing exaclty one of the following:\nIn user-space, running user code in a process. In kernel-space, in process context, running kernel code on behalf of a specific process. In kernel-space, in interrupt context, not associated with any process, handling an interrupt. This list is inclusive, every cases fit any of the previous element. E.G. Even when the Kernel is idle, the CPU is actually running an idle process\nMonolithic Kernels vs Microkernels # Most Unix kernels are monolithics and although the Linux kernel is monolithic, it can dynamically load and unload kernel modules on demand.\nMonolithic Kernels are:\nSimpler than Microkernels. Implemented as a single process. Exist as a single binary. Runs in kernel mode. Runs everything in the same address space. Because of this, communication is trivial, the kernel simply invoke functions. Microkernels are:\nBroken down into separate process, called servers Only some servers runs in priviledged mode, if they need it, the rest runs in user mode. Because of this, direct function invocation is impossible. Microkernel uses message passing, an Inter Process communication (IPC) method. Servers invoke services from other servers, using message passing. Microkernels are modular, if one server crash, it doesn\u0026rsquo;t crash the all kernel. IPC (message passing) and context switches (user mode vs privileged mode) involve some serious overhead To avoid this, modern Microkernels (Windows NT and mac OS) runs every servers in privileged mode, defeating the primary purpose of microkernels. "},{"id":1,"href":"/grimoire/docs/linux-kernel-development/","title":"Linux Kernel Development","section":"Docs","content":" 1. Introduction to the Linux Kernel 2. Getting Started with the Kernel 3. Process Management 4. Process Scheduling 5. System Calls 6. Kernel Data Structures 7. Interrupts and Interrupt Handlers 8. Bottom Halves and Deferring Work 9. An Introduction to Kernel Synchronization "},{"id":2,"href":"/grimoire/docs/linux-kernel-development/2-getting-started-with-the-kernel/","title":"2. Getting Started with the Kernel","section":"Linux Kernel Development","content":" TODO # "},{"id":3,"href":"/grimoire/docs/linux-kernel-development/3-process-management/","title":"3. Process Management","section":"Linux Kernel Development","content":" The Process # A process, or task in the Kernel, is a program (object code) that is currently running, but a program is not a process, two or more processes can actually run the same program at the same time.\nThey are more than juste code (text section), they also include a set of resources such as:\nOpen files. Pending signals. Internal Kernel data. Processor state. A memory address space with one or more mappings. One or more threads of execution. A data section containing global variables. Threads of execution, or threads, are the objects of activity within the process. Each thread includes:\nA unique program counter. A Process stack. A set of processor registers. The kernel schedules individual threads, not processes.\nProcess that runs more than one thread are call multithreaded.\nOn modern operating systems, processes provide two virtualizations:\nA virtualized processor: The virtual processor gives the process the illusion that it alone monopolizes the system, despite possibly sharing the processor among hundreds of other processes. A virtual memory: Virtual memory lets the process allocate and manage memory as if it alone owned all the memory in the system. Note that threads share the virtual memory abstraction, but they each receives their own virtualized processor.\nProcess lifecycle # The process lifecycle goes as follow:\nfork(): A parent process will run the fork() system call, effectively cloning himself (fork() is implemented using clone()) into a child process. The parent process will then resume execution and the child process will start execution at the same place, where the fork() system call returns. fork() will in fact return twice, once in the parent process, and once in the child process. At some point however, the parent process will have to run a wait() type system call to reap its exited child process. exec(): After its creation, the child process will run one of the exec() system calls that will create a new address space and load a new program into it. exit(): Finally, when a process exits, it run the exit() system call. This will terminate the process and free all its ressources. The process will then be placed in a zombie state and until its parent process run the wait4() system call. Process Descriptor and the Task Structure # The kernel stores the list of processes in a circular doubly linked list called the task list. Each element of this list is called a process descriptor that will contain all informations about a given process.\nThe system identifies processes by a unique process identification value or PID, stored in the process descriptor. The PID is typically an integer going up to 32,768. This maximum value is important because it is essentially the maximum number of processes that may exist concurrently on the system.\nThe Kernel will affect PID to each process in an ascending order, this creates the notion that the higher the PID, the later the process started. But if the maximum number of PID is reached, then it will wrap back and try to affect freed PID to new process, detroying this useful notion.\nYou can also increase this value up to four millons, by modifying /proc/sys/kernel/pid_max. Take note that this might break compatibility with older programs.\nProcess State # The state field of the process descriptor describes the current condition of the process. Each process on the system is in exactly one of the five following states:\nTASK_RUNNING: The process is runnable, it is either currently running or on a runqueue waiting to run. This is the only possible state for a process executing in user-space. It can also apply to a process in kernel-space that is actively running. TASK_INTERRUPTIBLE: The process is sleeping, or blocked, waiting for some condition to exist. When this condition exists, the kernel sets the process’s state to TASK_RUNNING. The process also awakes prematurely and becomes runnable if it receives a signal. TASK_UNINTERRUPTIBLE: This state is identical to TASK_INTERRUPTIBLE except that it does not wake up and become runnable if it receives a signal. This is used in situations where the process must wait without interruption or when the event is expected to occur quite quickly. Because the task does not respond to signals in this state, TASK_UNINTERRUPTIBLE is less often used than TASK_INTERRUPTIBLE. __TASK_TRACED: The process is being traced by another process, such as a debugger (like strace), using the ptrace() system call. __TASK_STOPPED: Process execution has stopped. The task is not running nor is it eligible to run. This occurs if the task receives the SIGSTOP, SIGTSTP, SIGTTIN, or SIGTTOU signal or if it receives any signal while it is being debugged. Take note that a process in the TASK_UNINTERRUPTIBLE state will not even respond to SIGKILL. This state is represented by the famous D state in the ps command.\nProcess Context # The goal of a process is to execute program code. This code is read from an executable file and executed from within the program\u0026rsquo;s address space. Normal program execution occurs in user-space, when a program executes a system call or triggers an exception, it enters kernel-space. At this point, the kernel is said to be “executing on behalf of the process” and the kernel is in process context.\nUpon exiting the kernel, the process resumes execution in user-space, unless a higher-priority process has become runnable in the interim, in which case the scheduler is invoked to select the higher priority process.\nSystem calls and exception handlers are well-defined interfaces into the kernel. A process can begin executing in kernel-space only through one of these interfaces. All access to the kernel is through these interfaces.\nThe Process Family Tree # There is a hierarchy between processes. All processes are descendant of the init process, whose PID is 1. The Kernel will start the init process as the last step of the boot procedure. The init process will then read the initscripts and start all other programs necessary to complete the boot procedure.\nEvery process on the system has exactly one parent. Likewise, every process has zero or more children. Processes that are all direct children of the same parent are called siblings.\nProcess Creation # Process creation in Unix is unique. Unix takes the unusual approach of separating these steps into two distinct system calls: fork() and exec().\nfork() will create a child process that is a copy of the current task. It only differs by its PID, PPID (Parent PID) and some other resources, such as pending signals which are not inherited. exec() will load a new program in the task address space and start executing it. Copy-on-Write # Traditionnaly, upon fork(), all resources owned by the parent process are copied to the child process memory space. Even worse, if the first call of the child process is exec(), then all the copied data is discarded to allow the new program to run. To optimize this, Linux\u0026rsquo;s fork() is using what is called copy-on-write pages.\nCopy-on-write, or COW is working like this:\nWhen the fork() system call is invoked, rather than duplicating the address space, each process, parent and child, will share a single read-only copy. The data is marked so that if it is written to, a duplicate is made and each process will get its own copy, therefor, copy-on-write. Only the pages that are written to are duplicated, the rest of the memory space will still be shared. If the pages are never written, for example if the child calls exec() directly after the fork(), then the pages are never copied. To make sure that the child process is given the possibility to run exec() before the parent write to the data, the Kernel will schedule first the child process.\nThreads # Thread provide multiple threads of execution within the same program in a shared memory address space. They can also share open files and other resources. Threads enable concurrent programming and, on multiple processor systems, true parallelism.\nTo the Linux kernel, there is no concept of a thread. Linux implements all threads as standard processes. The Linux kernel does not provide any special scheduling semantics or data structures to represent threads. Instead, a thread is merely a process that shares certain resources with other processes. Each thread has appears to the kernel as a normal process and just happen to share resources, such as an address space, with other processes.\nKernel Threads # It is often useful for the kernel to perform some operations in the background. The kernel accomplishes this via kernel threads, standard processes that exist solely in kernel-space. The significant difference between kernel threads and normal processes is that kernel threads do not have an address space. They operate only in kernel-space and do not context switch into user-space. Kernel threads, however, are schedulable and preemptable, the same as normal processes.\nKernel threads are created on system boot by other kernel threads. Indeed, a kernel thread can be created only by another kernel thread.\nProcess Termination # When a process terminates, the kernel:\nReleases the resources owned by the process. Notifies the process’s parent of the termination of its child. Generally, process destruction is self-induced. It occurs when the process calls the exit() system call. A process can also terminate involuntarily, this occurs when the process receives a signal or exception it cannot handle or ignore.\nUpon deletion, the task enters the EXIT_ZOMBIE exit state. The only memory consumed is the one in the task list of the kernel. After the parent reap the child using the wait() system call, the kernel fully removes the task from the task list and the process is deleted.\nParentless Tasks, AKA Zombie Processes # If a parent exits before its children, some mechanism must exist to reparent any child tasks to a new process, or else parentless terminated processes would forever remain zombies, wasting system memory. The solution is to reparent a task’s children on exit to either another process in the current thread group or, if that fails, the init process.\nWith the process successfully reparented, there is no risk of stray zombie processes. The init process routinely calls wait() on its children, cleaning up any zombies assigned to it.\n"},{"id":4,"href":"/grimoire/docs/linux-kernel-development/4-process-scheduling/","title":"4. Process Scheduling","section":"Linux Kernel Development","content":" The Scheduler # The process scheduler decides which process runs, when, and for how long.\nThe process scheduler (or simply the scheduler, to which it is often shortened) divides the finite resource of processor time between the runnable processes on a system. By deciding which process runs next, the scheduler is responsible for best utilizing the system and giving users the impression that multiple processes are executing simultaneously, AKA Multitasking.\nThe ideas behind the scheduler are simple:\nTo best utilize processor time, assuming there are runnable processes, a process should always be running. If there are more runnable processes than processors in a system, some processes will not be running at a given moment. These processes are waiting to run. Deciding which process runs next, given a set of runnable processes, is the fundamental decision that the scheduler must make. Multitasking # A multitasking operating system is one that can simultaneously interleave execution of more than one process.\nOn single processor machines, this gives the illusion of multiple processes running concurrently. On multiprocessor machines, such functionality enables processes to actually run concurrently, in parallel, on different processors. On either type of machine, it also enables many processes to block or sleep, not actually executing until work is available. These processes, although in memory, are not runnable. Instead, such processes utilize the kernel to wait until some event (keyboard input, network data, passage of time, and so on) occurs.\nMultitasking operating systems come in two flavors:\ncooperative multitasking: A process does not stop running until it voluntary decides to do so, it is called yielding. Ideally, processes yield often, giving each runnable process a decent chunk of the processor. preemptive multitasking: The scheduler decides when a process is to cease running and a new process is to begin running, it is called preemption. The time a process runs before it is preempted is usually predetermined, and it is called the timeslice of the process. The process is given a slice of processor\u0026rsquo;s time to run. The shortcomings of the cooperative multitasking approach are manifest, the scheduler cannot make global decisions regarding how long processes run. Processes can monopolize the processor for longer than the user desires, and a hung process that never yields can potentially bring down the entire system.\nOn preemptive multitasking, the scheduler manage each process timeslices, making global decisions for the system (e.g. priorization) and preventing any one process from monopolizing the processor. The timeslice is dynamically calculated as a function of process behavior and configurable system policy.\nLinux, like all Unix variants and most modern operating systems, implements preemptive multitasking.\nLinux\u0026rsquo;s Process Scheduler # The current Linux scheduler is the Completely Fair Scheduler, or CFS. The CFS was introduced as a replacement for the O(1) scheduler, which was performant, but was lacking when dealing with interactive process, like on a desktop for example.\nPolicy # The policy of a scheduler determines what runs when. Therefore it is very important.\nI/O-Bound Versus Processor-Bound Processes # Processes can be classified as two types:\nI/O-bound: A process that spend much of its time waiting on I/O requests (I/O here means any blockable resources, like keyboard inputs or network I/O, not only disk I/O). Such processes are runnable for only short durations before having to wait on requests, like GUI waiting on user inputs. processor-bound: Processor-bound processes spend much of their time executing code. They tend to run until they are preempted because they do not block on I/O requests very often. Linux, aiming to provide good interactive response and desktop performance, optimizes for response time (low latency), thus favoring I/O-bound processes over processor-bound processors. As we will see, this is done in a creative manner that does not neglect processor-bound processes.\nProcess Priority # A common type of scheduling algorithm is priority-based scheduling. The goal is to rank processes based on their worth and need for processor time. Processes with a higher priority run before those with a lower priority, whereas processes with the same priority are scheduled in round-robin (one after the next, repeating).\nThe runnable process with timeslice remaining and the highest priority always runs. Both the user and the system can set a process’s priority to influence the scheduling behavior of the system.\nThe Linux kernel implement two priority ranges:\nNiceness: A number from -20 to +19 with a default at 0. Larger nice values correspond to a lower priority, you are being “nice” to the other processes on the system. Processes with a lower nice value (higher priority) receive a larger proportion of the system’s processor compared to processes with a higher nice value (lower priority). Real-time priority: The values are configurable, but by default range from 0 to 99, inclusive. Higher real-time priority values correspond to a greater priority and all real-time processes are at a higher priority than normal processes. By default, processes are not real-time, their value is \u0026ldquo;-\u0026rdquo;, or null. Timeslice # The timeslice is the numeric value that represents how long a task can run until it is preempted. The scheduler policy must dictate a default timeslice, which is not a trivial exercise.\nToo long a timeslice causes the system to have poor interactive performance, the system will no longer feel as if applications are concurrently executed. Too short a timeslice causes significant amounts of processor time to be wasted on the overhead of switching processes. Furthermore, the conflicting goals of I/O-bound versus processor-bound processes again arise:\nI/O-bound processes do not need longer timeslices (although they do like to run often) Processor-bound processes crave long timeslices (to keep their caches hot). To avoid thoses issues, Linux’s CFS scheduler does not directly assign timeslices to processes. Instead, it will assign to processes a proportion of the processor, therefore, the amount of processor time that a process receives is a function of the load of the system. This assigned proportion is further affected by each process’s nice value. The nice value acts as a weight, changing the proportion of the processor time each process receives.\nWhen a process enters the runnable state, it becomes eligible to run. Whether the process runs immediately, preempting the currently running process, is a function of how much of a proportion of the processor the newly runnable processor has consumed.\nIf it has consumed a smaller proportion of the processor than the currently executing process, it runs immediately, preempting the current process. If not, it is scheduled to run at a later time. The Scheduling Policy in Action # Consider two runnable processes:\nA text editor: I/O-bound. It will spend nearly all its time waiting for user key presses. A video encoder: Processor-bound. Aside from waiting on disk to read the video at first, it will then spend all its time encoding the video, consuming 100% of the processor. The text editor, despite spending most of its time waiting on user input, when the text editor does receive a key press, the user expects the editor to respond immediately. Latency is a primary concern. The video encoder doesn\u0026rsquo;t have strong time constraint, we don\u0026rsquo;t really care if it runs now or at the next CPU cycle. The sooner it finishes the better, but latency is not a primary concern.\nIf both processes are runnable and have the same nicelevel, they would be awarded both with the same proportion of the processor\u0026rsquo;s time. But since the text editor spend a lot of time waiting, it would almost always have more processor time left than the video encoder. And because of that, CFS would let the text editor run in priority, therefore limiting latency.\nThis will allow the text editor to run everytime it\u0026rsquo;s ready, and the video encoder to run the rest of the time, assuming no other process needs to run.\nThe Linux Scheduling Algorithm # Scheduler Classes # The Linux scheduler is modular, enabling different algorithms to schedule different types of processes. This modularity is called scheduler classes. Scheduler classes enable different, pluggable algorithms to coexist, scheduling their own types of processes. Each scheduler class has a priority.\nThe base scheduler code, iterates over each scheduler class in order of priority. The highest priority scheduler class that has a runnable process wins, selecting who runs next. The Completely Fair Scheduler (CFS) is the registered scheduler class for normal processes, called SCHED_NORMAL in Linux.\nFair Scheduling # CFS is based on a simple concept: Model process scheduling as if the system had an ideal, perfectly multitasking processor. In such a system, each process would receive 1/n of the processor’s time, where n is the number of runnable processes. We’d schedule them for infinitely small durations, so that in any measurable period we’d have run all n processes for the same amount of time.\nThe Linux Scheduling Implementation # Time Accounting # All process schedulers must account for the time that a process runs. Most Unix systems do so, as discussed earlier, by assigning each process a timeslice. On each tick of the system clock, the timeslice is decremented by the tick period. When the timeslice reaches zero, the process is preempted in favor of another runnable process with a nonzero timeslice.\nCFS does not have the notion of a timeslice, but it must still keep account for the time that each process runs, because it needs to ensure that each process runs only for its fair share of the processor. This information is embedded in the process descriptor. We discussed the process descriptor in Chapter 3, “Process Management.”\nThe Virtual Runtime # The Virtual Runtime (vruntime) of a process the actual runtime (the amount of time spent running) normalized (or weighted) by the number of runnable processes, in nanoseconds. The Virtual Runtime update is triggered periodically by the system timer and also whenever a process becomes runnable or blocks, becoming unrunnable. In this manner, vruntime is an accurate measure of the runtime of a given process and an indicator of what process should run next.\nProcess Selection # On an ideal processor, the Virtual Runtime of all processes of the same priority would be identical, all tasks would have received an equal, fair share of the processor. In reality, we cannot perfectly multitask, so CFS attempts to balance a process’s Virtual Runtime with a simple rule: When CFS is deciding what process to run next, it picks the process with the smallest vruntime. The CFS maintains a red-black tree for the Virtual Runtimes.\nPreemption and Context Switching # Context switching, the switching from one runnable task to another, is handled by the scheduler. It is called when a new process has been selected to run. It does two basic jobs:\nSwitch the virtual memory mapping from the previous process’s to that of the new process. Switch the processor state from the previous process’s to the current’s. This involves saving and restoring stack information and the processor registers and any other architecture-specific state that must be managed and restored on a per-process basis. The kernel, however, must know when to call the scheduler. If it called it only when code explicitly did so, user-space programs could run indefinitely.\nInstead, the kernel provides the need_resched flag to signify whether a reschedule should be performed.\nThis flag is set by scheduler_tick() (which is ran by a timer) when a process should be preempted. This flag is set by try_to_wake_up() when a process that has a higher priority than the currently running process is awakened. Upon returning to user-space or returning from an interrupt, the kernel checks the need_resched flag. If it is set, the kernel invokes the scheduler (using schedule()) before continuing. The flag is a message to the kernel that the scheduler should be invoked as soon as possible because another process deserves to run.\nUser preemption # User preemption occurs when the kernel is about to return to user-space, need_resched is set, and therefore, the scheduler is invoked. If the kernel is returning to user-space, it knows it is safe to continue executing the current task or to pick a new task to execute.\nConsequently, whenever the kernel is preparing to return to user-space either on return from an interrupt or after a system call, the value of need_resched is checked. If it is set, the scheduler is invoked to select a new (more fit) process to execute.\nUser preemption can occur\nWhen returning to user-space from a system call. When returning to user-space from an interrupt handler. Kernel Preemption # The Linux kernel, unlike most other Unix variants and many other operating systems, is a fully preemptive kernel. In nonpreemptive kernels, kernel code runs until completion, the scheduler cannot reschedule a task while it is in the kernel. Kernel code runs until it finishes (returns to user-space) or explicitly blocks. However, the Linux kernel is able to preempt a task at any point, so long as the kernel is in a state in which it is safe to reschedule.\nSo when is it safe to reschedule? The kernel can preempt a task running in the kernel so long as it does not hold a lock. That is, locks are used as markers of regions of nonpreemptibility. If a lock is not held, the current code is reentrant and capable of being preempted.\nKernel preemption can occur\nWhen an interrupt handler exits, before returning to kernel-space. When kernel code becomes preemptible again, when all locks are released. If a task in the kernel explicitly calls schedule(). If a task in the kernel blocks (which results in a call to schedule()). Real-Time Scheduling Policies # Linux provides two real-time scheduling policies\nSCHED_FIFO SCHED_RR The normal, not real-time scheduling policy is SCHED_NORMAL (using the CFS). Via the scheduling classes framework, these real-time policies are managed not by the Completely Fair Scheduler, but by a special real-time scheduler.\nSCHED_FIFO implements a simple first-in, first-out scheduling algorithm without timeslices.\nA runnable SCHED_FIFO task is always scheduled over any SCHED_NORMAL tasks. When a SCHED_FIFO task becomes runnable, it continues to run until it blocks or explicitly yields the processor; it has no timeslice and can run indefinitely. Only a higher priority SCHED_FIFO or SCHED_RR task can preempt a SCHED_FIFO task. Two or more SCHED_FIFO tasks at the same priority run round-robin, but only yielding the processor when they explicitly choose to do so. If a SCHED_FIFO task is runnable, all other tasks at a lower priority cannot run until the SCHED_FIFO task becomes unrunnable. SCHED_RR is identical to SCHED_FIFO except that each process can run only until it exhausts a predetermined timeslice. In other words, SCHED_RR is SCHED_FIFO with timeslices. It is a real-time, round-robin scheduling algorithm.\nWhen a SCHED_RR task exhausts its timeslice, any other real-time processes at its priority are scheduled round-robin. The timeslice is used to allow only rescheduling of same-priority processes. As with SCHED_FIFO, a higher-priority process always immediately preempts a lower-priority one, and a lower-priority process can never preempt a SCHED_RR task, even if its timeslice is exhausted. Both real-time scheduling policies implement static priorities. The kernel does not calculate dynamic priority values for real-time tasks. This ensures that a real-time process at a given priority always preempts a process at a lower priority.\nTwo types of real-time behavior exists:\nSoft real-time, meaning that the kernel tries to schedule applications within timing deadlines, but the kernel does not promise to always achieve these goals. Hard real-time systems are guaranteed to meet any scheduling requirements within certain limits. The real-time scheduling policies in Linux provide soft real-time behavior. Linux makes no guarantees on the capability to schedule real-time tasks. Despite not having a design that guarantees hard real-time behavior, the real-time scheduling performance in Linux is quite good.\nReal-time priorities range inclusively from 0 to MAX_RT_PRIO - 1. By default, this range is 0 to 99, since MAX_RT_PRIO is 100. This priority space is shared with the nice values of SCHED_NORMAL tasks. They use the space from MAX_RT_PRIO to (MAX_RT_PRIO + 40). By default, this means the –20 to +19 nice range maps directly onto the priority space from 100 to 139.\nDefault priority ranges:\n0 to 99: real-time priorities. 100 to 139: normal priorities. Scheduler-Related System Calls # Linux provides a family of system calls for the management of scheduler parameters. These system calls allow manipulation of process priority, scheduling policy, and processor affinity, as well as provide an explicit mechanism to yield the processor to other tasks.\n"},{"id":5,"href":"/grimoire/docs/linux-kernel-development/5-system-calls/","title":"5. System Calls","section":"Linux Kernel Development","content":" System Calls # In any modern operating system, the kernel provides a set of interfaces by which processes running in user-space can interact with the system. These interfaces give applications controlled access to hardware, a mechanism with which to create new processes and communicate with existing ones, and the capability to request other operating system resources. The existence of these interfaces, and the fact that applications are not free to directly do whatever they want, is key to providing a stable system.\nCommunicating with the Kernel # System calls provide a layer between the hardware and user-space processes, which serves three primary purposes:\nProviding an abstracted hardware interface for userspace. For example, when reading or writing from a file, applications are not concerned with the type of disk, media, or even the type of filesystem on which the file resides. Ensuring system security and stability. The kernel acts as a middleman between system resources and user-space, so it can arbitrate access based on permissions, users, and other criteria. For example, this arbitration prevents applications from incorrectly using hardware, stealing other processes’ resources, or otherwise doing harm to the system. A single common layer between user-space and the rest of the system allows for the virtualized system provided to processes. It would be impossible to implement multitasking and virtual memory if applications were free to access access system resources without the kernel’s knowledge. In Linux, system calls are the only means user-space has of interfacing with the kernel and the only legal entry point into the kernel other than exceptions and traps. Other interfaces, such as device files or /proc, are ultimately accessed via system calls. Interestingly, Linux implements far fewer system calls than most systems.\nAPIs, POSIX, and the C Library # Applications are typically programmed against an Application Programming Interface (API) implemented in user-space, not directly to system calls, because no direct correlation is needed between the interfaces used by applications and the actual interface provided by the kernel. An API defines a set of programming interfaces used by applications. Those interfaces can be implemented as a system call, implemented through multiple system calls, or implemented without the use of system calls at all. The same API can exist on multiple systems and provide the same interface to applications while the implementation of the API itself can differ greatly from system to system.\nThe most common APIs in the Unix world is based on POSIX. Technically, POSIX is composed of a series of standards from the IEEE that aim to provide a portable operating system standard roughly based on Unix. Linux strives to be POSIX- and SUSv3-compliant where applicable.\nOn most Unix systems, the POSIX-defined API calls have a strong correlation to the system calls. Some systems that are rather un-Unix, such as Microsoft Windows, offer POSIX-compatible libraries.\nThe system call interface in Linux, as with most Unix systems, is provided in part by the C library.\nThe C library implements the main API on Unix systems, including the standard C library and the system call interface. The C library is used by all C programs and, because of C’s nature, is easily wrapped by other programming languages for use in their programs. The C library additionally provides the majority of the POSIX API.\nFrom the application programmer’s point of view, system calls are irrelevant; all the programmer is concerned with is the API. Conversely, the kernel is concerned only with the system calls; what library calls and applications make use of the system calls is not of the kernel’s concern. Nonetheless, it is important for the kernel to keep track of the potential uses of a system call and keep the system call as general and flexible as possible.\nSyscalls # System calls (often called syscalls in Linux) are typically accessed via function calls defined in the C library. The functions can define zero, one, or more arguments (inputs) and might result in one or more side effects. Although nearly all system calls have a side effect (that is, they result in some change of the system’s state), a few syscalls, such as getpid(), merely return some data from the kernel. System calls also provide a return value of type long (for compatibility with 64-bit architectures) that signifies success or error. Usually, although not always, a negative return value denotes an error. A return value of zero is usually (not always) a sign of success.\nSystem Call Numbers # In Linux, each system call is assigned a syscall number.This is a unique number that is used to reference a specific system call. When a user-space process executes a system call, the syscall number identifies which syscall was executed; the process does not refer to the syscall by name.\nWhen assigned, the syscall number cannot change; otherwise, compiled applications will break. If a system call is removed, its system call number cannot be recycled, or previously compiled code would aim to invoke one system call but would in reality invoke another. Linux provides a \u0026ldquo;not implemented\u0026rdquo; system call, sys_ni_syscall(), which does nothing except return ENOSYS, the error corresponding to an invalid system call. This function is used to \u0026ldquo;plug the hole\u0026rdquo; in the rare event that a syscall is removed or otherwise made unavailable.\nSystem Call Performance # System calls in Linux are faster than in many other operating systems. This is partly because of Linux’s fast context switch times; entering and exiting the kernel is a streamlined and simple affair. The other factor is the simplicity of the system call handler and the individual system calls themselves.\nSystem Call Handler # It is not possible for user-space applications to execute kernel code directly. They cannot simply make a function call to a method existing in kernel-space because the kernel exists in a protected memory space. If applications could directly read and write to the kernel’s address space, system security and stability would be nonexistent.\nUser-space applications signal the kernel that they want to execute a system call and have the system switch to kernel mode, where the system call can be executed in kernel-space by the kernel on behalf of the application. This mechanism is a software interrupt: incur an exception, and the system will switch to kernel mode and execute the exception handler. The exception handler in this case is actually the system call handler. The defined software interrupt on x86 for the syscall handler is interrupt number 128.\nRecently, x86 processors added a feature known as sysenter, which provides a faster, more specialized way of trapping into a kernel to execute a system call than using the int interrupt instruction.\nDenoting the Correct System Call # Simply entering kernel-space alone is not sufficient because multiple system calls exist, all of which enter the kernel in the same manner. Thus, the system call number must be passed into the kernel. On x86, the syscall number is fed to the kernel via the eax register. Before causing the trap into the kernel, user-space sticks in eax the number corresponding to the desired system call. The system call handler then reads the value from eax.\nParameter Passing # In addition to the system call number, most syscalls require that one or more parameters be passed to them. Somehow, user-space must relay the parameters to the kernel during the trap. The easiest way to do this is via the same means that the syscall number is passed: the parameters are stored in registers.\nOn x86-32, the registers ebx, ecx, edx, esi, and edi contain, in order, the first five arguments. In the unlikely case of six or more arguments, a single register is used to hold a pointer to user-space where all the parameters are stored.\nThe return value is sent to user-space also via register. On x86, it is written into the eax register.\nSystem Call Implementation # The actual implementation of a system call in Linux does not need to be concerned with the behavior of the system call handler. Thus, adding a new system call to Linux is relatively easy. The hard work lies in designing and implementing the system call; registering it with the kernel is simple.\nImplementing System Calls # Each syscall should have exactly one purpose. Multiplexing syscalls (a single system call that does wildly different things depending on a flag argument) is discouraged in Linux. System call should have a clean and simple interface with the smallest number of arguments possible. The semantics and behavior of a system call are important; they must not change, because existing applications will come to rely on them. Design the system call to be as general as possible with an eye toward the future. The purpose of the system call will remain constant but its uses may change. Verifying the Parameters # System calls must carefully verify all their parameters to ensure that they are valid, legal and correct to guarantee the system’s security and stability.\nOne of the most important checks is the validity of any pointers that the user provides. Before following a pointer into user-space, the system must ensure that:\nThe pointer points to a region of memory in user-space. Processes must not be able to trick the kernel into reading data in kernel-space on their behalf. The pointer points to a region of memory in the process’s address space.The process must not be able to trick the kernel into reading someone else’s data. The process must not be able to bypass memory access restrictions. If reading, the memory is marked readable. If writing, the memory is marked writable. If executing, the memory is marked executable. System Call Context # The kernel is in process context during the execution of a system call. The current pointer points to the current task, which is the process that issued the syscall.\nIn process context, the kernel is capable of sleeping (for example, if the system call blocks on a call or explicitly calls schedule()) and is fully preemptible. The capability to sleep means that system calls can make use of the majority of the kernel’s functionality.\n"},{"id":6,"href":"/grimoire/docs/linux-kernel-development/6-kernel-data-structures/","title":"6. Kernel Data Structures","section":"Linux Kernel Development","content":" Kernel Data Structures # TODO\n"},{"id":7,"href":"/grimoire/docs/linux-kernel-development/7-interrupts-and-interrupt-handlers/","title":"7. Interrupts and Interrupt Handlers","section":"Linux Kernel Development","content":" Interrupts and Interrupt Handlers # A core responsibility of any operating system kernel is managing the hardware connected to the machine, hard drives and Blu-ray discs, keyboards and mice, 3D processors and wireless radios. To meet this responsibility, the kernel needs to communicate with the machine’s individual devices.\nProcessors can be orders of magnitudes faster than the hardware they talk to; it is not ideal for the kernel to issue a request and wait for a response from slower hardware. Instead, the kernel must be free to go and handle other work, dealing with the hardware only after that hardware has actually completed its work.\nHow can the processor work with hardware without impacting the machine’s overall performance? One answer to this question is polling. Periodically, the kernel can check the status of the hardware in the system and respond accordingly. Polling incurs overhead, however, because it must occur repeatedly regardless of whether the hardware is active or ready. A better solution is to provide a mechanism for the hardware to signal to the kernel when attention is needed. This mechanism is called an interrupt. In this chapter, we discuss interrupts and how the kernel responds to them, with special functions called interrupt handlers.\nInterrupts # Interrupts enable hardware to signal to the processor. For example, as you type, the keyboard controller (the hardware device that manages the keyboard) issues an electrical signal to the processor to alert the operating system to newly available key presses. These electrical signals are interrupts. The processor receives the interrupt and signals the operating system to enable the operating system to respond to the new data.\nHardware devices generate interrupts asynchronously to the processor clock, they can occur at any time. The kernel can be interrupted at any time to process interrupts.\nAn interrupt is physically produced by electronic signals originating from hardware devices and directed into input pins on an interrupt controller, a simple chip that multiplexes multiple interrupt lines into a single line to the processor. Upon receiving an interrupt, the interrupt controller sends a signal to the processor. The processor detects this signal and interrupts its current execution to handle the interrupt. The processor can then notify the operating system that an interrupt has occurred, and the operating system can handle the interrupt appropriately. Different devices can be associated with different interrupts by means of a unique value associated with each interrupt. This way, interrupts from the keyboard are distinct from interrupts from the hard drive. This enables the operating system to differentiate between interrupts and to know which hardware device caused which interrupt. In turn, the operating system can service each interrupt with its corresponding handler. These interrupt values are often called interrupt request (IRQ) lines. Each IRQ line is assigned a numeric value, for example, on the classic PC, IRQ zero is the timer interrupt and IRQ one is the keyboard interrupt. Not all interrupt numbers, however, are so rigidly defined. Interrupts associated with devices on the PCI bus, for example, generally are dynamically assigned.\nA specific interrupt is associated with a specific device, and the kernel knows this. The hardware then issues interrupts to get the kernel’s attention.\nExceptions and Interrupts # Exceptions are often discussed at the same time as interrupts. Unlike interrupts, exceptions occur synchronously with respect to the processor clock; they are often called synchronous interrupts. Exceptions are produced by the processor while executing instructions either in response to a programming error (e.g. divide by zero) or abnormal conditions that must be handled by the kernel (e.g. a page fault). Because many processor architectures handle exceptions in a similar manner to interrupts, the kernel infrastructure for handling the two is similar.\nWe can define Interrupts and Exceptions as follows:\nInterrupts: asynchronous interrupts generated by hardware, also called hardware interrupts. Exceptions: synchronous interrupts generated by the processor, also called software interrupts. Interrupt Handlers # The function the kernel runs in response to a specific interrupt is called an interrupt handler or interrupt service routine (ISR). Each device that generates interrupts has an associated interrupt handler. For example, one function handles interrupts from the system timer, whereas another function handles interrupts generated by the keyboard. The interrupt handler for a device is part of the device’s driver, the kernel code that manages the device. Each device has one associated driver. If that device uses interrupts (and most do), that driver must register one interrupt handler.\nIn Linux, interrupt handlers are normal C functions, which match a specific prototype and thus enables the kernel to pass the handler information in a standard way. What differentiates interrupt handlers from other kernel functions is that the kernel invokes them in response to interrupts and that they run in a special context called interrupt context. This special context is occasionally called atomic context because code executing in this context is unable to block.\nBecause an interrupt can occur at any time, an interrupt handler can be executed at any time. It is imperative that the handler runs quickly, to resume execution of the code that was interrupted and the hardware as soon as possible. At the very least, an interrupt handler’s job is to acknowledge the interrupt’s receipt to the hardware to let it continue its normal operation.\nOften, however, interrupt handlers have a large amount of work to perform. For example, consider the interrupt handler for a network device. On top of responding to the hardware, the interrupt handler needs to copy networking packets from the hardware into memory, process them, and push the packets down to the appropriate protocol stack or application.\nTop Halves Versus Bottom Halves # These two goals, that an interrupt handler execute quickly and perform a large amount of work, conflict with one another.\nBecause of these competing goals, the processing of interrupts is split into two parts, or halves:\nTop half: The interrupt handler is the top half. The top half is run immediately upon receipt of the interrupt and performs only the work that is time-critical, such as acknowledging receipt of the interrupt or resetting the hardware. Bottom half: Work that can be performed later is deferred until the bottom half. The bottom half runs in the future, at a more convenient time, with all interrupts enabled. For example using the network card:\nWhen network cards receive packets from the network, the network cards immediately issue an interrupt. This optimizes network throughput and latency and avoids timeouts. The kernel responds by executing the network card\u0026rsquo;s registered interrupt. The interrupt runs, acknowledges the hardware, copies the new networking packets into main memory, and readies the network card for more packets. These jobs are the important, time-critical, and hardware-specific work. The kernel generally needs to quickly copy the networking packet into main memory because the network data buffer on the networking card is fixed and miniscule in size, particularly compared to main memory. Delays in copying the packets can result in a buffer overrun, with incoming packets overwhelming the networking card\u0026rsquo;s buffer and thus packets being dropped. After the networking data is safely in the main memory, the interrupt\u0026rsquo;s job is done, and it can return control of the system to whatever code was interrupted when the interrupt was generated. The rest of the processing and handling of the packets occurs later, in the bottom half. Reentrancy and Interrupt Handlers # Interrupt handlers in Linux need not be reentrant. When a given interrupt handler is executing, the corresponding interrupt line is masked out on all processors, preventing another interrupt on the same line from being received. Normally all other interrupts are enabled, so other interrupts are serviced, but the current line is always disabled. Consequently, the same interrupt handler is never invoked concurrently to service a nested interrupt. This greatly simplifies writing your interrupt handler.\nShared Handlers # Multiple devices could use the same IRQ line, its called Interrupt Sharing. When the kernel receives an interrupt, it invokes sequentially each registered handler on the line. Therefore, it is important that the handler be capable of distinguishing whether it generated a given interrupt. The handler must quickly exit if its associated device did not generate the interrupt. This requires the hardware device to have a status register (or similar mechanism) that the handler can check. Most hardware has such a feature.\nInterrupt Context # When executing an interrupt handler, the kernel is in interrupt context.\nThe Kernel can only be in two different context:\nIn process context In user-space, running user code in a process. In kernel-space, running kernel code on behalf of a specific process. in interrupt context, in kernel-space, not associated with any process, handling an interrupt. Interrupt context cannot sleep and cannot reschedule. Therefore, you cannot call certain functions from interrupt context. If a function sleeps, you cannot use it from your interrupt handler: this limits the functions that can be called from an interrupt handler. Interrupt context is time-critical, because the interrupt handler interrupts other code.\nImplementing Interrupt Handlers # The implementation of the interrupt handling system in Linux is architecture-dependent. The implementation depends on the processor, the type of interrupt controller used, and the design of the architecture and machine.\nInterrupt path # From a device to the processor\nA device issues an interrupt by sending an electric signal over its bus to the interrupt controller. If the interrupt line is enabled (they can be masked out), the interrupt controller sends the interrupt to the processor. In most architectures, this is accomplished by an electrical signal sent over a special pin to the processor. If interrupts are not disabled in the processor, the processor immediately stops what it is doing, disables the interrupt system, and jumps to a predefined location in memory and executes the code located there. This predefined point is set up by the kernel and is the entry point for interrupt handlers. Inside the Kernel\nSimilarly to system calls that enter the kernel through a predefined exception handler, the interrupt in the kernel begins at this predefined entry point.\nFor each interrupt line, the processor jumps to a unique location in memory and executes the code located there. In this manner, the kernel knows the IRQ number (the interrupt value) of the incoming interrupt. The initial entry point (assembly entry routine) saves the interrupt value and stores the current register values of the interrupted task on the stack. Then, the Kernel ensures that a valid handler is registered on the line and that it is enabled and not currently executing. If so, it calls the corresponding interrupt handler (using do_IRQ()). From this point, most of the interrupt handling code is written in C, but is still architecture-dependent.\nSince the processor disabled interrupts, they are turned back on if IRQF_DISABLED was not specified during the handler\u0026rsquo;s registration. Each potential handler is executed in a loop. If this line is not shared, the loop terminates after the first iteration. Otherwise, all handlers are executed. At this point, all potential handlers have been executed.\nIf a reschedule is pending If the kernel is returning to user-space (that is, the interrupt interrupted a user process), the scheduler is called. If the kernel is returning to kernel-space (that is, the interrupt interrupted the kernel itself), the scheduler is called only if the Kernel does not hold locks. Otherwise it is not safe to preempt the kernel. After schedule() returns, or if there is no work pending, the initial registers are restored and the kernel resumes whatever was interrupted. Interrupts and ProcFS # ProcFS is a virtual filesystem that exists only in kernel memory and is typically mounted at /proc. Reading or writing files in procfs invokes kernel functions that simulate reading or writing from a real file.\nThe /proc/interrupts file is populated with statistics related to interrupts on the system.\nInterrupt Control # The Linux kernel implements a family of interfaces for manipulating the state of interrupts. These interfaces enable you to disable the interrupt system for the current processor or mask out an interrupt line for the entire machine. These routines are all architecture-dependent.\nControling the interrupt system is needed to provide synchronization.\nDisabling interrupts guarantees that an interrupt handler will not preempt the current code. Disabling interrupts also disables kernel preemption. However, neither disabling interrupt delivery nor disabling kernel preemption provides any protection from concurrent access from another processor. Because Linux supports multiple processors, kernel code generally needs to obtain some sort of lock to prevent another processor from accessing shared data simultaneously. These locks are often obtained in conjunction with disabling local interrupts.\n"},{"id":8,"href":"/grimoire/docs/linux-kernel-development/8-bottom-halves-and-deferring-work/","title":"8. Bottom Halves and Deferring Work","section":"Linux Kernel Development","content":" Bottom Halves and Deferring Work # Interrupt handlers, can form only the first half of any interrupt processing solution, with the following limitations:\nInterrupt handlers run asynchronously and interrupt other potentially important code, including other interrupt handlers. Therefore, to avoid stalling the interrupted code for too long, interrupt handlers need to run as quickly as possible. Interrupt handlers are often timing-critical because they deal with hardware. Interrupt handlers do not run in process context. Therefore, they cannot block. This limits what they can do. Operating systems need a quick, asynchronous, and simple mechanism for immediately responding to hardware and performing any time-critical actions. Interrupt handlers serve this function well. Less critical work can and should be deferred to a later point when interrupts are enabled.\nConsequently, managing interrupts is divided into two parts, or halves. Interrupt handlers are the top halves.\nBottom Halves # The job of bottom halves is to perform any interrupt-related work not performed by the interrupt handler. You want the interrupt handler to perform as little work as possible and in turn be as fast as possible. By offloading as much work as possible to the bottom half, the interrupt handler can return control of the system to whatever it interrupted as quickly as possible.\nThe interrupt handler must perform some of the work. For example, the interrupt handler almost assuredly needs to acknowledge to the hardware the receipt of the interrupt. It may need to copy data to or from the hardware. This work is timing sensitive, so it makes sense to perform it in the interrupt handler. Almost anything else can be performed in the bottom half. For example, if you copy data from the hardware into memory in the top half, it makes sense to process it in the bottom half.\nNo hard and fast rules exist about what work to perform where. The decision is left entirely up to the device-driver author.\nThe point of a bottom half is not to do work at some specific point in the future, but simply to defer work until any point in the future when the system is less busy and interrupts are again enabled. Often, bottom halves run immediately after the interrupt returns. The key is that they run with all interrupts enabled.\nA World of Bottom Halves # While the top half is implemented entirely via the interrupt handler, multiple mechanisms are available for implementing a bottom half. These mechanisms are different interfaces and subsystems that enable you to implement bottom halves.\nThe Original \u0026ldquo;Bottom Half\u0026rdquo; # In the beginning, Linux provided only the \u0026ldquo;bottom half\u0026rdquo; for implementing bottom halves. This name was logical because at the time that was the only means available for deferring work. The infrastructure was also known as BH to avoid confusion with the generic term bottom half.\nThe BH interface was simple. It provided a statically created list of 32 bottom halves for the entire system. Each BH was globally synchronized. No two could run at the same time, even on different processors. This was simple and easy to use, but was also inflexible and a bottleneck.\nTask Queues # The kernel developers later introduced task queues both as a method of deferring work and as a replacement for the BH mechanism.\nThe kernel defined a family of queues.\nEach queue contained a linked list of functions to call. The queued functions were run at certain times, depending on which queue they were in. Drivers could register their bottom halves in the appropriate queue. This worked fairly well, but it was still too inflexible to replace the BH interface entirely. It also was not lightweight enough for performance-critical subsystems, such as networking.\nSoftirqs and Tasklets # The softirqs and tasklets were later introduced to completely replace the BH interface.\nSoftirqs are a set of statically defined bottom halves that can run simultaneously on any processor; even two of the same type can run concurrently. Tasklets are flexible, dynamically created bottom halves built on top of softirqs. Two different tasklets can run concurrently on different processors, but two of the same type of tasklet cannot run simultaneously. Tasklets are a good trade-off between performance and ease of use. For most bottom-half processing, the tasklet is sufficient. Softirqs are useful when performance is critical, such as with networking. Using softirqs requires more care, however, because two of the same softirq can run at the same time. In addition, softirqs must be registered statically at compile time. Conversely, code can dynamically register tasklets.\nAll BH users were converted to the other bottom-half interfaces. Additionally, the task queue interface was replaced by the work queue interface. Work queues are a simple yet useful method of queuing work to later be performed in process context.\nConsequently, the 2.6 kernel has three bottom-half mechanisms in the kernel:\nSoftirqs Tasklets Work queues Tasklets are built on softirqs and work queues are their own subsystem.\nKernel Timers # Kernel timers is another mechanism for deferring work. Unlike other mechanisms seen previously, timers defer work for a specified amount of time. That is, although the tools discussed before are useful to defer work to any time but now, you use timers to defer work until at least a specific time has elapsed. Therefore, timers have also other uses that Bottom Halves.\nSoftirqs # Softirqs are rarely used directly; tasklets, which are built on softirqs are a much more common form of bottom half.\nImplementing Softirqs # Softirqs are statically allocated at compile time. Unlike tasklets, you cannot dynamically register and destroy softirqs. A 32-entry array is used to store softirqs, registered softirq consumes one entry in the array. Consequently, there are predefined number of registered softirqs that is statically determined at compile time and cannot be changed dynamically. The kernel enforces a limit of 32 registered softirqs. In the current kernel, only nine exist.\nA softirq never preempts another softirq. The only event that can preempt a softirq is an interrupt handler. Another softirq (even the same one) can run on another processor, however.\nExecuting Softirqs # A registered softirq must be marked before it will execute. Usually, an interrupt handler marks its softirq for execution before returning. This is called raising the softirq. Then, at a suitable time, the softirq runs.\nPending softirqs are checked for and executed in the following places:\nIn the return from hardware interrupt code path. In the ksoftirqd kernel thread. In any code that explicitly checks for and executes pending softirqs, such as the networking subsystem. Using Softirqs # Softirqs are reserved for the most timing-critical and important bottom-half processing on the system.\nCurrently, only two subsystems directly use softirqs:\nNetworking devices Block devices Additionally, kernel timers and tasklets are built on top of softirqs.\nTasklets # Tasklets are a bottom-half mechanism built on top of softirqs. As mentioned, they have nothing to do with tasks. Tasklets are similar in nature and behavior to softirqs, but have a simpler interface and relaxed locking rules.\nImplementing Tasklets # Because tasklets are implemented on top of softirqs, they are softirqs. Tasklets are represented by two of the nine softirqs:\nHI_SOFTIRQ. TASKLET_SOFTIRQ. Since softirqs are sorted by priority, the HI_SOFTIRQ-based tasklets run prior to the TASKLET_SOFTIRQ-based tasklets.\nThe implementation of tasklets is simple, but rather clever:\nAll tasklets are multiplexed on top of two softirqs, HI_SOFTIRQ and TASKLET_SOFTIRQ. When a tasklet is scheduled, the kernel raises one of these softirqs. These softirqs, in turn, are handled by special functions that then run any scheduled tasklets. The special functions ensure that only one tasklet of a given type runs at the same time. However, other tasklets can run simultaneously. Tasklets are dynamically created and, as with softirqs, cannot sleep. You cannot use semaphores or other blocking functions in a tasklet. Tasklets also run with all interrupts enabled, so you must take precautions (for example, disable interrupts and obtain a lock) if your tasklet shares data with an interrupt handler. Unlike softirqs, two of the same tasklets never run concurrently, though two different tasklets can run at the same time on two different processors. If a tasklet shares data with another tasklet or softirq, proper locking need to be used.\nAfter a tasklet is scheduled, it runs once at some time in the near future. If the same tasklet is scheduled again, before it has had a chance to run, it still runs only once. If it is already running, for example on another processor, the tasklet is rescheduled and runs again. As an optimization, a tasklet always runs on the processor that scheduled it, making better use of the processor\u0026rsquo;s cache.\nksoftirqd # Softirq processing is aided by a set of per-processor kernel threads. These kernel threads help in the processing of softirqs when the system is overwhelmed with softirqs. Because tasklets are implemented using softirqs, the following discussion applies equally to softirqs and tasklets.\nAs described, the kernel processes softirqs in a number of places, most commonly on return from handling an interrupt. There are two characteristics with softirqs:\nSoftirqs might be raised at high rates, such as during heavy network traffic. Softirq functions can reactivate themselves. That is, while running, a softirq can raise itself so that it runs again. For example, the networking subsystem\u0026rsquo;s softirq raises itself. The issue is that in an high load environments, in which many softirqs continually reactivate themselves:\nIf the Kernel keep handling softirqs, it might not accomplish much else. Resulting in user-space programs being starved of processor time. If the Kernel stops handling softirqs, it prevents starving user-space, but it does starve the softirqs and does not take good advantage of an idle system. The solution ultimately implemented in the kernel is to not immediately process reactivated softirqs. Instead, if the number of softirqs grows excessive, the kernel wakes up a family of kernel threads to handle the load. The kernel threads run with the lowest possible priority (nice value of 19), which ensures they do not run in lieu of anything important.\nThe advantage it brings are:\nThe concession prevents heavy softirq activity from completely starving user-space of processor time. It also ensures that excessive softirqs do run eventually. On an idle system the softirqs are handled rather quickly because the kernel threads will schedule immediately. There is one thread per processor. Having a thread on each processor ensures an idle processor, if available, can always service softirqs.\nWork Queues # Work queues are a different form of deferring work. Work queues defer work into a kernel thread; this bottom half always runs in process context. Code deferred to a work queue has all the usual benefits of process context. Most importantly, work queues are schedulable and can therefore sleep.\nNormally, it is easy to decide between using work queues and softirqs/tasklets:\nIf the deferred work needs to sleep, work queues are used. If the deferred work need not sleep, softirqs or tasklets are used. If you need a schedulable entity to perform your bottom-half processing, you need work queues. They are the only bottom-half mechanisms that run in process context, and thus the only ones that can sleep. This means they are useful for situations in which you need to allocate a lot of memory, obtain a semaphore, or perform block I/O. If you do not need a kernel thread to handle your deferred work, consider a tasklet instead.\nImplementing Work Queues # In its most basic form, the work queue subsystem is an interface for creating kernel threads to handle work queued from elsewhere. These kernel threads are called worker threads. Work queues enables your driver to create a special worker thread to handle deferred work. The work queue subsystem, however, implements and provides a default worker thread for handling work. Therefore, in its most common form, a work queue is a simple interface for deferring work to a generic kernel thread.\nThe default worker threads are called events/n where n is the processor number; there is one per processor.\nThe default worker thread handles deferred work from multiple locations. Many drivers in the kernel defer their bottom-half work to the default thread. Unless a driver or subsystem has a strong requirement for creating its own thread, the default thread is preferred.\nUsing a dedicated worker thread might be advantageous if the driver performs large amounts of processing in the worker thread. Processor-intense and performance-critical work might benefit from its own thread. This also lightens the load on the default threads, which prevents starving the rest of the queued work.\nTasks for work threads are strung into a linked list, one for each type of queue on each processor. For example, there is one list of deferred work for the generic thread, per processor.\nWhen a worker thread wakes up (the thread\u0026rsquo;s state is set to TASK_RUNNING), it runs any work in its list. A worker thread executes the work in process context. By default, interrupts are enabled and no locks are held. If needed, it can sleep. As it completes work, it removes the corresponding task entries from the linked list. When the list is empty, it goes back to sleep (the thread\u0026rsquo;s state is set to TASK_INTERRUPTIBLE). Despite running in process context, the work handlers cannot access user-space memory because there is no associated user-space memory map for kernel threads. The kernel can access user memory only when running on behalf of a user-space process, such as when executing a system call. Only then is user memory mapped in.\nWhich Bottom Half Should I Use? # Softirqs: least serialization, for highly threaded code # Softirqs, by design, provide the least serialization. This requires softirq handlers to go through extra steps to ensure that shared data is safe because two or more softirqs of the same type may run concurrently on different processors. If the code in question is already highly threaded, such as in a networking subsystem that is chest-deep in per-processor variables, softirqs make a good choice. They are certainly the fastest alternative for timing-critical and high-frequency uses.\nTasklets: simple interface, for less threaded code # Tasklets make more sense if the code is not finely threaded. They have a simpler interface and, because two tasklets of the same type might not run concurrently, they are easier to implement. Tasklets are effectively softirqs that do not run concurrently. A driver developer should always choose tasklets over softirqs, unless prepared to utilize per-processor variables or similar magic to ensure that the softirq can safely run concurrently on multiple processors.\nWork queues: process context # If the deferred work needs to run in process context, the only choice of the three is work queues. If process context is not a requirements (specifically, if you have no need to sleep), softirqs or tasklets are perhaps better suited. Work queues involve the highest overhead because they involve kernel threads and, therefore, context switching. This doesn\u0026rsquo;t mean they are inefficient, but in light of thousands of interrupts hitting per second (as the networking subsystem might experience), other methods make more sense. However, work queues are sufficient for most situations.\nSoftirqs vs. tasklets vs. work queues # In terms of ease of use, work queues wins. Using the default events queue is easy. Next come tasklets, which also have a simple interface. Coming in last are softirqs, which need to be statically created and require careful thinking with their implementation.\nIf the driver need a schedulable entity to perform the deferred work, and if fundamentally, it need to sleep for any reason, then work queues are your only option. Otherwise, tasklets are preferred. Only if scalability becomes a concern the driver could use softirqs.\nThe following table is a comparison between the three bottom-half interfaces.\nBottom Half Context Inherent Serialization Softirq Interrupt None Tasklets Interrupt Against the same tasklet Work Queues Process None (scheduled as process context) Locking Between the Bottom Halves # It is crucial to protect shared data from concurrent access while using bottom halves, even on a single processor machine. A bottom half can run at virtually any moment.\nOne benefit of tasklets is that they are serialized with respect to themselves. The same tasklet will not run concurrently, even on two different processors. This means you do not have to worry about intra-tasklet concurrency issues. Inter-tasklet concurrency (when two different tasklets share the same data) requires proper locking.\nOn the other hand, because softirqs provide no serialization, (even two instances of the same softirq might run simultaneously), all shared data needs an appropriate lock.\nIf process context code and a bottom half share data, you need to do both of the following before accessing the data:\nDisable bottom-half processing. Obtain a lock. If interrupt context code and a bottom half share data, you need to do both of the following before accessing the data:\nDisable interrupts. Obtain a lock. In both cases, this ensures local and SMP protection and prevents a deadlock.\nBecause work queues run in process context, there are no issues with asynchronous execution, and thus, there is no need to disable them. On the other hand, protecting shared data is the same as in any process context and requires locking. However, because softirqs and tasklets can occur asynchronously (for example, on return from handling an interrupt), kernel code may need to disable them.\nThe locking is no different from normal kernel code because work queues run in process context.\n"},{"id":9,"href":"/grimoire/docs/linux-kernel-development/9-an-introduction-to-kernel-synchronization/","title":"9. An Introduction to Kernel Synchronization","section":"Linux Kernel Development","content":" An Introduction to Kernel Synchronization # In a shared memory application, developers must ensure that shared resources are protected from concurrent access. The kernel is no exception. Symmetrical multiprocessing is supported in Linux. Multiprocessing support implies that kernel code can simultaneously run on two or more processors.\nShared resources require protection from concurrent access because if multiple threads of execution access and manipulate the data at the same time, the threads may overwrite each other\u0026rsquo;s changes or access data while it is in an inconsistent state. Consequently, without protection, code in the kernel, running on two different processors, can simultaneously access shared data at exactly the same time.\nThe term threads of execution implies any instance of executing code. For example, this includes any of the following:\nA task in the kernel An interrupt handler A bottom half A kernel thread Concurrent access of shared data often results in instability is hard to track down and debug.\nThe Linux kernel is preemptive. This implies that (in the absence of protection) the scheduler can preempt kernel code at virtually any point and reschedule another task. A number of scenarios enable for concurrency inside the kernel, and they all require protection.\nCritical Regions and Race Conditions # Code paths that access and manipulate shared data are called critical regions (also called critical sections). It is usually unsafe for multiple threads of execution to access the same resource simultaneously. To prevent concurrent access during critical regions, the programmer must ensure that code executes atomically, which means that operations complete without interruption as if the entire critical region were one indivisible instruction.\nIf two threads of execution simultaneously execute within the same critical region, it is called a race condition, so-named because the threads raced to get there first. Debugging race conditions is often difficult because they are not easily reproducible. Ensuring that unsafe concurrency is prevented and that race conditions do not occur is called synchronization.\nWhy Do We Need Protection? # To best understand the need for synchronization, look at the ubiquity of race conditions. The first example is a real-world case: an ATM (Automated Teller Machine, called a cash machine).\nThe ATM works as follows:\nCheck whether the deduction is possible. Compute the new total funds. Finally execute the physical deduction. Assuming a user, with a $100 in the bank, and user\u0026rsquo;s spouse are initiating withdrawal of $75.\nBoth the user and user\u0026rsquo;s spouse initiate withdrawal at the same time. Both transactions verify that sufficient funds exist, in both cases, the user has $100 in the bank for the withdrawal of $75. In both transactions, the new computed total fund would be $25. In both cases, the users would be getting $75, for a total of $150, while still having in the bank $25. This means that the user funds would increase from $100 to $175. Clearly, financial institutions must ensure that this can never happen. They must lock the account during certain operations, making each transaction atomic with respect to any other transaction. Such transactions must occur in their entirety, without interruption, or not occur at all.\nThe Single Variable # Consider a simple shared resource, a single global integer, and a simple critical region, the operation of merely incrementing it: i++.\nThis might translate into machine instructions to the computer\u0026rsquo;s processor that resemble the following:\nGet the current value of i and copy it into a register. Add one to the value stored in the register. Write back to memory the new value of i. Assume that there are two threads of execution, both enter this critical region, and the initial value of i is 7. The desired outcome is then similar to the following (with each row representing a unit of time):\nThread 1 Thread 2 get i (7) — increment i (7 -\u0026gt; 8) — write back i (8) — — get i (8) — increment i (8 -\u0026gt; 9) — write back i (9) As expected, 7 incremented twice is 9.\nHowever, another possible outcome is the following:\nThread 1 Thread 2 get i (7) get i (7) increment i (7 -\u0026gt; 8) — — increment i (7 -\u0026gt; 8) write back i (8) — — write back i (9) If both threads of execution read the initial value of i before it is incremented, both threads increment and save the same value. As a result, the variable i contains the value 8 when, in fact, it should now contain 9. This is one of the simplest examples of a critical region. The solution is simple. We merely need a way to perform these operations in one indivisible step. Most processors provide an instruction to atomically read, increment, and write back a single variable.\nUsing this atomic instruction, the only possible outcome is:\nThread 1 Thread 2 increment \u0026amp; store i (7 -\u0026gt; 8) — — increment \u0026amp; store i (8 -\u0026gt; 9) Thread 2 could also be incrementing i first, but the result would be the same.\nIt would never be possible for the two atomic operations to interleave. The processor would physically ensure that it was impossible. Using such an instruction would alleviate the problem. The kernel provides a set of interfaces that implement these atomic instructions, which are discussed in the next chapter.\nLocking # Assuming a queue of requests that needs to be serviced and the implementation is a linked list, in which each node represents a request. Two functions manipulate the queue:\nOne function adds a new request to the tail of the queue. One function removes a request from the head of the queue and service request. Requests are continually being added, removed, and serviced, since various parts of the kernel invoke these two functions. Manipulating the request queues certainly requires multiple instructions. If one thread attempts to read from the queue while another is in the middle of manipulating it, the reading thread will find the queue in an inconsistent state. It should be apparent the sort of damage that could occur if access to the queue could occur concurrently. Often, when the shared resource is a complex data structure, the result of a race condition is corruption of the data structure.\nAlthough it is feasible for a particular architecture to implement simple instructions, such as arithmetic and comparison, atomically it is ludicrous for architectures to provide instructions to support the indefinitely sized critical regions that would exist in the example.\nWhat is needed is a way of making sure that only one thread manipulates the data structure at a time, a mechanism for preventing access to a resource while another thread of execution is in the marked region. A lock provides such a mechanism. Threads hold locks; locks protect data.\nWhenever there was a new request to add to the queue, the thread would first obtain the lock. Then it could safely add the request to the queue and ultimately release the lock. When a thread wanted to remove a request from the queue, it would also obtain the lock. Then it could read the request and remove it from the queue. Finally, it would release the lock. Any other access to the queue would similarly need to obtain the lock. Because the lock can be held by only one thread at a time, only a single thread can manipulate the queue at a time. If a thread comes along while another thread is already updating it, the second thread has to wait for the first to release the lock before it can continue. The lock prevents concurrency and protects the queue from race conditions.\nThread 1 Thread 2 try to lock the queue try to lock the queue succeeded: acquired lock failed: waiting\u0026hellip; access queue\u0026hellip; waiting\u0026hellip; unlock the queue waiting\u0026hellip; — succeeded: acquired lock — access queue\u0026hellip; — unlock the queue Notice that locks are advisory and voluntary. Locks are entirely a programming construct that the programmer must take advantage of. Nothing prevents you from writing code that manipulates the fictional queue without the appropriate lock, but such a practice would eventually result in a race condition and corruption.\nLocks come in various shapes and sizes. Linux alone implements a handful of different locking mechanisms. The most significant difference between the various mechanisms is the behavior when the lock is unavailable because another thread already holds it:\nSome lock variants busy wait (spin in a tight loop, checking the status of the lock over and over, waiting for the lock to become available). Other locks put the current task to sleep until the lock becomes available. The itself lock does not solve the problem; it simply shrinks the critical region down to just the lock and unlock code: probably much smaller, but still a potential race. What if a lock is aquired by two threads at the exact same time?\nFortunately, locks are implemented using atomic operations that ensure no race exists. A single instruction can verify whether the key is taken and, if not, seize it. How this is done is architecture-specific, but almost all processors implement an atomic test and set instruction that tests the value of an integer and sets it to a new value only if it is zero. A value of zero means unlocked.\nCauses of Concurrency # In user-space, programs are scheduled preemptively at the will of the scheduler. Because a process can be preempted at any time and another process can be scheduled onto the processor, a process can be involuntarily preempted in the middle of accessing a critical region. If the newly scheduled process then enters the same critical region (for example, if the two processes manipulate the same shared memory or write to the same file descriptor), a race can occur. The same problem can occur with multiple single-threaded processes sharing files, or within a single program with signals, because signals can occur asynchronously. This type of concurrency in which two things do not actually happen at the same time but interleave with each other is called pseudo-concurrency.\nWith a symmetrical multiprocessing machine (multiple processors or cores), two processes can actually be executed in a critical region at the exact same time. That is called true concurrency. Although the causes and semantics of true versus pseudo concurrency are different, they both result in the same race conditions and require the same sort of protection.\nThe kernel has similar causes of concurrency:\nInterrupts: An interrupt can occur asynchronously at almost any time, interrupting the currently executing code. Softirqs and tasklets: The kernel can raise or schedule a softirq or tasklet at almost any time, interrupting the currently executing code. Kernel preemption: Because the kernel is preemptive, one task in the kernel can preempt another. Sleeping and synchronization with user-space: A task in the kernel can sleep and thus invoke the scheduler, resulting in the running of a new process. Symmetrical multiprocessing: Two or more processors can execute kernel code at exactly the same time. This is the reason locking and synchronization mechanisms are needed.\nDeadlocks # A deadlock is a condition involving one or more threads of execution and one or more resources, such that each thread waits for one of the resources, but all the resources are already held.\nThe threads all wait for each other, but they never make any progress toward releasing the resources that they already hold. Therefore, none of the threads can continue, which results in a deadlock.\nA good analogy is a four-way traffic stop. If each car at the stop decides to wait for the other cars before going, no car will ever proceed, and we have a traffic deadlock.\nThe most common example is with two threads and two locks, which is often called the deadly embrace or the ABBA deadlock:\nThread 1 Thread 2 acquire lock A acquire lock B try to acquire lock B try to acquire lock A wait for lock B wait for lock A Each thread is waiting for the other, and neither thread will ever release its original lock; therefore, neither lock will become available.\nPrevention of deadlock scenarios is important. Although it is difficult to prove that code is free of deadlocks, the following rules can help in writing a deadlock-free code:\nImplement lock ordering. Nested locks must always be obtained in the same order. This prevents the deadly embrace deadlock. Prevent starvation. Does this code always finish? If foo does not occur, will bar wait forever? Do not double acquire the same lock. Design for simplicity. Complexity locking schemes invites deadlocks. The first point is most important and worth stressing. If two or more locks are acquired at the same time, they must always be acquired in the same order. The order of unlock does not matter with respect to deadlock, although it is common practice to release the locks in an order inverse to that in which they were acquired.\nPreventing deadlocks is important. The Linux kernel has some basic debugging facilities for detecting deadlock scenarios in a running kernel, which are discussed in the next chapter.\nContention and Scalability # The term lock contention, or simply contention, describes a lock currently in use but that another thread is trying to acquire. A lock that is highly contended often has threads waiting to acquire it. High contention can occur because a lock is frequently obtained, held for a long time after it is obtained, or both. Because a lock\u0026rsquo;s job is to serialize access to a resource, they can slow down a system\u0026rsquo;s performance. A highly contended lock can become a bottleneck in the system, quickly limiting its performance. However, a solution to high contention must continue to provide the necessary concurrency protection, because locks are also required to prevent the system from tearing itself to shreds.\nScalability is a measurement of how well a system can be expanded. In operating systems, we talk of the scalability with a large number of processes, a large number of processors, or large amounts of memory. We can discuss scalability in relation to virtually any component of a computer to which we can attach a quantity. Ideally, doubling the number of processors should result in a doubling of the system\u0026rsquo;s processor performance, which, of course, is never the case.\nThe granularity of locking is a description of the size or amount of data that a lock protects:\nA very coarse lock protects a large amount of data, e.g. an entire subsystem’s set of data structures. On the other hand, a very fine-grained lock protects a small amount of data, e.g. only a single element in a larger structure. Scalability improvement is generally a good thing because it improves Linux\u0026rsquo;s performance on larger and more powerful systems. However, rampant scalability \u0026ldquo;improvements\u0026rdquo; can lead to a decrease in performance on smaller SMP and UP machines, because smaller machines may not need such fine-grained locking but will nonetheless need to put up with the increased complexity and overhead.\n"}]