[{"id":0,"href":"/grimoire/docs/linux-kernel-development/","title":"Linux Kernel Development","section":"Docs","content":" 1. Introduction to the Linux Kernel 2. Getting Started with the Kernel 3. Process Management 4. Process Scheduling 5. System Calls "},{"id":1,"href":"/grimoire/docs/linux-kernel-development/1-introduction-to-the-linux-kernel/","title":"1. Introduction to the Linux Kernel","section":"Linux Kernel Development","content":" Introduction to Unix # Unix is a family of operating systems featuring a similar API and design, including Unix, BSD, Solaris and Linux.\nSome Unix characteristics that are at the core of its strength:\nUnix is simple: only hundreds of system calls and have a straightforward, even basic, design. In Unix, everything is a file: simplifies the manipulation of data and devices into a set of core system calls: open(),read(), write(), lseek(), and close(). Unix has robust IPC (Interprocess communication) systems. Overview of Operating Systems and Kernels # The operating system is the parts of the system responsible for basic use and administration. The kernel is the innermost portion of the operating system. It is the core internals: the software that provides basic services for all other parts of the system, manages hardware, and distributes system resource.\nAn OS contains:\nKernel: Interrupt handlers to service interrupt requests. A scheduler to share processor time among multiple processes. A memory management system to manage process address spaces. System services such as networking and interprocess communication. Device drivers Boot loader Command shell User interfaces Basic utilities Kernel-space and User-space # Kernel-space: on modern systems with protected memory management units, the kernel typically resides in an elevated system state, which includes a protected memory space and full access to the hardware. This system state and memory space is collectively referred to as kernel-space.\nUser-space: applications execute in user-space, where they can access a subset of the machine’s available resources and can perform certain system functions, directly access hardware, access memory outside of that allotted them by the kernel, or otherwise misbehave.\nWhen executing kernel code, the system is in kernel-space executing in kernel mode. When running a regular process, the system is in user-space executing in user mode.\nApplications running on the system comminicate with the kernel using system calls (Figure 1.1)\nBasically, an application will use a C library to use the system call interface. The system calls will allow the Kernel to do operation on behalf of the application. Furthermore, the application is said to be executing a system call in kernel-space, and the kernel is running in process context.\nSome function of this C library provide many feature not found in system calls, E.G. the printf() function will not only wall the write() system call but will also do formating, etc\u0026hellip;\nOn the other hand, the open() function simply use the open() system call. Other functions of this C library does not use system calls at all, E.G. strcpy().\nThe Kernel also manage system\u0026rsquo;s hardware. When the hardware want to communicate, it will send an hardware interrupt to the CPU, which will in turn interrupt the Kernel, which will in turn operate a context switch to execute the interrupt handler.\ngraph LR; A[Hardware] --\u003e|Interrupt lines| B[CPU] B[CPU] --\u003e|Interrupt| C[Kernel] C[Kernel] --\u003e|Interrupt Handler| D[Run the interrupt handler code] A number identifies each interrupt, the kernel reads this number and then execute the handler code.\nFor example:\nsequenceDiagram participant Y as You participant KC as Keyboard Controller participant C as CPU participant KR as Kernel participant IH as Interrupt Handler Process Y-\u003e\u003eKC: Type on keyboard KC-\u003e\u003eC: Issue an hardware interrupt C-\u003e\u003eKR: Forward interrupt with interrupt number KR-\u003e\u003eIH: Run interrupt handler according to interrupt number IH-\u003e\u003eKC: Process Keyboard data IH-\u003e\u003eKC: Let it know it's ready for more data Running interrupt handlers put the Kernel in a special context, the interrupt context, not associated with any process.\nEach CPU in the Linux Kernel is doing exaclty one of the following:\nIn user-space, running user code in a process. In kernel-space, in process context, running kernel code on behalf of a specific process. In kernel-space, in interrupt context, not associated with any process, handling an interrupt. This list is inclusive, every cases fit any of the previous element. E.G. Even when the Kernel is idle, the CPU is actually running an idle process\nMonolithic Kernels vs Microkernels # Most Unix kernels are monolithics and although the Linux kernel is monolithic, it can dynamically load and unload kernel modules on demand.\nMonolithic Kernels are:\nSimpler than Microkernels. Implemented as a single process. Exist as a single binary. Runs in kernel mode. Runs everything in the same address space. Because of this, communication is trivial, the kernel simply invoke functions. Microkernels are:\nBroken down into separate process, called servers Only some servers runs in priviledged mode, if they need it, the rest runs in user mode. Because of this, direct function invocation is impossible. Microkernel uses message passing, an Inter Process communication (IPC) method. Servers invoke services from other servers, using message passing. Microkernels are modular, if one server crash, it doesn\u0026rsquo;t crash the all kernel. IPC (message passing) and context switches (user mode vs privileged mode) involve some serious overhead To avoid this, modern Microkernels (Windows NT and mac OS) runs every servers in privileged mode, defeating the primary purpose of microkernels. "},{"id":2,"href":"/grimoire/docs/linux-kernel-development/2-getting-started-with-the-kernel/","title":"2. Getting Started with the Kernel","section":"Linux Kernel Development","content":" TODO # "},{"id":3,"href":"/grimoire/docs/linux-kernel-development/3-process-management/","title":"3. Process Management","section":"Linux Kernel Development","content":" The Process # A process, or task in the Kernel, is a program (object code) that is currently running, but a program is not a process, two or more processes can actually run the same program at the same time.\nThey are more than juste code (text section), they also include a set of resources such as:\nOpen files. Pending signals. Internal Kernel data. Processor state. A memory address space with one or more mappings. One or more threads of execution. A data section containing global variables. Threads of execution, or threads, are the objects of activity within the process. Each thread includes:\nA unique program counter. A Process stack. A set of processor registers. The kernel schedules individual threads, not processes.\nProcess that runs more than one thread are call multithreaded.\nOn modern operating systems, processes provide two virtualizations:\nA virtualized processor: The virtual processor gives the process the illusion that it alone monopolizes the system, despite possibly sharing the processor among hundreds of other processes. A virtual memory: Virtual memory lets the process allocate and manage memory as if it alone owned all the memory in the system. Note that threads share the virtual memory abstraction, but they each receives their own virtualized processor.\nProcess lifecycle # The process lifecycle goes as follow:\nfork(): A parent process will run the fork() system call, effectively cloning himself (fork() is implemented using clone()) into a child process. The parent process will then resume execution and the child process will start execution at the same place, where the fork() system call returns. fork() will in fact return twice, once in the parent process, and once in the child process. At some point however, the parent process will have to run a wait() type system call to reap its exited child process. exec(): After its creation, the child process will run one of the exec() system calls that will create a new address space and load a new program into it. exit(): Finally, when a process exits, it run the exit() system call. This will terminate the process and free all its ressources. The process will then be placed in a zombie state and until its parent process run the wait4() system call. Process Descriptor and the Task Structure # The kernel stores the list of processes in a circular doubly linked list called the task list. Each element of this list is called a process descriptor that will contain all informations about a given process.\nThe system identifies processes by a unique process identification value or PID, stored in the process descriptor. The PID is typically an integer going up to 32,768. This maximum value is important because it is essentially the maximum number of processes that may exist concurrently on the system.\nThe Kernel will affect PID to each process in an ascending order, this creates the notion that the higher the PID, the later the process started. But if the maximum number of PID is reached, then it will wrap back and try to affect freed PID to new process, detroying this useful notion.\nYou can also increase this value up to four millons, by modifying /proc/sys/kernel/pid_max. Take note that this might break compatibility with older programs.\nProcess State # The state field of the process descriptor describes the current condition of the process. Each process on the system is in exactly one of the five following states:\nTASK_RUNNING: The process is runnable, it is either currently running or on a runqueue waiting to run. This is the only possible state for a process executing in user-space. It can also apply to a process in kernel-space that is actively running. TASK_INTERRUPTIBLE: The process is sleeping, or blocked, waiting for some condition to exist. When this condition exists, the kernel sets the process’s state to TASK_RUNNING. The process also awakes prematurely and becomes runnable if it receives a signal. TASK_UNINTERRUPTIBLE: This state is identical to TASK_INTERRUPTIBLE except that it does not wake up and become runnable if it receives a signal. This is used in situations where the process must wait without interruption or when the event is expected to occur quite quickly. Because the task does not respond to signals in this state, TASK_UNINTERRUPTIBLE is less often used than TASK_INTERRUPTIBLE. __TASK_TRACED: The process is being traced by another process, such as a debugger (like strace), using the ptrace() system call. __TASK_STOPPED: Process execution has stopped. The task is not running nor is it eligible to run. This occurs if the task receives the SIGSTOP, SIGTSTP, SIGTTIN, or SIGTTOU signal or if it receives any signal while it is being debugged. Take note that a process in the TASK_UNINTERRUPTIBLE state will not even respond to SIGKILL. This state is represented by the famous D state in the ps command.\nProcess Context # The goal of a process is to execute program code. This code is read from an executable file and executed from within the program\u0026rsquo;s address space. Normal program execution occurs in user-space, when a program executes a system call or triggers an exception, it enters kernel-space. At this point, the kernel is said to be “executing on behalf of the process” and the kernel is in process context.\nUpon exiting the kernel, the process resumes execution in user-space, unless a higher-priority process has become runnable in the interim, in which case the scheduler is invoked to select the higher priority process.\nSystem calls and exception handlers are well-defined interfaces into the kernel. A process can begin executing in kernel-space only through one of these interfaces. All access to the kernel is through these interfaces.\nThe Process Family Tree # There is a hierarchy between processes. All processes are descendant of the init process, whose PID is 1. The Kernel will start the init process as the last step of the boot procedure. The init process will then read the initscripts and start all other programs necessary to complete the boot procedure.\nEvery process on the system has exactly one parent. Likewise, every process has zero or more children. Processes that are all direct children of the same parent are called siblings.\nProcess Creation # Process creation in Unix is unique. Unix takes the unusual approach of separating these steps into two distinct system calls: fork() and exec().\nfork() will create a child process that is a copy of the current task. It only differs by its PID, PPID (Parent PID) and some other resources, such as pending signals which are not inherited. exec() will load a new program in the task address space and start executing it. Copy-on-Write # Traditionnaly, upon fork(), all resources owned by the parent process are copied to the child process memory space. Even worse, if the first call of the child process is exec(), then all the copied data is discarded to allow the new program to run. To optimize this, Linux\u0026rsquo;s fork() is using what is called copy-on-write pages.\nCopy-on-write, or COW is working like this:\nWhen the fork() system call is invoked, rather than duplicating the address space, each process, parent and child, will share a single read-only copy. The data is marked so that if it is written to, a duplicate is made and each process will get its own copy, therefor, copy-on-write. Only the pages that are written to are duplicated, the rest of the memory space will still be shared. If the pages are never written, for example if the child calls exec() directly after the fork(), then the pages are never copied. To make sure that the child process is given the possibility to run exec() before the parent write to the data, the Kernel will schedule first the child process.\nThreads # Thread provide multiple threads of execution within the same program in a shared memory address space. They can also share open files and other resources. Threads enable concurrent programming and, on multiple processor systems, true parallelism.\nTo the Linux kernel, there is no concept of a thread. Linux implements all threads as standard processes. The Linux kernel does not provide any special scheduling semantics or data structures to represent threads. Instead, a thread is merely a process that shares certain resources with other processes. Each thread has appears to the kernel as a normal process and just happen to share resources, such as an address space, with other processes.\nKernel Threads # It is often useful for the kernel to perform some operations in the background. The kernel accomplishes this via kernel threads, standard processes that exist solely in kernel-space. The significant difference between kernel threads and normal processes is that kernel threads do not have an address space. They operate only in kernel-space and do not context switch into user-space. Kernel threads, however, are schedulable and preemptable, the same as normal processes.\nKernel threads are created on system boot by other kernel threads. Indeed, a kernel thread can be created only by another kernel thread.\nProcess Termination # When a process terminates, the kernel:\nReleases the resources owned by the process. Notifies the process’s parent of the termination of its child. Generally, process destruction is self-induced. It occurs when the process calls the exit() system call. A process can also terminate involuntarily, this occurs when the process receives a signal or exception it cannot handle or ignore.\nUpon deletion, the task enters the EXIT_ZOMBIE exit state. The only memory consumed is the one in the task list of the kernel. After the parent reap the child using the wait() system call, the kernel fully removes the task from the task list and the process is deleted.\nParentless Tasks, AKA Zombie Processes # If a parent exits before its children, some mechanism must exist to reparent any child tasks to a new process, or else parentless terminated processes would forever remain zombies, wasting system memory. The solution is to reparent a task’s children on exit to either another process in the current thread group or, if that fails, the init process.\nWith the process successfully reparented, there is no risk of stray zombie processes. The init process routinely calls wait() on its children, cleaning up any zombies assigned to it.\n"},{"id":4,"href":"/grimoire/docs/linux-kernel-development/4-process-scheduling/","title":"4. Process Scheduling","section":"Linux Kernel Development","content":" The Scheduler # The process scheduler decides which process runs, when, and for how long.\nThe process scheduler (or simply the scheduler, to which it is often shortened) divides the finite resource of processor time between the runnable processes on a system. By deciding which process runs next, the scheduler is responsible for best utilizing the system and giving users the impression that multiple processes are executing simultaneously, AKA Multitasking.\nThe ideas behind the scheduler are simple:\nTo best utilize processor time, assuming there are runnable processes, a process should always be running. If there are more runnable processes than processors in a system, some processes will not be running at a given moment. These processes are waiting to run. Deciding which process runs next, given a set of runnable processes, is the fundamental decision that the scheduler must make. Multitasking # A multitasking operating system is one that can simultaneously interleave execution of more than one process.\nOn single processor machines, this gives the illusion of multiple processes running concurrently. On multiprocessor machines, such functionality enables processes to actually run concurrently, in parallel, on different processors. On either type of machine, it also enables many processes to block or sleep, not actually executing until work is available. These processes, although in memory, are not runnable. Instead, such processes utilize the kernel to wait until some event (keyboard input, network data, passage of time, and so on) occurs.\nMultitasking operating systems come in two flavors:\ncooperative multitasking: A process does not stop running until it voluntary decides to do so, it is called yielding. Ideally, processes yield often, giving each runnable process a decent chunk of the processor. preemptive multitasking: The scheduler decides when a process is to cease running and a new process is to begin running, it is called preemption. The time a process runs before it is preempted is usually predetermined, and it is called the timeslice of the process. The process is given a slice of processor\u0026rsquo;s time to run. The shortcomings of the cooperative multitasking approach are manifest, the scheduler cannot make global decisions regarding how long processes run. Processes can monopolize the processor for longer than the user desires, and a hung process that never yields can potentially bring down the entire system.\nOn preemptive multitasking, the scheduler manage each process timeslices, making global decisions for the system (e.g. priorization) and preventing any one process from monopolizing the processor. The timeslice is dynamically calculated as a function of process behavior and configurable system policy.\nLinux, like all Unix variants and most modern operating systems, implements preemptive multitasking.\nLinux\u0026rsquo;s Process Scheduler # The current Linux scheduler is the Completely Fair Scheduler, or CFS. The CFS was introduced as a replacement for the O(1) scheduler, which was performant, but was lacking when dealing with interactive process, like on a desktop for example.\nPolicy # The policy of a scheduler determines what runs when. Therefore it is very important.\nI/O-Bound Versus Processor-Bound Processes # Processes can be classified as two types:\nI/O-bound: A process that spend much of its time waiting on I/O requests (I/O here means any blockable resources, like keyboard inputs or network I/O, not only disk I/O). Such processes are runnable for only short durations before having to wait on requests, like GUI waiting on user inputs. processor-bound: Processor-bound processes spend much of their time executing code. They tend to run until they are preempted because they do not block on I/O requests very often. Linux, aiming to provide good interactive response and desktop performance, optimizes for response time (low latency), thus favoring I/O-bound processes over processor-bound processors. As we will see, this is done in a creative manner that does not neglect processor-bound processes.\nProcess Priority # A common type of scheduling algorithm is priority-based scheduling. The goal is to rank processes based on their worth and need for processor time. Processes with a higher priority run before those with a lower priority, whereas processes with the same priority are scheduled in round-robin (one after the next, repeating).\nThe runnable process with timeslice remaining and the highest priority always runs. Both the user and the system can set a process’s priority to influence the scheduling behavior of the system.\nThe Linux kernel implement two priority ranges:\nNiceness: A number from -20 to +19 with a default at 0. Larger nice values correspond to a lower priority, you are being “nice” to the other processes on the system. Processes with a lower nice value (higher priority) receive a larger proportion of the system’s processor compared to processes with a higher nice value (lower priority). Real-time priority: The values are configurable, but by default range from 0 to 99, inclusive. Higher real-time priority values correspond to a greater priority and all real-time processes are at a higher priority than normal processes. By default, processes are not real-time, their value is \u0026ldquo;-\u0026rdquo;, or null. Timeslice # The timeslice is the numeric value that represents how long a task can run until it is preempted. The scheduler policy must dictate a default timeslice, which is not a trivial exercise.\nToo long a timeslice causes the system to have poor interactive performance, the system will no longer feel as if applications are concurrently executed. Too short a timeslice causes significant amounts of processor time to be wasted on the overhead of switching processes. Furthermore, the conflicting goals of I/O-bound versus processor-bound processes again arise:\nI/O-bound processes do not need longer timeslices (although they do like to run often) Processor-bound processes crave long timeslices (to keep their caches hot). To avoid thoses issues, Linux’s CFS scheduler does not directly assign timeslices to processes. Instead, it will assign to processes a proportion of the processor, therefore, the amount of processor time that a process receives is a function of the load of the system. This assigned proportion is further affected by each process’s nice value. The nice value acts as a weight, changing the proportion of the processor time each process receives.\nWhen a process enters the runnable state, it becomes eligible to run. Whether the process runs immediately, preempting the currently running process, is a function of how much of a proportion of the processor the newly runnable processor has consumed.\nIf it has consumed a smaller proportion of the processor than the currently executing process, it runs immediately, preempting the current process. If not, it is scheduled to run at a later time. The Scheduling Policy in Action # Consider two runnable processes:\nA text editor: I/O-bound. It will spend nearly all its time waiting for user key presses. A video encoder: Processor-bound. Aside from waiting on disk to read the video at first, it will then spend all its time encoding the video, consuming 100% of the processor. The text editor, despite spending most of its time waiting on user input, when the text editor does receive a key press, the user expects the editor to respond immediately. Latency is a primary concern. The video encoder doesn\u0026rsquo;t have strong time constraint, we don\u0026rsquo;t really care if it runs now or at the next CPU cycle. The sooner it finishes the better, but latency is not a primary concern.\nIf both processes are runnable and have the same nicelevel, they would be awarded both with the same proportion of the processor\u0026rsquo;s time. But since the text editor spend a lot of time waiting, it would almost always have more processor time left than the video encoder. And because of that, CFS would let the text editor run in priority, therefore limiting latency.\nThis will allow the text editor to run everytime it\u0026rsquo;s ready, and the video encoder to run the rest of the time, assuming no other process needs to run.\nThe Linux Scheduling Algorithm # Scheduler Classes # The Linux scheduler is modular, enabling different algorithms to schedule different types of processes. This modularity is called scheduler classes. Scheduler classes enable different, pluggable algorithms to coexist, scheduling their own types of processes. Each scheduler class has a priority.\nThe base scheduler code, iterates over each scheduler class in order of priority. The highest priority scheduler class that has a runnable process wins, selecting who runs next. The Completely Fair Scheduler (CFS) is the registered scheduler class for normal processes, called SCHED_NORMAL in Linux.\nFair Scheduling # CFS is based on a simple concept: Model process scheduling as if the system had an ideal, perfectly multitasking processor. In such a system, each process would receive 1/n of the processor’s time, where n is the number of runnable processes. We’d schedule them for infinitely small durations, so that in any measurable period we’d have run all n processes for the same amount of time.\nThe Linux Scheduling Implementation # Time Accounting # All process schedulers must account for the time that a process runs. Most Unix systems do so, as discussed earlier, by assigning each process a timeslice. On each tick of the system clock, the timeslice is decremented by the tick period. When the timeslice reaches zero, the process is preempted in favor of another runnable process with a nonzero timeslice.\nCFS does not have the notion of a timeslice, but it must still keep account for the time that each process runs, because it needs to ensure that each process runs only for its fair share of the processor. This information is embedded in the process descriptor. We discussed the process descriptor in Chapter 3, “Process Management.”\nThe Virtual Runtime # The Virtual Runtime (vruntime) of a process the actual runtime (the amount of time spent running) normalized (or weighted) by the number of runnable processes, in nanoseconds. The Virtual Runtime update is triggered periodically by the system timer and also whenever a process becomes runnable or blocks, becoming unrunnable. In this manner, vruntime is an accurate measure of the runtime of a given process and an indicator of what process should run next.\nProcess Selection # On an ideal processor, the Virtual Runtime of all processes of the same priority would be identical, all tasks would have received an equal, fair share of the processor. In reality, we cannot perfectly multitask, so CFS attempts to balance a process’s Virtual Runtime with a simple rule: When CFS is deciding what process to run next, it picks the process with the smallest vruntime. The CFS maintains a red-black tree for the Virtual Runtimes.\nPreemption and Context Switching # Context switching, the switching from one runnable task to another, is handled by the scheduler. It is called when a new process has been selected to run. It does two basic jobs:\nSwitch the virtual memory mapping from the previous process’s to that of the new process. Switch the processor state from the previous process’s to the current’s. This involves saving and restoring stack information and the processor registers and any other architecture-specific state that must be managed and restored on a per-process basis. The kernel, however, must know when to call the scheduler. If it called it only when code explicitly did so, user-space programs could run indefinitely.\nInstead, the kernel provides the need_resched flag to signify whether a reschedule should be performed.\nThis flag is set by scheduler_tick() (which is ran by a timer) when a process should be preempted. This flag is set by try_to_wake_up() when a process that has a higher priority than the currently running process is awakened. Upon returning to user-space or returning from an interrupt, the kernel checks the need_resched flag. If it is set, the kernel invokes the scheduler (using schedule()) before continuing. The flag is a message to the kernel that the scheduler should be invoked as soon as possible because another process deserves to run.\nUser preemption # User preemption occurs when the kernel is about to return to user-space, need_resched is set, and therefore, the scheduler is invoked. If the kernel is returning to user-space, it knows it is safe to continue executing the current task or to pick a new task to execute.\nConsequently, whenever the kernel is preparing to return to user-space either on return from an interrupt or after a system call, the value of need_resched is checked. If it is set, the scheduler is invoked to select a new (more fit) process to execute.\nUser preemption can occur\nWhen returning to user-space from a system call. When returning to user-space from an interrupt handler. Kernel Preemption # The Linux kernel, unlike most other Unix variants and many other operating systems, is a fully preemptive kernel. In nonpreemptive kernels, kernel code runs until completion, the scheduler cannot reschedule a task while it is in the kernel. Kernel code runs until it finishes (returns to user-space) or explicitly blocks. However, the Linux kernel is able to preempt a task at any point, so long as the kernel is in a state in which it is safe to reschedule.\nSo when is it safe to reschedule? The kernel can preempt a task running in the kernel so long as it does not hold a lock. That is, locks are used as markers of regions of nonpreemptibility. If a lock is not held, the current code is reentrant and capable of being preempted.\nKernel preemption can occur\nWhen an interrupt handler exits, before returning to kernel-space. When kernel code becomes preemptible again, when all locks are released. If a task in the kernel explicitly calls schedule(). If a task in the kernel blocks (which results in a call to schedule()). Real-Time Scheduling Policies # Linux provides two real-time scheduling policies\nSCHED_FIFO SCHED_RR The normal, not real-time scheduling policy is SCHED_NORMAL (using the CFS). Via the scheduling classes framework, these real-time policies are managed not by the Completely Fair Scheduler, but by a special real-time scheduler.\nSCHED_FIFO implements a simple first-in, first-out scheduling algorithm without timeslices.\nA runnable SCHED_FIFO task is always scheduled over any SCHED_NORMAL tasks. When a SCHED_FIFO task becomes runnable, it continues to run until it blocks or explicitly yields the processor; it has no timeslice and can run indefinitely. Only a higher priority SCHED_FIFO or SCHED_RR task can preempt a SCHED_FIFO task. Two or more SCHED_FIFO tasks at the same priority run round-robin, but only yielding the processor when they explicitly choose to do so. If a SCHED_FIFO task is runnable, all other tasks at a lower priority cannot run until the SCHED_FIFO task becomes unrunnable. SCHED_RR is identical to SCHED_FIFO except that each process can run only until it exhausts a predetermined timeslice. In other words, SCHED_RR is SCHED_FIFO with timeslices. It is a real-time, round-robin scheduling algorithm.\nWhen a SCHED_RR task exhausts its timeslice, any other real-time processes at its priority are scheduled round-robin. The timeslice is used to allow only rescheduling of same-priority processes. As with SCHED_FIFO, a higher-priority process always immediately preempts a lower-priority one, and a lower-priority process can never preempt a SCHED_RR task, even if its timeslice is exhausted. Both real-time scheduling policies implement static priorities. The kernel does not calculate dynamic priority values for real-time tasks. This ensures that a real-time process at a given priority always preempts a process at a lower priority.\nTwo types of real-time behavior exists:\nSoft real-time, meaning that the kernel tries to schedule applications within timing deadlines, but the kernel does not promise to always achieve these goals. Hard real-time systems are guaranteed to meet any scheduling requirements within certain limits. The real-time scheduling policies in Linux provide soft real-time behavior. Linux makes no guarantees on the capability to schedule real-time tasks. Despite not having a design that guarantees hard real-time behavior, the real-time scheduling performance in Linux is quite good.\nReal-time priorities range inclusively from 0 to MAX_RT_PRIO - 1. By default, this range is 0 to 99, since MAX_RT_PRIO is 100. This priority space is shared with the nice values of SCHED_NORMAL tasks. They use the space from MAX_RT_PRIO to (MAX_RT_PRIO + 40). By default, this means the –20 to +19 nice range maps directly onto the priority space from 100 to 139.\nDefault priority ranges:\n0 to 99: real-time priorities. 100 to 139: normal priorities. Scheduler-Related System Calls # Linux provides a family of system calls for the management of scheduler parameters. These system calls allow manipulation of process priority, scheduling policy, and processor affinity, as well as provide an explicit mechanism to yield the processor to other tasks.\n"},{"id":5,"href":"/grimoire/docs/linux-kernel-development/5-system-calls/","title":"5. System Calls","section":"Linux Kernel Development","content":" System Calls # In any modern operating system, the kernel provides a set of interfaces by which processes running in user-space can interact with the system. These interfaces give applications controlled access to hardware, a mechanism with which to create new processes and communicate with existing ones, and the capability to request other operating system resources. The existence of these interfaces, and the fact that applications are not free to directly do whatever they want, is key to providing a stable system.\nCommunicating with the Kernel # System calls provide a layer between the hardware and user-space processes, which serves three primary purposes:\nProviding an abstracted hardware interface for userspace. For example, when reading or writing from a file, applications are not concerned with the type of disk, media, or even the type of filesystem on which the file resides. Ensuring system security and stability. The kernel acts as a middleman between system resources and user-space, so it can arbitrate access based on permissions, users, and other criteria. For example, this arbitration prevents applications from incorrectly using hardware, stealing other processes’ resources, or otherwise doing harm to the system. A single common layer between user-space and the rest of the system allows for the virtualized system provided to processes. It would be impossible to implement multitasking and virtual memory if applications were free to access access system resources without the kernel’s knowledge. In Linux, system calls are the only means user-space has of interfacing with the kernel and the only legal entry point into the kernel other than exceptions and traps. Other interfaces, such as device files or /proc, are ultimately accessed via system calls. Interestingly, Linux implements far fewer system calls than most systems.\nAPIs, POSIX, and the C Library # Applications are typically programmed against an Application Programming Interface (API) implemented in user-space, not directly to system calls, because no direct correlation is needed between the interfaces used by applications and the actual interface provided by the kernel. An API defines a set of programming interfaces used by applications. Those interfaces can be implemented as a system call, implemented through multiple system calls, or implemented without the use of system calls at all. The same API can exist on multiple systems and provide the same interface to applications while the implementation of the API itself can differ greatly from system to system.\nThe most common APIs in the Unix world is based on POSIX. Technically, POSIX is composed of a series of standards from the IEEE that aim to provide a portable operating system standard roughly based on Unix. Linux strives to be POSIX- and SUSv3-compliant where applicable.\nOn most Unix systems, the POSIX-defined API calls have a strong correlation to the system calls. Some systems that are rather un-Unix, such as Microsoft Windows, offer POSIX-compatible libraries.\nThe system call interface in Linux, as with most Unix systems, is provided in part by the C library.\nThe C library implements the main API on Unix systems, including the standard C library and the system call interface. The C library is used by all C programs and, because of C’s nature, is easily wrapped by other programming languages for use in their programs. The C library additionally provides the majority of the POSIX API.\nFrom the application programmer’s point of view, system calls are irrelevant; all the programmer is concerned with is the API. Conversely, the kernel is concerned only with the system calls; what library calls and applications make use of the system calls is not of the kernel’s concern. Nonetheless, it is important for the kernel to keep track of the potential uses of a system call and keep the system call as general and flexible as possible.\nSyscalls # System calls (often called syscalls in Linux) are typically accessed via function calls defined in the C library. The functions can define zero, one, or more arguments (inputs) and might result in one or more side effects. Although nearly all system calls have a side effect (that is, they result in some change of the system’s state), a few syscalls, such as getpid(), merely return some data from the kernel. System calls also provide a return value of type long (for compatibility with 64-bit architectures) that signifies success or error. Usually, although not always, a negative return value denotes an error. A return value of zero is usually (not always) a sign of success.\nSystem Call Numbers # In Linux, each system call is assigned a syscall number.This is a unique number that is used to reference a specific system call. When a user-space process executes a system call, the syscall number identifies which syscall was executed; the process does not refer to the syscall by name.\nWhen assigned, the syscall number cannot change; otherwise, compiled applications will break. If a system call is removed, its system call number cannot be recycled, or previously compiled code would aim to invoke one system call but would in reality invoke another. Linux provides a \u0026ldquo;not implemented\u0026rdquo; system call, sys_ni_syscall(), which does nothing except return ENOSYS, the error corresponding to an invalid system call. This function is used to \u0026ldquo;plug the hole\u0026rdquo; in the rare event that a syscall is removed or otherwise made unavailable.\nSystem Call Performance # System calls in Linux are faster than in many other operating systems. This is partly because of Linux’s fast context switch times; entering and exiting the kernel is a streamlined and simple affair. The other factor is the simplicity of the system call handler and the individual system calls themselves.\nSystem Call Handler # It is not possible for user-space applications to execute kernel code directly. They cannot simply make a function call to a method existing in kernel-space because the kernel exists in a protected memory space. If applications could directly read and write to the kernel’s address space, system security and stability would be nonexistent.\nUser-space applications signal the kernel that they want to execute a system call and have the system switch to kernel mode, where the system call can be executed in kernel-space by the kernel on behalf of the application. This mechanism is a software interrupt: incur an exception, and the system will switch to kernel mode and execute the exception handler. The exception handler in this case is actually the system call handler. The defined software interrupt on x86 for the syscall handler is interrupt number 128.\nRecently, x86 processors added a feature known as sysenter, which provides a faster, more specialized way of trapping into a kernel to execute a system call than using the int interrupt instruction.\nDenoting the Correct System Call # Simply entering kernel-space alone is not sufficient because multiple system calls exist, all of which enter the kernel in the same manner. Thus, the system call number must be passed into the kernel. On x86, the syscall number is fed to the kernel via the eax register. Before causing the trap into the kernel, user-space sticks in eax the number corresponding to the desired system call. The system call handler then reads the value from eax.\nParameter Passing # In addition to the system call number, most syscalls require that one or more parameters be passed to them. Somehow, user-space must relay the parameters to the kernel during the trap. The easiest way to do this is via the same means that the syscall number is passed: the parameters are stored in registers.\nOn x86-32, the registers ebx, ecx, edx, esi, and edi contain, in order, the first five arguments. In the unlikely case of six or more arguments, a single register is used to hold a pointer to user-space where all the parameters are stored.\nThe return value is sent to user-space also via register. On x86, it is written into the eax register.\nSystem Call Implementation # The actual implementation of a system call in Linux does not need to be concerned with the behavior of the system call handler. Thus, adding a new system call to Linux is relatively easy. The hard work lies in designing and implementing the system call; registering it with the kernel is simple.\nImplementing System Calls # Each syscall should have exactly one purpose. Multiplexing syscalls (a single system call that does wildly different things depending on a flag argument) is discouraged in Linux. System call should have a clean and simple interface with the smallest number of arguments possible. The semantics and behavior of a system call are important; they must not change, because existing applications will come to rely on them. Design the system call to be as general as possible with an eye toward the future. The purpose of the system call will remain constant but its uses may change. Verifying the Parameters # System calls must carefully verify all their parameters to ensure that they are valid, legal and correct to guarantee the system’s security and stability.\nOne of the most important checks is the validity of any pointers that the user provides. Before following a pointer into user-space, the system must ensure that:\nThe pointer points to a region of memory in user-space. Processes must not be able to trick the kernel into reading data in kernel-space on their behalf. The pointer points to a region of memory in the process’s address space.The process must not be able to trick the kernel into reading someone else’s data. The process must not be able to bypass memory access restrictions. If reading, the memory is marked readable. If writing, the memory is marked writable. If executing, the memory is marked executable. System Call Context # The kernel is in process context during the execution of a system call. The current pointer points to the current task, which is the process that issued the syscall.\nIn process context, the kernel is capable of sleeping (for example, if the system call blocks on a call or explicitly calls schedule()) and is fully preemptible. The capability to sleep means that system calls can make use of the majority of the kernel’s functionality.\n"}]